Discriminative Robust Transformation Learning

Jiaji Huang

Qiang Qiu

Guillermo Sapiro

Robert Calderbank

Department of Electrical Engineering, Duke University
Durham, NC 27708 {jiaji.huang,qiang.qiu,guillermo.sapiro,robert.calderbank}@duke.edu

Abstract
This paper proposes a framework for learning features that are robust to data variation, which is particularly important when only a limited number of training samples are available. The framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm. Robustness is achieved by encouraging the transform that maps data to features to be a local isometry. This geometric property is shown to improve (K, )-robustness, thereby providing theoretical justification for reductions in generalization error observed in experiments. The proposed optimization framework is used to train standard learning algorithms such as deep neural networks. Experimental results obtained on benchmark datasets, such as labeled faces in the wild, demonstrate the value of being able to balance discrimination and robustness.
1 Introduction
Learning features that are able to discriminate is a classical problem in data analysis. The basic idea is to reduce the variance within a class while increasing it between classes. One way to implement this is by regularizing a certain measure of the variance, while assuming some prior knowledge about the data. For example, Linear Discriminant Analysis (LDA) [4] measures sample covariance and implicitly assumes that each class is Gaussian distributed. The Low Rank Transform (LRT) [10], instead uses nuclear norm to measure the variance and assumes that each class is near a low-rank subspace. A different approach is to regularize the pairwise distances between data points. Examples include the seminal work on metric learning [17] and its extensions [5, 6, 16].
While great attention has been paid to designing objectives to encourage discrimination, less effort has been made in understanding and encouraging robustness to data variation, which is especially important when a limited number of training samples are available. One exception is [19], which promotes robustness by regularizing the traditional metric learning objective using prior knowledge from an auxiliary unlabeled dataset.
In this paper we develop a general framework for balancing discrimination and robustness. Robustness is achieved by encouraging the learned data-to-features transform to be locally an isometry within each class. We theoretically justify this approach using (K, )-robustness [1, 18] and give an example of the proposed formulation, incorporating it in deep neural networks. Experiments validate the capability to trade-off discrimination against robustness. Our main contributions are the following: 1) prove that locally near isometry leads to robustness; 2) propose a practical framework that allows to robustify a wide class of learned transforms, both linear and nonlinear; 3) provide an explicit realization of the proposed framework, achieving competitive results on difficult face verification tasks.
The paper is organized as follows. Section 2 motivates the proposed study and proposes a general formulation for learning a Discriminative Robust Transform (DRT). Section 3 provides a theoretical justification for the framework by making an explicit connection to robustness. Section 4 gives a
1

specific example of DRT, denoted as Euc-DRT. Section 5 provides experimental validation of EucDRT, and section 6 presents conclusions. 1
2 Problem Formulation
Consider an L-way classification problem. The training set is denoted by T = {(xi, yi)}, where xi  Rn is the data and yi  {1, . . . , L} is the class label. We want to learn a feature transform f(*) such that a datum x becomes more discriminative when it is transformed to feature f(x). The transform f is parametrized by a vector , a framework that includes linear transforms and neural networks where the entries of  are the learned network parameters.
2.1 Motivation
The transform f promotes discriminability by reducing intra-class variance and enlarging interclass variance. This aim is expressed in the design of objective functions [5, 10] or the structure of the transform f [7, 11]. However the robustness of the learned transform is an important issue that is often overlooked. When training samples are scarce, statistical learning theory [15] predicts overfitting to the training data. The result of overfitting is that discrimination achieved on test data will be significantly worse than that on training data. Our aim in this paper is the design of robust transforms f for which the training-to-testing degradation is small [18].
We formally measure robustness of the learned transform f in terms of (K, )-robustness [1]. Given a distance metric , a learning algorithm is said to be (K, )-robust if the input data space can be partitioned into K disjoint sets Sk, k = 1, ..., K, such that for all training sets T , the learned parameter T determines a loss for which the value on pairs of training samples taken from different sets Sj and Sk is very close to the value of any pair of data samples taken from Sj and Sk.
(K, )-robustness is illustrated in Fig. 1, where S1 and S2 are both of diameter  and |e - e | = |(f(x1), f(x2)) - (f(x1), f(x2))|.
If the transform f preserves all distances within S1 and S2, then |e - e | cannot deviate much from |d - d |  2.

Figure 1: (K, )-robustness: Here d = (x1, x2), d = (x1, x2), e = (f(x1), f(x2)), and e = (f(x1), f(x2)). The difference |e - e | cannot deviate too much from |d - d |.

2.2 Formulation and Discussion

Motivated by the above reasoning, we now present our proposed framework. First we define a pair

label i,j

1 -1

if yi = yj otherwise

. Given a metric , we use the following hinge loss to encourage

high inter-class distance and small intra-class distance.

1 |P |

max {0, i,j [ (f(xi), f(xj)) - t( i,j)]} ,

i,jP

(1)

Here P = {(i, j|i = j)} is the set of all data pairs. t( i,j)  0 is a function of i,j and t(1) < t(-1).

Similar to metric learning [17], this loss function connects pairwise distance to discrimination. How-

ever traditional metric learning typically assumes squared Euclidean distance and here the metric 

can be arbitrary.

For robustness, as discussed above, we may want f(*) to be distance-preserving within each small local region. In particular, we define the set of all local neighborhoods as
N B {(i, j)| i,j = 1, (xi, xj)  } .

1A note on the notations: matrices (vectors) are denoted in upper (lower) case bold letters. Scalars are denoted in plain letters.

2

Therefore, we minimize the following objective function

1 |N B|

|(f(xi), f(xj)) - (xi, xj)| .

(i,j)N B

(2)

Note that we do not need to have the same metric in both the input and the feature space, they do not

even have in general the same dimension. With a slight abuse of notation we use the same symbol

to denote both metrics.

To achieve discrimination and robustness simultaneously, we formulate the objective function as a

weighted linear combination of the two extreme cases in (1) and (2)

 |P |

max {0,

i,j [ (f(xi), f(xj)) - t(

i,j

)]}+

1- |N B|

|(f(xi), f(xj)) - (xi, xj)|

i,jP

(i,j)N B

(3)

where   [0, 1]. The formulation (3) balances discrimination and robustness. When  = 1 it seeks

discrimination, and as  decreases it starts to encourage robustness. We shall refer to a transform

that is learned by solving (3) as a Discriminative Robust Transform (DRT). The DRT framework

provides opportunity to select both the distance measure and the transform family.

3 Theoretical Analysis

In this section, we provide a theoretical explanation for robustness. In particular, we show that if the solution to (1) yields a transform f that is locally a near isometry, then f is robust.

3.1 Theoretical Framework

Let X denote the original data, let Y = {1, ..., L} denote the set of class labels, and let Z = X x Y.
The training samples are pairs zi = (xi, yi), i = 1, . . . , n drawn from some unknown distribution D defined on Z. The indicator function is defined as i,j = 1 if yi = yj and -1 otherwise. Let f be a transform that maps a low-level feature x to a more discriminative feature f(x), and let F denote the space of transformed features.

For simplicity we consider an arbitrary metric  defined on both X and F (the general case of different metrics is a straightforward extension), and a loss function g((f(xi), f(xj)), i,j) that encourages (f(xi), f(xj)) to be small (big) if i,j = 1 (-1). We shall require the Lipschtiz constant of g(*, 1) and g(*, -1) to be upper bounded by A > 0. Note that the loss function in Eq. (1) has a Lipschtiz constant of 1. We abbreviate
g((f(xi), f(xj)), i,j) h(zi, zj).

The empirical loss on the training set is a function of  given by

Remp()

2 n(n-1)

n
i,j

=1

h

(zi

,

zj

),

i=j

and the expected loss on the test data is given by

(4)

R()

Ez1,z2D [h(z1, z2)] .

The algorithm operates on pairs of training samples and finds parameters

(5)

T arg min Remp(),

(6)



that minimize the empirical loss on the training set T . The difference Remp - R between expected

loss on the test data and empirical loss on the training data is the generalization error of the algorithm.

3.2 (K, )-robustness and Covering Number
We work with the following definition of (K, )-robustness [1]. Definition 1. A learning algorithm is (K, )-robust if Z = X xY can be partitioned into K disjoint sets Zk, k = 1, . . . , K such that for all training sets T  Zn, the learned parameter T determines a loss function where the value on pairs of training samples taken from sets Zp and Zq is "very close" to the value of any pair of data samples taken from Zp and Zq. Formally, assume zi, zj  T , with zi  Zp and zj  Zq, if zi  Zp and zj  Zq, then
hT (zi, zj ) - hT (zi, zj )  .

3

Remark 1. (K, )-robustness means that the loss incurred by a testing pair (zi, zj) in Zp x Zq is very close to the loss incurred by any training pair (zi, zj) in Zp x Zq. It is shown in [1] that the generalization error of (K, )-robust algorithms is bounded as

R(T ) - Remp(T )  + O

K .
n

(7)

Therefore the smaller , the smaller is the generalization error, and the more robust is the learning algorithm.

Given a metric space, the covering number specifies how many balls of a given radius are needed to cover the space. The more complex the metric space, the more balls are needed to cover it. Covering number is formally defined as follows. Definition 2 (Covering number). Given a metric space (S, ), we say that a subset S of S is a -cover of S, if for every element s  S, there exists s  S such that (s, s)  . The -covering number of S is
N(S, ) = min{|S| : S is a -cover of S}.
Remark 2. The covering number is a measure of the geometric complexity of (S, ). A set S with covering number N/2(S, ) can be partitioned into N/2(S, ) disjoint subsets, such that any two points within the same subset are separated by no more than .
Lemma 1. The metric space Z = X x Y can be partitioned into LN/2(X , ) subsets, denoted
as Z1, . . . , ZLN/2(X ,), such that any two points z1 (x1, y1), z2 (x2, y2) in the same subset satisfy y1 = y2 and (x1, x2)  .

Proof. Assuming the metric space (X , ) is compact, we can partition X into N/2(X , ) subsets, each with diameter at most . Since Y is a finite set of size L, we can partition Z = X x Y into
LN/2(X , ) subsets with the property that two samples (x1, y1), (x2, y2) in the same subset satisfy
y1 = y2 and (x1, x2)  .

It follows from Lemma 1 that we may partition X into subsets X1, . . . , XLN/2(X ,), such that pairs of points x1, x2 from the same subset have the same label and satisfy (xi, xj)  . Before we connect local geometry to robustness we need one more definition. We say that a learned transform f is a -isometry if the metric is distorted by at most : Definition 3 (-isometry). Let A, B be metric spaces with metrics A and B. A map f : A  B is a -isometry if for any a1, a2  A, |A(f (a1), f (a2)) - B(a1, a2)|  . Theorem 1. Let f be a transform derived via Eq. (6) and let X1, . . . , XLN/2(X ,) be a cover of X as described above. If f is a -isometry, then it is (LN/2(X , ), 2A( + ))-robust.
Proof sketch. Consider training samples zi, zj and testing samples zi, zj such that zi, zi  Zp and zj, zj  Zq for some p, q  {1, . . . , LN/2(X , )}. Then by Lemma 1,
(xi, xi)   and (xj, xj)  , yi = yi and yj = yj,
and xi, xi  Xp and xj, xj  Xq. By definition of -isometry,
|(fT (xi), fT (xi)) - (xi, xi)|   and |(fT (xj), fT (xj)) - (xj, xj)|  .
Rearranging the terms gives
(fT (xi), fT (xi))  (xi, xi) +    +  and (fT (xj), fT (xj))  (xj, xj) +    + .

Figure 2: Proof without words. 4

In order to bound the generalization error, we need to bound the difference between (fT (xi), fT (xj)) and (fT (xi), fT (xj)). The details can be found in [9]; here we appeal to the proof schematic in Fig. 2. We need to bound |e - e | and it cannot exceed twice the
diameter of a local region in the transformed domain.

Robustness of the learning algorithm depends on the granularity of the cover and the degree to

which the learned transform f distorts distances between pairs of points in the same covering subset. The subsets in the cover constitute regions where the local geometry makes it possible to bound generalization error. It now follows from [1] that the generalization error satisfies R(T ) -

Remp(T )  2A( + ) + O

K n

. The DRT proposed here is a particular example of a local

isometry, and Theorem 1 explains why the generalization error is smaller than that of pure metric

learning.

The transform described in [9] partitions the metric space X into exactly L subsets, one for each class. The experiments reported in Section 5 demonstrate that the performance improvements derived from working with a finer partition can be worth the cost of learning finer grained local regions.

4 An Illustrative Realization of DRT

Having justified robustness, we now provide a realization of the proposed general DRT where the metric  is Euclidean distance. We use Gaussian random variables to initialize , then, on the randomly transformed data, we set t(1) (t(-1)) to be the average intra-class (inter-class) pairwise distance. In all our experiments, the solution satisfied the condition t(1) < t(-1) required in Eq. (1). We calculate the diameter  of the local regions N B indirectly, using the -nearest neighbors of each
training sample to define a local neighborhood. We leave the question of how best to initialize the indicator t and the diameter  for future research.

We denote this particular example as Euc-DRT and use gradient descent to solve for . Denoting

the objective by J , we define yi f(xi), i,j f(xi) - f(xj), and 0i.j xi - xj . Then

J =
yi

(i,j)P

 |P |

*

i,j *

i,j i,j

+

1- |N B|

* sgn(

i,j

(i,j)N B

- 0i,j ) *

i,j . i,j

(8)

i,j ( i,j -t( i,j ))>0

In general, f defines a D-layer neural network (when D = 1 it defines a linear transform). Let (d)
be the linear weights at the d-th layer, and let x(d) be the output of the d-th layer, so that yi = x(iD). Then the gradients are computed as,

J (D) =

i

J yi

*



yi (D)

,

and

J (d)

=

i

J * x(id+1) * x(id) xi(d+1) x(id) (d)

for 1  d  D -1.

(9)

Algorithm 1 provides a summary, and we note that the extension to stochastic training using min-

batches is straightforward.

5 Experimental Results
In this section we report on experiments that confirm robustness of Euc-DRT. Recall that empirical loss is given by Eq. (4) where  is learned as T from the training set T , and |T | = N . The generalization error is R - Remp where the expected loss R is estimated using a large test set.
5.1 Toy Example
This illustrative example is motivated by the discussion in Section 2.1. We first generate a 2D dataset consisting of two noisy half-moons, then use a random 100 x 2 matrix to embed the data in a 100-dimensional space. We learn a linear transform f that maps the 100 dimensional data to 2 dimensional features, and we use  = 5 nearest neighbors to construct the set N B. We consider  = 1, 0.5, 0.25, representing the most discriminative, balanced, and more robust scenarios.
When  = 1 the transformed training samples are rather discriminative (Fig. 3a), but when the transform is applied to testing data, the two classes are more mixed (Fig. 3d). When  = 0.5, the

5

Algorithm 1 Gradient descent solver for Euc-DRT

Input:   [0, 1], training pairs {(xi, xj, i,j)}, a pre-defined D-layer network (D = 1 as linear

transform), stepsize , neighborhood size .

Output: 

1: Randomly initialize , compute yi = f(xi). 2: On the yi, compute the average intra and inter-class pairwise distances, assign to t(1), t(-1) 3: For each training datum, find its  nearest neighbor and define the set N B.

4: while stable objective not achieved do

5: Compute yi = f(xi) by a forward pass.

6: Compute objective J.

7:

Compute

J yi

as

Eq.

(8).

8: for l = D down to 1 do

9:

Compute

J  (d)

as

Eq.

(9).

10:

(d)



(d)

-

 J
 (d)

.

11: end for

12: end while

30 30 30

20 20 20

10 10 10

000

-10 -10 -10

-20 -20 -20

-30 -20 0 20

-30 -20 0 20

-30 -20 0 20

(a)  = 1 Transformed training (b)  = 0.5 transformed training (c)  = 0.25 Transformed train-

samples. (discriminative case) samples. (balanced case)

ing samples. (robust case)

30 30 30

20 20 20

10 10 10

000

-10 -10 -10

-20 -20 -20

-30 -20 0 20

-30 -20 0 20

-30 -20 0 20

(d)  = 1 Transformed testing (e)  = 0.5 transformed testing (f)  = 0.25 Transformed testing

samples. (discriminative case) samples. (balanced case)

samples. (robust case)

Figure 3: Original and transformed training/testing samples embedded in 2-dimensional space with different colors representing different classes.

transformed training data are more dispersed within each class (Fig. 3b), hence less easily separated than when  = 1. However Fig. 3e shows that it is easier to separate the two classes on the test data. When  = 0.25, robustness is preferred to discriminative power as shown in Figs. 3c and 3f. Tab. 1 quantifies empirical loss Remp, generalization error, and classification performance (by 1-nn) for  = 1, 0.5 and 0.25. As  decreases, Remp increases, indicating loss of discrimination on the training set. However, generalization error decreases, implying more robustness. We conclude that by varying , we can balance discrimination and robustness.
5.2 MNIST Classfication Using a Very Small Training Set
The transform f learned in the previous section was linear, and we now apply a more sophisticated convolutional neural network to the MNIST dataset. The network structure is similar to LeNet, and is
6

Table 1: Varying  on a toy dataset.

 Remp generalization error 1-nn accuracy (original data 93.35%)

1 1.5983 10.5855
92.20%

0.5 1.6025 9.5071
98.30%

0.25 1.9439 8.8040
91.55%

Table 2: Classification error on MNIST.

Training/class original pixels
LeNet DML Euc-DRT

30 81.91% 87.51% 92.32% 94.14%

50 86.18% 89.89% 94.45% 95.20%

70 86.86% 91.24% 95.67% 96.05%

100 88.49% 92.75% 96.19% 96.21%

Table 3: Implementation details of the neural network for MNIST classification.

name conv1 pool1 conv2 pool2 conv3

parameters size: 5 x 5 x 1 x 20
stride: 1, pad: 0 size: 2 x 2
size: 5 x 5 x 20 x 50 stride: 1, pad: 0 size: 2 x 2
size: 4 x 4 x 50 x 128 stride: 1, pad: 0

made up of alternating convolutional layers and pooling layers, with parameters detailed in Table 3. We map the original 784-dimensional pixel values (28x28 image) to 128-dimensional features.
While state-of-art results often use the full training set (6,000 training samples per class), here we are interested in small training sets. We use only 30 training samples per class, and we use  = 7 nearest neighbors to define local regions in Euc-DRT. We vary  and study empirical error, generalization error, and classification accuracy (1-nn). We observe in Fig. 4 that when  decreases, the empirical error also decreases, but that the generalization error actually increases. By balancing between these two factors, a peak classification accuracy is achieved at  = 0.25. Next, we use 30, 50, 70, 100

R emp R-R emp 1-nn accuracy(%)

0.14 0.12
0.1 0.08 0.06 0.04 0.02
0 0

0.25 0.5 
(a)

0.75

1

4.5 94.5

4 94
3.5 93.5
3 93
2.5
2 92.5

1.5 0

0.25 0.5 
(b)

0.75

1

92 0

0.25

0.5 0.75



(c)

1

Figure 4: MNIST test: with only 30 training samples per class. We vary  and assess (a) Remp; (b) generalization error; and (c) 1-nn classification accuracy. Peak accuracy is achieved at  = 0.25.
training samples per class and compare the performance of Euc-DRT with LeNet and Deep Metric Learning (DML) [7]. DML minimizes a hinge loss on the squared Euclidean distances. It shares the same spirit with our Euc-DRT using  = 1. All methods use the same network structure, Tab. 3, to map to the features. For classification, LeNet uses a linear softmax classifier on top of the "conv3" layer and minimizes the standard cross-entropy loss during training. DML and Euc-DRT both use a 1-nn classifier on the learned features. Classification accuracies are reported in Tab. 2. In Tab. 2, we see that all the learned features improve upon the original ones. DML is very discriminative and achieves higher accuracy than LeNet. However, when the training set is very small, robustness becomes more important and Euc-DRT significantly outperforms DML.

5.3 Face Verification on LFW
We now present face verification on the more challenging Labeled Faces in the Wild (LFW) benchmark, where our experiments will show that there is an advantage to balancing disciminability and robustness. Our goal is not to reproduce the success of deep learning in face verification [7, 14], but to stress the importance of robust training and to compare the proposed Euc-DRT objective with popular alternatives. Note also that it is difficult to compare with deep learning methods when training sets are proprietary [12-14].

7

We adopt the experimental framework used in [2], and train a deep network on the WDRef dataset, where each face is described using a high dimensional LBP feature [3] (available at 2) that is reduced to a 5000-dimensional feature using PCA. The WDRef dataset is significantly smaller than the proprietary datasets typical of deep learning, such as the 4.4 million labeled faces from 4030 individuals in [14], or the 202,599 labeled faces from 10,177 individuals in [12]. It contains 2,995 subjects with about 20 samples per subject.
We compare the Euc-DRT objective with DeepFace (DF) [14] and Deep Metric Learning (DML) [7], two state-of-the-art deep learning objectives. For a fair comparison, we employ the same network structure and train on the same input data. DeepFace feeds the output of the last network layer to an L-way soft-max to generate a probability distribution over L classes, then minimizes a cross entropy loss. The Euc-DRT feature f is implemented as a two-layer fully connected network with tanh as the squash function. Weight decay (conventional Frobenius norm regularization) is employed in both DF and DML, and results are only reported for the best weight decay factor. After a network is trained on WDRef, it is tested on the LFW benchmark. Verification simply consists of comparing the cosine distance between a given pair of faces to a threshold.
Fig. 5 displays ROC curves and Table 4 reports area under the ROC curve (AUC) and verification accuracy. High-Dim LBP refers to verification using the initial LBP features. DeepFace (DF) optimizes for a classification objective by minimizing a softmax loss, and it successfully separates samples from different classes. However the constraint that assigns similar representations to the same class is weak, and this is reflected in the true positive rate displayed in Fig. 5. In Deep Metric Learning (DML) this same constraint is strong, but robustness is a concern when the training set is small. The proposed Euc-DRT improves upon both DF and DML by balancing disciminability and robustness. It is less conservative than DF for better discriminability, and more responsive to local geometry than DML for smaller generalization error. Face verification accuracy for Euc-DRT was obtained by varying the regularization parameter  between 0.4 and 1 (as shown in Fig 6), then reporting the peak accuracy observed at  = 0.9.

1

0.9

0.8

0.7 0.6 0.5
0

HD-LBP deepFace DML Euc-DRT
0.5 1

Figure 5: Comparison of ROCs for all methods

92.4

verification accuracy (%)

92.2

92

91.8

91.6

91.4 0.4

0.6 0.8 

1

Figure 6: Verification accuracy of Euc-DRT as  varies

Table 4: Verification accuracy and AUCs on LFW

Method
HD-LBP deepFace
DML Euc-DRT

Accuracy (%) 74.73 88.72 90.28 92.33

AUC (x10-2) 82.221.00 95.50 0.29 96.740.33 97.77 0.25

6 Conclusion
We have proposed an optimization framework within which it is possible to tradeoff the discriminative value of learned features with robustness of the learning algorithm. Improvements to generalization error predicted by theory are observed in experiments on benchmark datasets. Future work will investigate how to initialize and tune the optimization, also how the Euc-DRT algorithm compares with other methods that reduce generalization error.
7 Acknowledgement
The work of Huang and Calderbank was supported by AFOSR under FA 9550-13-1-0076 and by NGA under HM017713-1-0006. The work of Qiu and Sapiro is partially supported by NSF and DoD.

2http://home.ustc.edu.cn/chendong/

8

References
[1] A. Bellet and A. Habrard. Robustness and generalization for metric learning. Neurocomputing, 151(14):259-267, 2015.
[2] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian face revisited: A joint formulation. In European Conference on Computer Vision (ECCV), 2012.
[3] D. Chen, X. Cao, F. Wen, and J. Sun. Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.
[4] K. Fukunaga. Introduction to Statistical Pattern Recognition. San Diego: Academic Press, 1990.
[5] A. Globerson and S. Roweis. Metric learning by collapsing classes. In Advances in Neural Information Processing Systems (NIPS), 2005.
[6] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In Advances in Neural Information Processing Systems (NIPS), 2004.
[7] J. Hu, J. Lu, and Y. Tan. Discriminative deep metric learning for face verification in the wild. In Computer Vision and Pattern Recognition (CVPR), pages 1875-1882, 2014.
[8] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 0749, University of Massachusetts, Amherst, October 2007.
[9] J. Huang, Q. Qiu, R. Calderbank, and G. Sapiro. Geometry-aware deep transform. In International Conference on Computer Vision, 2015.
[10] G. Sapiro Q. Qiu. Learning transformations for clustering and classification. Journal of Machine Learning Research (JMLR), pages 187-225, 2015.
[11] C. Sumit, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face verification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pages 539-546, 2005.
[12] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face representation by joint identification-verification. In Advances in Neural Information Processing Systems (NIPS), pages 1988-1996, 2014.
[13] Y. Sun, X. Wang, and X. Tang. Deep learning face representation from predicting 10,000 classes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1891-1898, 2014.
[14] Y. Taigman, M. Yang, M. A. Ranzato, and L. Wolf. Deepface: Closing the gap to humanlevel performance in face verification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1701-1708, 2014.
[15] V. N. Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural Networks, 10(5):988-999, 1999.
[16] K. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor classification. Journal of Machine Learning Research, 10:207-244, 2009.
[17] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning, with application to clustering with side-information. In Advances in Neural Information Processing Systems (NIPS), 2002.
[18] H. Xu and S. Mannor. Robustness and generalization. Machine Learning, 86(3):391-423, 2012.
[19] Z. Zha, T. Mei, M. Wang, Z. Wang, and X. Hua. Robust distance metric learning with auxiliary knowledge. In International Joint Conference on Artificial Intelligence (IJCAI), 2009.
9

