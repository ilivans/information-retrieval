Learning with Group Invariant Features: A Kernel Perspective.

Youssef Mroueh IBM Watson Group mroueh@us.ibm.com

Stephen Voinea CBMM, MIT.
voinea@mit.edu Co-first author

Tomaso Poggio CBMM, MIT . tp@ai.mit.edu

Abstract
We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1]. More specifically, a group invariant signal signature is obtained through cumulative distributions of group-transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar-integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of N points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.
1 Introduction
Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning, as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4]. Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks, but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by applying identitypreserving transformations such as shearing, rotation, translation, etc., to the training data. In this work, we adopt the approach of [1], where the representation of the signal is designed to reflect the invariant properties and model the world symmetries with group actions. The ultimate aim is to bridge unsupervised learning of invariant representations with invariant kernel methods, where we can use tools from classical supervised learning to easily address the statistical consistency and sample complexity questions [9, 10]. Indeed, many invariant kernel methods and related invariant kernel networks have been proposed. We refer the reader to the related work section for a review (Section 5) and we start by showing how to accomplish this invariance through group-invariant Haarintegration kernels [11], and then show how random features derived from a memory-based theory of invariances introduced in [1] approximate such a kernel.
1.1 Group Invariant Kernels
We start by reviewing group-invariant Haar-integration kernels introduced in [11], and their use in a binary classification problem. This section highlights the conceptual advantages of such kernels as well as their practical inconvenience, putting into perspective the advantage of approximating them with explicit and invariant random feature maps.
1

Invariant Haar-Integration Kernels. We consider a subset X of the hypersphere in d dimensions Sd-1. Let X be a measure on X . Consider a kernel k0 on X , such as a radial basis function kernel. Let G be a group acting on X , with a normalized Haar measure . G is assumed to be a compact and unitary group. Define an invariant kernel K between x, z  X through Haar-integration [11] as
follows:

K(x, z) =

k0(gx, g z)d(g)d(g ).

GG

(1)

As we are integrating over the entire group, it is easy to see that: K(g x, gz) = K(x, z), g, g 

G, x, z  X . Hence the Haar-integration kernel is invariant to the group action. The symmetry of

K is obvious. Moreover, if k0 is a positive definite kernel, it follows that K is positive definite as well [11]. One can see the Haar-integration kernel framework as another form of data augmentation,

since we have to produce group-transformed points in order to compute the kernel.

Invariant Decision Boundary. Turning now to a binary classification problem, we assume that we

are given a labeled training set: S = {(xi, yi) | xi  X , yi  Y = {1}}Ni=1. In order to learn a decision function f : X  Y, we minimize the following empirical risk induced by an L-Lipschitz,

convex loss function V , with V

(0) < 0 [12]:

minfHK EV (f ) :=

1 N

N i=1

V

(yif

(xi)),

where

we

restrict f to belong to a hypothesis class induced by the invariant kernel K, the so called Reproducing

Kernel Hilbert Space HK. The representer theorem [13] shows that the solution of such a problem,

or the optimal decision boundary fN has the following form: fN (x) =

N i=1

iK(x,

xi).

Since

the

kernel K is group-invariant it follows that :

ffNN

(x), (gx)

g  G. = fN (x),

Hence the g  G, x

the decision  X.

fN (gx) = boundary f

isNi=g1rouipK-i(ngvxar,ixain)t

= as

N i=1

i

K(x,

xi)

=

well, and we have:

Reduced Sample Complexity. We have shown that a group-invariant kernel induces a group-
invariant decision boundary, but how does this translate to the sample complexity of the learning algorithm? To answer this question, we will assume that the input set X has the following structure: X = X0  GX0, GX0 = {z|z = gx, x  X0, g  G/ {e}}, where e is the identity group element. This structure implies that for a function f in the invariant RKHS HK, we have:

z  GX0,  x  X0,  g  G such that, z = gx, and f (z) = f (x).

Let y(x) = P(Y = y|x) be the label posteriors. We assume that y(gx) = y(x), g  G. This is a natural assumption since the label is unchanged given the group action. Assume that the set X
is endowed with a measure X that is also group-invariant. Let f be the group-invariant decision function and consider the expected risk induced by the loss V , EV (f ), defined as follows:

EV (f ) =

V (yf (x))y(x)X (x)dx,

X yY

(2)

EV (f ) is a proxy to the misclassification risk [12]. Using the invariant properties of the function class and the data distribution we have by invariance of f , y, and :

EV (f ) =

V (yf (x))y(x)X (x)dx +

V (yf (z))y(z)X (z)dz

X0 yY

GX0 yY

= d(g)

V (yf (gx))y(gx)X (x)dx

G X0 yY

= d(g)

V (yf (x))y(x)X (x)dx (By invariance of f , y, and  )

G X0 yY

= V (yf (x))y(x)X (x)dx.
X0 yY
Hence, given an invariant kernel to a group action that is identity preserving, it is sufficient to minimize the empirical risk on the core set X0, and it generalizes to samples in GX0. Let us imagine that X is finite with cardinality |X |; the cardinality of the core set X0 is a small fraction of the cardinality of X : |X0| = |X |, where 0 <  < 1. Hence, when we sample training points from X0, the maximum size of the training set is N = |X | << |X |, yielding a reduction in the sample complexity.

2

1.2 Contributions
We have just reviewed the group-invariant Haar-integration kernel. In summary, a group-invariant kernel implies the existence of a decision function that is invariant to the group action, as well as a reduction in the sample complexity due to sampling training points from a reduced set, a.k.a the core set X0. Kernel methods with Haar-integration kernels come at a very expensive computational price at both training and test time: computing the Kernel is computationally cumbersome as we have to integrate over the group and produce virtual examples by transforming points explicitly through the group action. Moreover, the training complexity of kernel methods scales cubicly in the sample size. Those practical considerations make the usefulness of such kernels very limited. The contributions of this paper are on three folds:
1. We first show that a non-linear random feature map  : X  RD derived from a memorybased theory of invariances introduced in [1] induces an expected group-invariant Haarintegration kernel K. For fixed points x, z  X , we have: E (x), (z) = K(x, z), where K satisfies: K(gx, g z) = K(x, z), g, g  G, x, z  X .
2. We show a Johnson-Lindenstrauss type result that holds uniformly on a set of N points that assess the concentration of this random feature map around its expected induced kernel. For sufficiently large D, we have (x), (z)  K(x, z), uniformly on an N points set.
3. We show that, with a linear model, an invariant decision function can be learned in this random feature space by sampling points from the core set X0 i.e: fN (x)  w, (x) and generalizes to unseen points in GX0, reducing the sample complexity. Moreover, we show that those features define a function space that approximates a dense subset of the invariant RKHS, and assess the error rates of the empirical risk minimization using such random features.
4. We demonstrate the validity of these claims on three datasets: text (artificial), vision (MNIST), and speech (TIDIGITS).

2 From Group Invariant Kernels to Feature Maps

In this paper we show that a random feature map based on I-theory [1]:  : X  RD approximates a group-invariant Haar-integration kernel K having the form given in Equation (1):

(x), (z)  K(x, z).

We start with some notation that will be useful for defining the feature map. Denote the cumulative distribution function of a random variable X by,

FX ( ) = P(X   ),
Fix x  X , Let g  G be a random variable drawn according to the normalized Haar measure  and let t be a random template whose distribution will be defined later. For s > 0, define the following truncated cumulative distribution function (CDF) of the dot product x, gt :

(x, t,  ) = Pg( x, gt   ) = F x,gt ( ),   [-s, s], x  X ,

Let   (0, 1). We consider the following Gaussian vectors (sampling with rejection) for the tem-

plates t:

t=nN

1 0, d Id

, if

n

2 2

< 1 + ,

t =

else .

The reason behind this sampling is to keep the range of x, gt under control: The squared norm

n

2 2

will be bounded by 1 +  with high probability by a classical concentration result (See proof

of Theorem 1 for more details). The group being unitary and x  Sd-1, we know that : | x, gt | 

n 2 < 1 +   1 + , for   (0, 1).

Remark 1. We can also consider templates t, drawn uniformly on the unit sphere Sd-1. Uniform

templates on the sphere can be drawn as follows:

t=

 

,
2





N (0, Id),

3

 since the norm of a gaussian vector is highly concentrated around its mean d, we can use the gaussian sampling with rejection. Results proved for gaussian templates (with rejection) will hold true for templates drawn at uniform on the sphere with different constants.

Define the following kernel function,
s
Ks(x, z) = Et (x, t,  )(z, t,  )d,
-s
where s will be fixed throughout the paper to be s = 1+ since the gaussian sampling with rejection controls the dot product to be in that range. Let g  G. As the group is closed, we have (t, gx,  ) = G 1I ggx,t  d(g) = G 1I gx,t  d(g) = (t, x,  ) and hence K(gx, g z) = K(x, z), for all g, g  G. It is clear now that K is a group-invariant kernel. In order to approximate K, we sample |G| elements uniformly and independently from the group G, i.e. gi, i = 1 . . . |G|, and define the normalized empirical CDF :

|G|

(x, t,  ) = 1 |G| m

1I git,x  , - s    s.

i=1

We discretize the continuous threshold  as follows:

sk  x, t,
n

 |G|

=

s nm|G|

1I

gi t,x



s n

k

,

- n  k  n.

i=1

We sample m templates independently according to the Gaussian sampling with rejection, tj, j = 1 . . . m. We are now ready to define the random feature map :

sk (x) =  x, tj, n

 R(2n+1)xm.
j=1...m,k=-n...n

It is easy to see that:

mn

sk

lim
n

Et,g

(x), (z)

R(2n+1)xm

=

lim
n

Et,g

 x, tj, n

j=1 k=-n

sk  z, tj, n

= Ks(x, z).

In Section 3 we study the geometric information captured by this kernel by stating explicitly the similarity it computes.

Remark 2 (Efficiency of the representation). 1) The main advantage of such a feature map, as outlined in [1], is that we store transformed templates in order to compute , while if we wanted to compute an invariant kernel of type K (Equation (1)), we would need to explicitly transform the points. The latter is computationally expensive. Storing transformed templates and computing the signature  is much more efficient. It falls in the category of memory-based learning, and is biologically plausible [1]. 2) As |G|,m,n get large enough, the feature map  approximates a group-invariant Kernel, as we will see in next section.

3 An Equivalent Expected Kernel and a Uniform Concentration Result

In this section we present our main results, with proofs given in the supplementary material . Theo-

rem 1 shows that the random feature map , defined in the previous section, corresponds in expec-

tation to a group-invariant Haar-integration kernel Ks(x, z). Moreover, s - Ks(x, z) computes the average pairwise distance between all points in the orbits of x and z, where the orbit is defined as

the collection of all group-transformations of a given point x : Ox = {gx, g  G}.

Theorem 1 (Expectation). Let   (0, 1) and x, z  X . Define the distance dG between the orbits

Ox and Oz:

dG(x, z) =  1 2d

G

G

gx - g z 2 d(g)d(g ),

and the group-invariant expected kernel

s

Ks(x,

z)

=

lim
n

Et,g

(x), (z) R(2n+1)xm = Et

(x, t,  )(z, t,  )d, s = 1 + .
-s

4

1. The following inequality holds with probability 1:

 - 2(d, )  Ks(x, z) - (1 - dG(x, z))   + 1(d, ),

(3)

where 1(, d) =

e-d2 /16 d

-1

e-d/2 (1+)

d 2

2d

and 2(, ) =

e-d2 /16 d

+ (1 + )e-d2/8.

2. For any   (0, 1) as the dimension d   we have 1(, d)  0 and 2(, d)  0, and we have asymptotically Ks(x, z)  1 - dG(x, z) +  = s - dG(x, z).

3. Ks is symmetric and Ks is positive semi-definite.
Remark 3. 1) , 1(d, ), and 2(d, ) are not errors due to results holding with high probability but are due to the truncation and are a technical artifact of the proof. 2) Local invariance can be defined by restricting the sampling of the group elements to a subset G  G. Assuming that for each g  G, g-1  G, the equivalent kernel has asymptotically the following form:

Ks(x,

z)



s

-

1 2d

G

G

gx - g z 2 d(g)d(g ).

3) The norm-one constraint can be relaxed, let R = supxX x 2 < , hence we can set s = R(1 + ), and

-2(d, )  Ks(x, z) - (R(1 + ) - dG(x, z))  1(d, ),

(4)

where

1(,

d)

=

R e-d2/16
d

-R

e-d/2 (1+)

d 2

2d

and

2(, )

=

R e-d2/16
d

+

R(1 + )e-d2/8.

Theorem 2 is, in a sense, an invariant Johnson-Lindenstrauss [14] type result where we show that

the dot product defined by the random feature map  , i.e (x), (z) , is concentrated around the

invariant expected kernel uniformly on a data set of N points, given a sufficiently large number of templates m, a large number of sampled group elements |G|, and a large bin number n. The error

naturally decomposes to a numerical error 0 and statistical errors 1, 2 due to the sampling of the templates and the group elements respectively.

Theorem 2. [Johnson-Lindenstrauss type Theorem- N point Set] Let D = {xi | xi  X }Ni=1

be a finite dataset.

Fix 0, 1, 2, 1, 2



(0, 1).

For a number of bins n



1 0

,

templates

m



C1 21

log(

N 1

),

and group

elements

|G|



C2 22

log(

Nm 2

),

where

C1, C2

are

universal

numeric

constants,

we have:

| (xi), (xj) - Ks(xi, xj)|  0 + 1 + 2, i = 1 . . . N, j = 1 . . . N,

(5)

with probability 1 - 1 - 2.

Putting together Theorems 1 and 2, the following Corollary shows how the group-invariant random feature map  captures the invariant distance between points uniformly on a dataset of N points.

Corollary 1 (Invariant Features Maps and Distances between Orbits). Let D = {xi | xi  X }Ni=1

be

a

finite dataset.

Fix

0, 



(0, 1).

For

a

number

of

bins n



3 0

,

templates

m



9C1 20

log(

N 

),

and

group

elements

|G|



9C2 20

log(

Nm 

),

where

C1, C2

are

universal

numeric

constants,

we

have:

 - 2(d, ) - 0  (xi), (xj) - (1 - dG(xi, xj))  0 +  + 1(d, ),

(6)

i = 1 . . . N, j = 1 . . . N , with probability 1 - 2.

Remark 4. Assuming that the templates are unitary and drawn form a general distribution p(t), the equivalent kernel has the following form:

Ks(x, z) =

d(g)d(g ) s - max( x, gt , z, g t )p(t)dt .

GG

Indeed when we use the gaussian sampling with rejection for the templates, the integral

max( x, gt , z, g t )p(t)dt is asymptotically proportional to g-1x - g ,-1z . It is interesting
2
to consider different distributions that are domain-specific for the templates and assess the number
of the templates needed to approximate such kernels. It is also interesting to find the optimal tem-
plates that achieve the minimum distortion in equation 6, in a data dependent way, but we will
address these points in future work.

5

4 Learning with Group Invariant Random Features

In this section, we show that learning a linear model in the invariant, random feature space, on a

training set sampled from the reduced core set X0, has a low expected risk, and generalizes to unseen

test points generated from the distribution on X = X0  GX0. The architecture of the proof follows

ideas from [15] and [16]. Recall that given an L-Lipschitz convex loss function V , our aim is to

minimize the expected risk given in Equation (2). Denote the CDF by (x, t,  ) = P( gt, x   ),

and

the empirical

CDF

by (x, t,  )

=

1 |G|

|G| i=1

1I

gi t,x

 .

Let

p(t)

be

the

distribution

of

templates

t. The RKHS defined by the invariant kernel Ks, Ks(x, z) =

s -s

(x,

t,



)(z

,

t,



)p(t)dtd

denoted HKs , is the completion of the set of all finite linear combinations of the form:

f (x) = iKs(x, xi), xi  X , i  R.
i

(7)

Similarly to [16], we define the following infinite-dimensional function space:

Fp = f (x) =

s
w(t,  )(x, t,  )dtd

| sup |w(t,  )|  C

.

-s ,t p(t)

Lemma 1. Fp is dense in HKs . For f  Fp we have EV (f ) = X0 yY V (yf (x))y(x)dX (x), where X0 is the reduced core set.

Since Fp is dense in HKs , we can learn an invariant decision function in the space Fp, instead

of learning in HKs . Let (x) =



x,

tj ,

sk n

. , and  are equivalent up to
j=1...m,k=-n...n

constants. We will approximate the set Fp as follows:



F

=

 f (x)

=



w, (x)

sm =
n

n
wj,k 

j=1 k=-n

sk x, tj, n

C

, tj  p, j = 1 . . . m |

w



. m

Hence, we learn the invariant decision function via empirical risk minimization where we restrict the function to belong to F, and the sampling in the training set is restricted to the core set X0. Note that with this function space we are regularizing for convenience the norm infinity of the weights
but this can be relaxed in practice to a classical Tikhonov regularization.

Theorem 3 (Learning with Group invariant features). Let S = {(xi, yi) | xi  X0, yi 

Y, i = 1 . . . N }, a training set sampled from the core set X0. Let fN = arg minfF EV (f ) =

1 N

N i=1

V

(yif (xi)).Fix



>

0,

then

EV

(fN

)



min
f Fp

EV

(f

)

+

2

1 N

4LsC + 2V (0) + LC

1 log
2

1 

+ 2sLC

1+

1 2 log

m

2sC m 2sC

+L

1 + 2 log

+,

|G|  n

with probability at least 1 - 3 on the training set and the choice of templates and group elements.

The proof of Theorem 3 is given in Appendix B. Theorem 3 shows that learning a linear model

in the invariant random feature space defined by  (or equivalently ), has a low expected

risk. More importantly, this risk is arbitrarily close to the optimal risk achieved in an infinite-

dimensional class of functions, namely Fp. The training set is sampled from the reduced core set X0, and invariant learning generalizes to unseen test points generated from the distribution on X = X0  GX0, hence the reduction in the sample complexity. Recall that Fp is dense in
the RKHS of the Haar-integration invariant Kernel, and so the expected risk achieved by a linear

model in the invariant random feature space is not far from the one attainable in the invariant RKHS. Note that the error decomposes into two terms. The first, O( 1 ), is statistical and it
N
depends on the training sample complexity N . The other is governed by the approximation error of
functions Fp, with functions in F, and depends on the number of templates m, number of group

elements

sampled

|G|,

the

number

of

bins

n,

and

has

the

following

form

O(

1 m

)

+O

log m |G|

+

1 n

.

6

5 Relation to Previous Work
We now put our contributions in perspective by outlining some of the previous work on invariant kernels and approximating kernels with random features. Approximating Kernels. Several schemes have been proposed for approximating a non-linear kernel with an explicit non-linear feature map in conjunction with linear methods, such as the Nystrom method [17] or random sampling techniques in the Fourier domain for translation-invariant kernels [15]. Our features fall under the random sampling techniques where, unlike previous work, we sample both projections and group elements to induce invariance with an integral representation. We note that the relation between random features and quadrature rules has been thoroughly studied in [18], where sharper bounds and error rates are derived, and can apply to our setting. Invariant Kernels. We focused in this paper on Haar-integration kernels [11], since they have an integral representation and hence can be represented with random features [18]. Other invariant kernels have been proposed: In [19] authors introduce transformation invariant kernels, but unlike our general setting, the analysis is concerned with dilation invariance. In [20], multilayer arccosine kernels are built by composing kernels that have an integral representation, but does not explicitly induce invariance. More closely related to our work is [21], where kernel descriptors are built for visual recognition by introducing a kernel view of histogram of gradients that corresponds in our case to the cumulative distribution on the group variable. Explicit feature maps are obtained via kernel PCA, while our features are obtained via random sampling. Finally the convolutional kernel network of [22] builds a sequence of multilayer kernels that have an integral representation, by convolution, considering spatial neighborhoods in an image. Our future work will consider the composition of Haar-integration kernels, where the convolution is applied not only to the spatial variable but to the group variable akin to [2].

6 Numerical Evaluation

In this paper, and specifically in Theorems 2 and 3, we showed that the random, group-invariant

feature map  captures the invariant distance between points, and that learning a linear model

trained in the invariant, random feature space will generalize well to unseen test points. In this

section, we validate these claims through three experiments. For the claims of Theorem 2, we

will use a nearest neighbor classifier, while for Theorem 3, we will rely on the regularized least

squares (RLS) classifier, one of the simplest algorithms for supervised learning. While our proofs

focus on norm-infinity regularization, RLS corresponds to Tikhonov regularization with square

loss. Specifically, for performing T -way classification on a batch of N training points in Rd,

summarized in the data matrix X  RNxd and label matrix Y  RNxT , RLS will perform the

optimization, minW RmxT

1 N

||Y

- (X)W ||2F

+ ||W ||2F

, where || * ||F is the Frobenius norm,

 is the regularization parameter, and  is the feature map, which for the representation described

in this paper will be a CDF pooling of the data projected onto group-transformed random templates.

All RLS experiments in this paper were completed with the GURLS toolbox [23]. The three

datasets we explore are:

Xperm (Figure 1): An artificial dataset consisting of all sequences of length 5 whose elements

come from an alphabet of 8 characters. We want to learn a function which assigns a positive value

to any sequence that contains a target set of characters (in our case, two of them) regardless of their

position. Thus, the function label is globally invariant to permutation, and so we project our data

onto all permuted versions of our random template sequences.

MNIST (Figure 2): We seek local invariance to translation and rotation, and so all random templates

are translated by up to 3 pixels in all directions and rotated between -20 and 20 degrees.

TIDIGITS (Figure 3): We use a subset of TIDIGITS consisting of 326 speakers (men, women,

children) reading the digits 0-9 in isolation, and so each datapoint is a waveform of a single word.

We seek local invariance to pitch and speaking rate [25], and so all random templates are pitch

shifted up and down by 400 cents and warped to play at half and double speed. The task is 10-way

classification with one class-per-digit. See [24] for more detail.

Acknowledgements: Stephen Voinea acknowledges the support of a Nuance Foundation Grant. This work was also supported in part by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216.

7

Xperm Sample Complexity RLS



Raw

Haar

CDF(25,10)

Bag-Of-Words CDF(25,1) CDF(25,25)

1.0

Xperm Sample Complexity 1 - NN



Raw CDF(25,1) Bag-Of-Words

0.9

0.8

Accuracy

0.7

0.6

0.5

0.4 10

100 1000

Number of Training Points Per Class

10 100 1000
Number of Training Points Per Class

Figure 1: Classification accuracy as a function of training set size, averaged over 100 random training samples at each size.  = CDF(n, m) refers to a random feature map with n bins and m templates. With 25 templates, the random feature map outperforms the raw features and a bag-ofwords representation (also invariant to permutation) and even approaches an RLS classifier with a Haar-integration kernel. Error bars were removed from the RLS plot for clarity. See supplement.

Accuracy

MNIST Accuracy RLS (1000 Points Per Class)

Bins 5 25
1.0

1.0

0.9 0.8 0.9

0.7
0.8 0.6

0.5 0.7
0.4

0.3 0.6 0.2

0.1 1

10 100
Number of Templates

0.5 10

MNIST Sample Complexity RLS
 Raw CDF(50,500)
100 1000
Number of Training Points Per Class

Figure 2: Left Plot) Mean classification accuracy as a function of number of bins and templates, averaged over 30 random sets of templates. Right Plot) Classification accuracy as a function of training set size, averaged over 100 random samples of the training set at each size. At 1000 examples per class, we achieve an accuracy of 98.97%.

Accuracy

1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 10

TIDIGITS Speaker RLS Bins 5 25 100
100 1000
Number of Templates

TIDIGITS Gender RLS
Bins 5 25 100
10 100 1000
Number of Templates

Figure 3: Mean classification accuracy as a function of number of bins and templates, averaged over 30 random sets of templates. In the "Speaker" dataset, we test on unseen speakers, and in the "Gender" dataset, we test on a new gender, giving us an extreme train/test mismatch. [25].

8

References
[1] F. Anselmi, J. Z. Leibo, L. Rosasco, J. Mutch, A. Tacchetti, and T. Poggio, "Unsupervised learning of invariant representations in hierarchical architectures.," CoRR, vol. abs/1311.4158, 2013.
[2] J. Bruna and S. Mallat, "Invariant scattering convolution networks," CoRR, vol. abs/1203.1513, 2012.
[3] G. Hinton, A. Krizhevsky, and S. Wang, "Transforming auto encoders," ICANN-11, 2011.
[4] Y. Bengio, A. C. Courville, and P. Vincent, "Representation learning: A review and new perspectives," IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1798-1828, 2013.
[5] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," in Proceedings of the IEEE, vol. 86, pp. 2278-2324, 1998.
[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks.," in NIPS, pp. 1106-1114, 2012.
[7] P. Niyogi, F. Girosi, and T. Poggio, "Incorporating prior information in machine learning by creating virtual examples," in Proceedings of the IEEE, pp. 2196-2209, 1998.
[8] Y.-A. Mostafa, "Learning from hints in neural networks," Journal of complexity, vol. 6, pp. 192-198, June 1990.
[9] V. N. Vapnik, Statistical learning theory. A Wiley-Interscience Publication 1998.
[10] I. Steinwart and A. Christmann, Support vector machines. Information Science and Statistics, New York: Springer, 2008.
[11] B. Haasdonk, A. Vossen, and H. Burkhardt, "Invariance in kernel methods by haar-integration kernels.," in SCIA , Springer, 2005.
[12] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe, "Convexity, classification, and risk bounds," Journal of the American Statistical Association, vol. 101, no. 473, pp. 138-156, 2006.
[13] G. Wahba, Spline models for observational data, vol. 59 of CBMS-NSF Regional Conference Series in Applied Mathematics. Philadelphia, PA: SIAM, 1990.
[14] W. B. Johnson and J. Lindenstrauss, "Extensions of lipschitz mappings into a hilbert space.," Conference in modern analysis and probability, 1984.
[15] A. Rahimi and B. Recht, "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning.," in NIPS 2008.
[16] A. Rahimi and B. Recht, "Uniform approximation of functions with random bases," in Proceedings of the 46th Annual Allerton Conference, 2008.
[17] C. Williams and M. Seeger, "Using the nystrm method to speed up kernel machines," in NIPS, 2001.
[18] F. R. Bach, "On the equivalence between quadrature rules and random features," CoRR, vol. abs/1502.06800, 2015.
[19] C. Walder and O. Chapelle, "Learning with transformation invariant kernels," in NIPS, 2007.
[20] Y. Cho and L. K. Saul, "Kernel methods for deep learning," in NIPS, pp. 342-350, 2009.
[21] L. Bo, X. Ren, and D. Fox, "Kernel descriptors for visual recognition," in NIPS., 2010.
[22] J. Mairal, P. Koniusz, Z. Harchaoui, and C. Schmid, "Convolutional kernel networks," in NIPS, 2014.
[23] A. Tacchetti, P. K. Mallapragada, M. Santoro, and L. Rosasco, "Gurls: a least squares library for supervised learning," CoRR, vol. abs/1303.0934, 2013.
[24] S. Voinea, C. Zhang, G. Evangelopoulos, L. Rosasco, and T. Poggio, "Word-level invariant representations from acoustic waveforms," vol. 14, pp. 3201-3205, September 2014.
[25] M. Benzeghiba, R. De Mori, O. Deroo, S. Dupont, T. Erbes, D. Jouvet, L. Fissore, P. Laface, A. Mertins, C. Ris, R. Rose, V. Tyagi, and C. Wellekens, "Automatic speech recognition and speech variability: A review," Speech Communication, vol. 49, pp. 763-786, 01 2007.
9

