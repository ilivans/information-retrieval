Optimal Testing for Properties of Distributions
Jayadev Acharya, Constantinos Daskalakis, Gautam Kamath EECS, MIT
{jayadev, costis, g}@mit.edu
Abstract
Given samples from an unknown discrete distribution p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, and in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of discrete distributions such as monotonicity, independence, log-concavity, unimodality, and monotone-hazard rate, the optimal sample complexity is unknown. We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution p, and a known distribution q, are p and q close in 2-distance, or far in total variation distance? The optimality of our testers is established by providing matching lower bounds, up to constant factors. Finally, a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave, monotone hazard rate distributions.
1 Introduction
The quintessential scientific question is whether an unknown object has some property, i.e. whether a model from a specific class fits the object's observed behavior. If the unknown object is a probability distribution, p, to which we have sample access, we are typically asked to distinguish whether p belongs to some class C or whether it is sufficiently far from it.
This question has received tremendous attention in the field of statistics (see, e.g.,[1, 2]), where test statistics for important properties such as the ones we consider here have been proposed. Nevertheless, the emphasis has been on asymptotic analysis, characterizing the rates of convergence of test statistics under null hypotheses, as the number of samples tends to infinity. In contrast, we wish to study the following problem in the small sample regime:
(C, ): Given a family of distributions C, some  > 0, and sample access to an unknown distribution p over a discrete support, how many samples are required to distinguish between p  C versus dTV(p, C) > ?1
The problem has been studied intensely in the literature on property testing and sublinear algorithms [3, 4, 5], where the emphasis has been on characterizing the optimal tradeoff between p's support size and the accuracy  in the number of samples. Several results have been obtained, roughly clustering into three groups, where (i) C is the class of monotone distributions over [n], or more
1We want success probability at least 2/3, which can be boosted to 1 -  by repeating the test O(log(1/)) times and taking the majority.
1

generally a poset [6, 7]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [8, 9]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [8, 10, 11].
With respect to (iii), [11] exactly characterizes the number of samples required to test identity to each distribution q, providing a single tester matching this bound simultaneously for all q. Nevertheless, this tester and its precursors are not applicable to the composite identity testing problem that we consider. If our class C were finite, we could test against each element in the class, albeit this would not necessarily be sample optimal. If our class C were a continuum, we would need tolerant identity testers, which tend to be more expensive in terms of sample complexity [12], and result in substantially suboptimal testers for the classes we consider. Or we could use approaches related to generalized likelihood ratio test, but their behavior is not well-understood in our regime, and optimizing likelihood over our classes becomes computationally intense.

Our Contributions We obtain sample-optimal and computationally efficient testers for (C, ) for the most fundamental shape restrictions to a distribution. Our contributions are the following:

1. For a known distribution q over [n], and sample access to p, we show that distinguishing the acbraegstwuesme:ee(nnatp)thawnahtdesqhthoieswrastthtlheeaatsti2d2-edni,strtieatyqnucteeirsebtisentgw(reeeqnnu/iprea2sn)dsa(qmipnsl/east .2m)Aossastamcp2ol/ero2sl,(lpavrreyerv,swiuoseu(soblby)tasthhineowan1nadilnitse[tr1an1na]ct)ee.

2.

For the number

class C = Mdn of monotone distributions of samples, where prior work requires

over[nn]dlowg en/req6uirseamanpolepstimfoarl

 d

nd/2/2 = 1 and

 nd-1/2poly (1/) for d > 1 [6, 7]. Our results improve the exponent of n with respect

to d, shave all logarithmic factors in n, and improve the exponent of  by at least a factor of 2.

(a) A useful building block and interesting byproduct of our analysis is extending Birge's obliv-
ious decomposition for single-dimensional monotone distributions [13] to monotone distributions in d  1, and to the stronger notion of 2-distance. See Section C.1.
(b) Moreover, we show that O(logd n) samples suffice to learn a monotone distribution over [n]d in 2-distance. See Lemma 3 for the precise statement.

3. For the class C = d of product distributions over [n1] x * * * x [nd], our algorithm requires O ( n )1/2 + n /2 samples. We note that a product distribution is one where all

marginals are independent, so this is equivalent to testing if a collection of random variables are

all independent. In the case where n 's are large, then the first term dominates, and the sample

complexity is O(( n )1/2 /2). In particular, when d is a constant and all n 's are equal to n,

we achieve the optimal sample complexity of (nd/2/2). To the best of our knowledge, this

is the first result for d  3, and when d = 2, this improves the previously known complexity

from O

n 6

polylog(n/)

[8, 14], significantly improving the dependence on  and shaving all

logarithmic factors.

4.

For and

the classes C = LCDn, C = MHRn and unimodal distributions over [n], we require

C an

=optUimnaol flog-cno/nc2aven,ummobnerotoofnsea-mhapzlaersd.-Orauter

testers for LCDn and C = MHRn are to our knowledge the first for these classes for the

low sample regime we are studying--see [15] and its references for statistics literature on the

asymptotic regime. Our tester for Un improves the dependence of the sample complexity on  by

at least a factor of 2 in the exponent, and shaves all logarithmic factors in n, compared to testers

based on testing monotonicity.

(a) A useful building block and important byproduct of our analysis are the first computationally efficient algorithms for properly learning log-concave and monotone-hazard-rate distributions, to within  in total variation distance, from poly(1/) samples, independent of the domain size n. See Corollaries 4 and 6. Again, these are the first computationally efficient algorithms to our knowledge in the low sample regime. [16] provide algorithms for density estimation, which are non-proper, i.e. will approximate an unknown distribution from these classes with a distribution that does not belong to these classes. On the other hand, the statistics literature focuses on maximum-likelihood estimation in the asymptotic regime--see e.g. [17] and its references.

2

5. For all the above classes we obtain matching lower bounds, showing that the sample complexity of our testers is optimal with respect to n,  and when applicable d. See Section 8. Our lower bounds are based on extending Paninski's lower bound for testing uniformity [10].

Our Techniques At the heart of our tester lies a novel use of the 2 statistic. Naturally, the 2
and its related 2 statistic have been used in several of the afore-cited results. We propose a new use of the 2 statistic enabling our optimal sample complexity. The essence of our approach is to
first draw a small number of samples (independent of n for log-concave and monotone-hazard-rate
distributions and only logarithmic in n for monotone and unimodal distributions) to approximate the unknown distribution p in 2 distance. If p  C, our learner is required to output a distribution q that is O()-close to C in total variation and O(2)-close to p in 2 distance. Then some analysis
reduces our testing problem to distinguishing the following cases:

* p and q are O(2)-close in 2 distance; this case corresponds to p  C. * p and q are ()-far in total variation distance; this case corresponds to dTV(p, C) > .

We draw a comparison with robust identity testing, in which one must distinguish whether p and q are

c1-close or c2-far in total variation distance, for constants c2 > c1 > 0. In [12], Valiant and Valiant

show that (n/ log n) samples are required for this problem - a nearly-linear sample complexity,

which may be prohibitively large in many settings. In comparison, the problem we study tests for

2 closeness rather than total variation closeness: a relaxation of the previous problem. However,

oouf rOte(stenr/de2m).oOnsntrtahteesotthhaetr

this relaxation allows hand, this relaxation

us to achieve a substantially sublinear complexity is still tight enough to be useful, demonstrated by

our application in obtaining sample-optimal testers.

We note that while the 2 statistic for testing hypothesis is prevalent in statistics providing opti-
mal error exponents in the large-sample regime, to the best of our knowledge, in the small-sample regime, modified-versions of the 2 statistic have only been recently used for closeness-testing
in [18, 19] and for testing uniformity of monotone distributions in [20]. In particular, [18] design an unbiased statistic for estimating the 2 distance between two unknown distributions.

Organization In Section 4, we show that a version of the 2 statistic, appropriately excluding certain elements of the support, is sufficiently well-concentrated to distinguish between the above cases. Moreover, the sample complexity of our algorithm is optimal for most classes. Our base tester is combined with the afore-mentioned extension of Birge's decomposition theorem to test monotone distributions in Section 5 (see Theorem 2 and Corollary 1), and is also used to test independence of distributions in Section 6 (see Theorem 3).
In Section 7, we give our results on testing unimodal, log-concave and monotone hazard rate distributions. Naturally, there are several bells and whistles that we need to add to the above skeleton to accommodate all classes of distributions that we are considering. In Remark 1 we mention the additional modifications for these classes.

Related Work. For the problems that we study in this paper, we have provided the related works in the previous section along with our contributions. We cannot do justice to the role of shape restrictions of probability distributions in probabilistic modeling and testing. It suffices to say that the classes of distributions that we study are fundamental, motivating extensive literature on their learning and testing [21]. In the recent times, there has been work on shape restricted statistics, pioneered by Jon Wellner, and others. [22, 23] study estimation of monotone and k-monotone densities, and [24, 25] study estimation of log-concave distributions. Due to the sheer volume of literature in statistics in this field, we will restrict ourselves to those already referenced.
As we have mentioned, statistics has focused on the asymptotic regime as the number of samples tends to infinity. Instead we are considering the low sample regime and are more stringent about the behavior of our testers, requiring 2-sided guarantees. We want to accept if the unknown distribution is in our class of interest, and also reject if it is far from the class. For this problem, as discussed above, there are few results when C is a whole class of distributions. Closer related to our paper is the line of papers [6, 7, 26] for monotonicity testing, albeit these papers have sub-optimal sample complexity as discussed above. Testing independence of random variables has a long history in statisics [27, 28]. The theoretical computer science community has also considered the problem of

3

testing independence of two random variables [8, 14]. While our results sharpen the case where the variables are over domains of equal size, they demonstrate an interesting asymmetric upper bound when this is not the case. More recently, Acharya and Daskalakis provide optimal testers for the family of Poisson Binomial Distributions [29].
Finally, contemporaneous work of Canonne et al [30] provides a generic algorithm and lower bounds for the single-dimensional families of distributions considered here. We note that their algorithm has a sample complexity which is suboptimal in both n and , while our algorithms are optimal. Their algorithm also extends to mixtures of these classes, though some of these extensions are not computationally efficient. They also provide a framework for proving lower bounds, giving the optimal bounds for many classes when  is sufficiently large with respect to 1/n. In comparison, we provide these lower bounds unconditionally by modifying Paninski's construction [10] to suit the classes we consider.

2 Preliminaries

We use the following probability distances in our paper.

The total variation distance between distributions p and q is dTV(p, q) d=ef supA |p(A) - q(A)| =

1 2

p-q

1. The 2-distance between p and q over [n] is defined as 2(p, q) d=ef

i[n]

.(pi -qi )2
qi

The

Kolmogorov distance between two probability measures p and q over an ordered set (e.g., R) with

cumulative density functions Fp and Fq is dK(p, q) d=ef supxR |Fp(x) - Fq(x)|.

Our paper is primarily concerned with testing against classes of distributions, defined formally as:

Definition 1. Given   (0, 1] and sample access to a distribution p, an algorithm is said to test a class C if it has the following guarantees:

* If p  C, the algorithm outputs ACCEPT with probability at least 2/3; * If dTV(p, C)  , the algorithm outputs REJECT with probability at least 2/3.

We note the following useful relationships between these distances [31]:

Proposition 1.

dK(p, q)2

 dTV(p, q)2



1 4

2(p,

q).

Definition 2. An -effective support of a distribution p is any set S such that p(S)  1 - .

The flattening of a function f over a subset S is the function f such that fi = p(S)/|S|.
Definition 3. Let p be a distribution, and support I1, . . . is a partition of the domain. The flattening of p with respect to I1, . . . is the distribution p which is the flattening of p over the intervals I1, . . ..

Poisson Sampling Throughout this paper, we use the standard Poissonization approach. Instead of drawing exactly m samples from a distribution p, we first draw m  Poisson(m), and then draw m samples from p. As a result, the number of times different elements in the support of p occur in the sample become independent, giving much simpler analyses. In particular, the number of times we will observe domain element i will be distributed as Poisson(mpi), independently for each i. Since Poisson(m) is tightly concentrated around m, this additional flexibility comes only at a sub-constant cost in the sample complexity with an inversely exponential in m, additive increase in the error probability.

3 The Testing Algorithm - An Overview
Our algorithm for testing a class C can be decomposed into three steps. Near-proper learning in 2-distance. Our first step requires learning with very specific guarantees. Given sample access to p  C, we wish to output q such that (i) q is close to C in total variation distance, and (ii) p and q are O(2)-close in 2-distance on an -effective support2 of p. When
2We also require the algorithm to output a description of an effective support for which this property holds. This requirement can be slightly relaxed, as we show in our results for testing unimodality.

4

p is not in C, we do not guarantee anything about q. From an information theoretic standpoint, this problem is harder than learning the distribution in total variation, since 2-distance is more restrictive than total variation distance. Nonetheless, for the structured classes we consider, we are able to learn in 2 by modifying the approaches to learn in total variation.
Computation of distance to class. The next step is to see if the hypothesis q is close to the class C or not. Since we have an explicit description of q, this step requires no further samples from p, i.e. it is purely computational. If we find that q is far from the class C, then it must be that p  C, as otherwise the guarantees from the previous step would imply that q is close to C. Thus, if it is not, we can terminate the algorithm at this point.
2-testing. At this point, the previous two steps guarantee that our distribution q is such that:
- If p  C, then p and q are close in 2 distance on a (known) effective support of p;
- If dTV(p, C)  , then p and q are far in total variation distance. We can distinguish between these two cases using O(n/2) samples with a simple statistical 2test, that we describe in Section 4.
Using the above three-step approach, our tester, as described in the next section, can directly test monotonicity, log-concavity, and monotone hazard rate. With an extra trick, using Kolmogorov's max inequality, it can also test unimodality.

4 A Robust 2- 1 Identity Test

Our main result in this section is Theorem 1.
Theorem 1. Given   (0, 1], a class of probability distributions C, sample access to a distribution p, and an explicit description of a distribution q, both over [n] with the following properties:

Property 1.

dTV(q, C) 

 2

.

Property 2.

If p  C, then 2(p, q) 

2 500

.

Then there exists an algorithm such that: If p  C, it outputs ACCEPT with probability at least 2/3;

If of

dTV this

(aplg, Cor)ithm,airteoOutputsn/RE2J

EC
.

T

with

probability

at

least

2/3.

The

time

and

sample

complexity

Proof. Algorithm 1 describes a 2 testing procedure that gives the guarantee of the theorem.

Algorithm 1 Chi-squared testing algorithm

1: Input: ; an explicit distribution q; (Poisson) m samples from a distribution p, where Ni denotes

the number of occurrences of the ith domain element.

2: A  {i : qi  2/50n}

3: Z 

(Ni -mqi )2 -Ni

iA

mqi

4: if Z  m2/10 return close

5: else return far

In Section A we compute the mean and variance of the statistic Z (defined in Algorithm 1) as:

E [Z]

=

m*
iA

(pi

- qi)2 qi

=

m * 2(pA, qA),

Var [Z] =
iA

2

p2i qi2

+

4m

*

pi

*

(pi - qi2

qi)2

(1)

where by pA and qA we denote respectively the vectors p and q restricted to the coordinates in A, and we slightly abuse notation when we write 2(pA, qA), as these do not then correspond to probability distributions.

Lemma 1 demonstrates the separation in the means of the statistic Z in the two cases of interest, i.e., p  C versus dTV(p, C)  , and Lemma 2 shows the separation in the variances in the two cases. These two results are proved in Section B.

5

Lemma 1. If p  C, then E [Z]  m2/500. If dTV(p, C)  , then E [Z]  m2/5.

Lemma 2.

Let m



20000n/2.

If p



C

then Var [Z]



1 500000

m2

4.

If dTV(p, C)



, then

Var [Z]



1 100

E

[Z

]2.

Assuming Lemmas 1 and 2, Theorem 1 is now a simple application of Chebyshev's inequality.

When p  C, we have that E [Z] + 3 Var [Z]  1/500 + 3/500000 m2  m2/200. Thus,

Chebyshev's inequality gives

Pr Z  m2/10

 Pr Z  m2/200

 Pr

Z

-

E

[Z ]



 3

Var

[Z ]1/2

 1/3.

When dTV(p, C)  , E [Z] - 3 Var [Z]  1 - 3/100 E[Z]  3m2/20. Therefore,

Pr Z  m2/10

 Pr Z  3m2/20

 Pr

Z

-

E

[Z ]



 -3

Var

[Z ]1/2

 1/3.

This proves the correctness of Algorithm 1. For the running time, we divide the summation in Z
into the elements for which Ni > 0 and Ni = 0. When Ni = 0, the contribution of the term to the summation is mqi, and we can sum them up by subtracting the total probability of all elements appearing at least once from 1.

Remark 1. To apply Theorem 1, we need to learn distribution in C and find a q that is O(2)-close in 2-distance to p. For the class of monotone distributions, we are able to efficiently obtain such
a q, which immediately implies sample-optimal learning algorithms for this class. However, for
some classes, we may not be able to learn a q with such strong guarantees, and we must consider
modifications to our base testing algorithm.

For example, for log-concave and monotone hazard rate distributions, we can obtain a distribution q and a set S with the following guarantees:

- If p  C, then 2(pS, qS)  O(2) and p(S)  1 - O();

- If dTV(p, C)  , then dTV(p, q)  /2. In this scenario, the tester will simply pretend that the support of p and q is S, ignoring any samples and support elements in [n] \ S. Analysis of this

tester is extremely similar to Theorem 1. In particular, we can still show that the statistic Z will be

separated in the two cases. When p  C, excluding [n] \ S will only reduce Z. On the other hand,

when dTV(p, C)  , since p(S)  1 - O(), p and q must still be far on the remaining support, and

we can show that Z is this case with the same

ssatimll psluefficcoimenptlleyxiltayrgoef .OT(henre/fo2re).,

a

small

modification

allows

us

to

handle

For unimodal distributions, we are even unable to identify a large enough subset of the support where the 2 approximation is guaranteed to be tight. But we can show that there exists a light
enough piece of the support (in terms of probability mass under p) that we can exclude to make the 2 approximation tight. Given that we only use Chebyshev's inequality to prove the concentration
of the test statistic, it would seem that our lack of knowledge of the piece to exclude would involve a
union bound and a corresponding increase in the required number of samples. We avoid this through
a careful application of Kolmogorov's max inequality in our setting. See Theorem 7 of Section 7.

5 Testing Monotonicity
As the first application of our testing framework, we will demonstrate how to test for monotonicity. Let d  1, and i = (i1, . . . , id), j = (j1, . . . , jd)  [n]d. We say i j if il  jl for l = 1, . . . , d. A distribution p over [n]d is monotone (decreasing) if for all i j, pi  pj3. We follow the steps in the overview. The learning result we show is as follows (proved in Section C).
3This definition describes monotone non-increasing distributions. By symmetry, identical results hold for monotone non-decreasing distributions.

6

Lemma 3. Let d  1. There is an algorithm that takes m = O((d log(n)/2)d/2) samples from a

distribution p over [n]d, and outputs a distribution q such that if p is monotone, then with probability

at least 5/6, 2(p, q)



2 500

.

Furthermore, the distance of q to monotone distributions can be

computed in time poly(m).

This accomplishes the first two steps in the overview. In particular, if the distance of q from mono-
tone distributions is more than /2, we declare that p is not monotone. Therefore, Property 1 in
Theorem 1 is satisfied, and the lemma states that Property 2 holds with probability at least 5/6. We then proceed to the 2 - 1 test. At this point, we have precisely the guarantees needed to apply Theorem 1 over [n]d, directly implying our main result of this section:

Theorem 2. For any d  1, there exists an algorithm for testing monotonicity over [n]d with sample

complexity

O

nd/2 2 +

d log n 2

d

*

1 2

and time complexity O nd/2/2 + poly(log n, 1/)d .

In particular, this implies the following optimal algorithms for monotonicity testing for all d  1:

Corollary 1.

Fix any d 

1, and suppose 

>

 d log

n/n1/4.

Then there exists an algorithm for

testing monotonicity over [n]d with sample complexity O nd/2/2 .

We note that the class of monotone distributions is the simplest of the classes we consider. We now consider testing for log-concavity, monotone hazard rate, and unimodality, all of which are much more challenging to test. In particular, these classes require a more sophisticated structural understanding, more complex proper 2-learning algorithms, and non-trivial modifications to our 2-tester. We have already given some details on the required adaptations to the tester in Remark 1.
Our algorithms for learning these classes use convex programming. One of the main challenges is to enforce log-concavity of the PDF when learning LCDn (respectively, of the CDF when learning MHRn), while simultaneously enforcing closeness in total variation distance. This involves a careful choice of our variables, and we exploit structural properties of the classes to ensure the soundness of particular Taylor approximations. We encourage the reader to refer to the proofs of Theorems 7, 8, and 9 for more details.

6 Testing Independence of Random Variables

Let X d=ef [n1] x . . . x [nd], and let d be the class of all product distributions over X . Similar to learning monotone distributions in 2 distance we prove the following result in Section E.

Lemma 4. There is an algorithm that takes O (

d =1

n

)/2

samples from a distribution p and

outputs a q  d such that if p  d, then with probability at least 5/6, 2(p, q)  O(2).

The distribution q always satisfies Property 1 since it is in d, and by this lemma, with probability at least 5/6 satisfies Property 2 in Theorem 1. Therefore, we obtain the following result.

Theorem 3. For any d  1, there exists an algorithm for testing independence of random variables

over [n1] x . . . [nd] with sample and time complexity O

(

d =1

n

)1/2

+

d =1

n

/2 .

When d = 2 and n1 = n2 = n this improves the result of [8] for testing independence of two random variables.
Corollary 2. Testing if two distributions over [n] are independent has sample complexity (n/2).

7 Testing Unimodality, Log-Concavity and Monotone Hazard Rate
Unimodal distributions over [n] (denoted by Un) are all distributions p for which there exists an i such that pi is non-decreasing for i  i and non-increasing for i  i. Log-concave distributions over [n] (denoted by LCDn), is the sub-class of unimodal distributions for which pi-1pi+1  p2i .

7

Monotone hazard rate (MHR) distributions over [n] (denoted by MHRn), are distributions p with

CDF F

for which i

<

j

implies

fi 1-Fi



fj 1-Fj

.

The following theorem bounds the complexity of testing these classes (for moderate ).

Theorem 4. Suppose  > n-1/5. exists an algorithm for testing the

cFlaosrseoavcehro[fnt]hweicthlassasmesp, luenciommopdlaelx, iltoygO-c(oncna/ve2,)a. nd

MHR,

there

This result is a corollary of the specific results for each class, which is proved in the appendix. In particular, a more complete statement for unimodality, log-concavity and monotone-hazard rate, with precise dependence on both n and  is given in Theorems 7, 8 and 9 respectively. We mention some key points about each class, and refer the reader to the respective appendix for further details.
TtoouenrsitcliointwygetUro nbgoiimvuenoddana(alaintldygoarUsithwsimnegwwaiiltlhudnOeiomnobnnostulronagtden,a/trhgeu2 mtrsueanemt,cpoolmensep. lceHaxonitwyuesovefertt,hhteihs irpsersiousblutlsenmsoan)tiistsefasctitnnog/rym,2s.oinnWcoeeovercome the logarithmic barrier introduced by the union bound, by employing a non-oblivious decomposition of the domain, and using Kolmogorov's max-inequality.
Testing Log-Concavity The key step is to design an algorithm to learn a log-concave distribution in 2 distance. We formulate the problem as a linear program in the logarithms of the distribution and show that using O(1/5) samples, it is possible to output a log-concave distribution that has a 2 distance at most O(2) from the underlying log-concave distribution.
Testing Monotone Hazard Rate For learning MHR distributions in 2 distance, we formulate a linear program in the logarithms of the CDF and show that using O(log(n/)/5) samples, it is possible to output a MHR distribution that has a 2 distance at most O(2) from the underlying MHR distribution.

8 Lower Bounds

We now prove sharp lower bounds for the classes of distributions we consider. We show that the
example studied by Paninski [10] to prove lower bounds on testing uniformity can be used to prove lower bounds for the classes we consider. They consider a class Q consisting of 2n/2 distributions defined as follows. Without loss of generality assume that n is even. For each of the 2n/2 vectors z0z1 . . . zn/2-1  {-1, 1}n/2, define a distribution q  Q over [n] as follows.

qi =

(1+z c) n
(1-z c) n

for i = 2 + 1 for i = 2 .

(2)

Each distribution in Q has a total variation distance c/2 from Un, the uniform distribution over

[n]. By choosing c to be an uniformly at random from Q

appropriate constant, Paninski [10] showed cannot be distinguished from Un with fewer

that than

adni/str2ibsuatmiopnlepsicwkietdh

probability at least 2/3.

Suppose C is a class of distributions such that (i) The uniform distribution Un is in C, (ii) For

appropriately Invoking [10]

chosen c, dTV(C, Q) immediately implies

that ,tethsteinngtetshteincglaCssisCnroetqeuaisreiesrth(annd/ist2i)ngsaumishpilnesg.

Un

from

Q.

The lower bounds for all the one dimensional distributions will follow directly from this construction, and for testing monotonicity in higher dimensions, we extend this construction to d  1, appropriately. These arguments are proved in Section H, leading to the following lower bounds for testing these classes:
Theorem 5.

* For any d  1, any algorithm for testing monotonicity over [n]d requires (nd/2/2) samples.

* For d  1, testing independence over [n1]x* * *x[nd] requires  (n1n2 . . . nd)1/2/2 samples. * Testing unimodality, log-concavity, or monotone hazard rate over [n] needs (n/2) samples.

8

References
[1] R. A. Fisher, Statistical Methods for Research Workers. Edinburgh: Oliver and Boyd, 1925. [2] E. Lehmann and J. Romano, Testing statistical hypotheses. Springer Science & Business Media, 2006. [3] E. Fischer, "The art of uninformed decisions: A primer to property testing," Science, 2001. [4] R. Rubinfeld, "Sublinear-time algorithms," in International Congress of Mathematicians, 2006. [5] C. L. Canonne, "A survey on distribution testing: your data is big, but is it blue," ECCC, 2015. [6] T. Batu, R. Kumar, and R. Rubinfeld, "Sublinear algorithms for testing monotone and unimodal distribu-
tions," in Proceedings of STOC, 2004. [7] A. Bhattacharyya, E. Fischer, R. Rubinfeld, and P. Valiant, "Testing monotonicity of distributions over
general partial orders," in ICS, 2011, pp. 239-252. [8] T. Batu, E. Fischer, L. Fortnow, R. Kumar, R. Rubinfeld, and P. White, "Testing random variables for
independence and identity," in Proceedings of FOCS, 2001. [9] N. Alon, A. Andoni, T. Kaufman, K. Matulef, R. Rubinfeld, and N. Xie, "Testing k-wise and almost
k-wise independence," in Proceedings of STOC, 2007. [10] L. Paninski, "A coincidence-based test for uniformity given very sparsely sampled discrete data." IEEE
Transactions on Information Theory, vol. 54, no. 10, 2008. [11] G. Valiant and P. Valiant, "An automatic inequality prover and instance optimal identity testing," in FOCS,
2014. [12] ----, "Estimating the unseen: An n/ log n-sample estimator for entropy and support size, shown optimal
via new CLTs," in Proceedings of STOC, 2011. [13] L. Birge, "Estimating a density under order restrictions: Nonasymptotic minimax risk," The Annals of
Statistics, vol. 15, no. 3, pp. 995-1012, September 1987. [14] R. Levi, D. Ron, and R. Rubinfeld, "Testing properties of collections of distributions," Theory of Com-
puting, vol. 9, no. 8, pp. 295-347, 2013. [15] P. Hall and I. Van Keilegom, "Testing for monotone increasing hazard rate," Annals of Statistics, pp.
1109-1137, 2005. [16] S. O. Chan, I. Diakonikolas, R. A. Servedio, and X. Sun, "Learning mixtures of structured distributions
over discrete domains," in Proceedings of SODA, 2013. [17] M. Cule and R. Samworth, "Theoretical properties of the log-concave maximum likelihood estimator of
a multidimensional density," Electronic Journal of Statistics, vol. 4, pp. 254-270, 2010. [18] J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, S. Pan, and A. T. Suresh, "Competitive classification and
closeness testing," in COLT, 2012, pp. 22.1-22.18. [19] S. Chan, I. Diakonikolas, G. Valiant, and P. Valiant, "Optimal algorithms for testing closeness of discrete
distributions," in Proceedings of SODA, 2014, pp. 1193-1203. [20] J. Acharya, A. Jafarpour, A. Orlitsky, and A. Theertha Suresh, "A competitive test for uniformity of
monotone distributions," in Proceedings of AISTATS, 2013, pp. 57-65. [21] R. E. Barlow, D. J. Bartholomew, J. M. Bremner, and H. D. Brunk, Statistical Inference under Order
Restrictions. New York: Wiley, 1972. [22] H. K. Jankowski and J. A. Wellner, "Estimation of a discrete monotone density," Electronic Journal of
Statistics, vol. 3, pp. 1567-1605, 2009. [23] F. Balabdaoui and J. A. Wellner, "Estimation of a k-monotone density: characterizations, consistency and
minimax lower bounds," Statistica Neerlandica, vol. 64, no. 1, pp. 45-70, 2010. [24] F. Balabdaoui, H. Jankowski, and K. Rufibach, "Maximum likelihood estimation and confidence bands
for a discrete log-concave distribution," 2011. [Online]. Available: http://arxiv.org/abs/1107.3904v1 [25] A. Saumard and J. A. Wellner, "Log-concavity and strong log-concavity: a review," Statistics Surveys,
vol. 8, pp. 45-114, 2014. [26] M. Adamaszek, A. Czumaj, and C. Sohler, "Testing monotone continuous distributions on high-
dimensional real cubes," in SODA, 2010, pp. 56-65. [27] J. N. Rao and A. J. Scott, "The analysis of categorical data from complex sample surveys," Journal of the
American Statistical Association, vol. 76, no. 374, pp. 221-230, 1981. [28] A. Agresti and M. Kateri, Categorical data analysis. Springer, 2011. [29] J. Acharya and C. Daskalakis, "Testing Poisson Binomial Distributions," in Proceedings of SODA, 2015,
pp. 1829-1840. [30] C. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld, "Testing shape restrictions of discrete
distributions," arXiv preprint arXiv:1507.03558, 2015. [31] A. L. Gibbs and F. E. Su, "On choosing and bounding probability metrics," International Statistical
Review, vol. 70, no. 3, pp. 419-435, dec 2002. [32] J. Acharya, A. Jafarpour, A. Orlitsky, and A. T. Suresh, "Efficient compression of monotone and m-modal
distributions," in ISIT, 2014. [33] S. Kamath, A. Orlitsky, D. Pichapati, and A. T. Suresh, "On learning distributions from their samples," in
COLT, 2015. [34] P. Massart, "The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality," The Annals of Probability,
vol. 18, no. 3, pp. 1269-1283, 07 1990.
9

A Moments of the Chi-Squared Statistic

We analyze the mean and variance of the statistic

Z=

(Xi - mqi)2 - Xi ,

iA

mqi

where each Xi is independently distributed according to Poisson(mpi).

We start with the mean:

E [Z] = E
iA

(Xi - mqi)2 - Xi mqi

= E Xi2 - 2mqiE [Xi] + m2qi2 - E [Xi]

iA

mqi

= m2p2i + mpi - 2m2qipi + m2qi2 - mpi

iA

mqi

=m

(pi - qi)2

iA

qi

= m * 2(pA, qA)

Next, we analyze the variance. Let i = E [Xi] = mpi and i = mqi.

1 Var [Z] = iA i2 Var

(Xi - i)2 + 2(Xi - i)(i - i) - (Xi - i)

1 = iA i2 Var

(Xi - i)2 + (Xi - i)(2i - 2i - 1)

1 = iA i2 E

(Xi - i)4 + 2(Xi - i)3(2i - 2i - 1) + (Xi - i)2(2i - 2i - 1)2 - 2i

=

iA

1 i2

[32i

+

i

+

2i(2i

-

2i

-

1)

+

i(2i

-

2i

-

1)2

-

2i ]

=

iA

1 i2

[22i

+

i

+

4i(i

-

i)

-

2i

+

i(4(i

-

i)2

-

4(i

-

i)

+

1)]

=

iA

1 i2

[22i

+

4i(i

-

i)2]

=
iA

2

p2i qi2

+ 4m *

pi

* (pi - qi)2 qi2

(3)

The third equality is by noting the random variable has expectation i and the fourth equality substitutes the values of centralized moments of the Poisson distribution.

B Analysis of our 2-Test Statistic

We first prove the key lemmas in the analysis of our 2-test.

Proof of Lemma 1: The former case is straightforward from (1) and Property 2 of q.

We turn to the latter case. Recall that A = {i : qi  2/50n}, and thus q(A)  2/50. We first show that

dTV(pA, qA)



6 25

,

where

pA

,

qA

are defined as above and

in our slight abuse of notation we

use dTV(pA, qA)

for

non-probability

vectors

to

denote

1 2

pA - qA

1.

Partitioning the support into A and A, we have

dTV(p, q) = dTV(pA, qA) + dTV(pA, qA).

(4)

We consider the following cases separately:

10

* p(A)  /2: In this case,

1 dTV(pA, qA) = 2

|pi

- qi|



1 (p(A) + q(A)) 2



1 2

 2 +
2 50

= 13 . 50

iA

Plugging

this

in

(4),

and

using

the

fact

that

dTV(p, q)





shows

that

dTV(pA, qA)



6 25

.

* p(A) > /2: In this case, by the reverse triangle inequality,

dTV(pA, qA)



1 (q(A) - p(A)) 2



1 ((1 - 2/50) - (1 - /2)) 2

=

6 . 25

By the Cauchy-Schwarz inequality,

Plugging in (1) proves the result.

2(pA,

qA)



4

dTV(pA, qA)2 q(A)



2 .

5

Proof of Lemma 2: We bound the terms of (1) separately, starting with the first.

2

iA

p2i qi2

=2
iA

(pi

- qi)2 qi2

+

2piqi - qi2

qi2

=2
iA

(pi

- qi)2 qi2

+

2qi(pi

- qi) qi2

+

qi2

 2n + 2
iA

(pi

- qi)2 qi2

+

2 (pi

- qi

qi)



4n

+

4

iA

(pi

- qi)2 qi2

 4n + 200n

(pi - qi)2

2 iA

qi

200n E[Z] = 4n + 2 m

 4n +

1

 nE [Z ]

100

(5) (6) (7) (8) (9)

(5) uses |A|



n,

(6)

is

the

AM-GM 

inequality,

the

(7)

uses

that

qi



 50n

for all i



A, (8) uses (1), and

(9)substitutes

a

value

m



20000

n 2

.

The second term can be similarly bounded:

4m
iA

pi(pi - qi)2 qi2



4m

p2i iA qi2

1/2

(pi - qi)4

iA

qi2

1/2

 4m

1

1/2

4n +

nE [Z ]

100

(pi - qi)4

iA

qi2

1/2

 4m

 2 n+

1 n1/4E[Z]1/2

10

=

 8n

+

2 n1/4E[Z]1/2

E [Z ]

5

(pi - qi)2

iA

qi

The first inequality is Cauchy-Schwarz, the second inequality uses (9), the third inequality uses the monotonicity of the p norms, and the equality uses (1).
11

Combining the two terms, we get

Var

[Z ]



4n

+

 9 nE

[Z ]

+

2 5

n1/4E

[Z ]3/2

.

We now consider the two cases in the statement of our lemma.

*

When p  

C, we know from Lemma 1 that E [Z]



1 500

m2

.

Combined with a choice of m



20000

n 2

and

the

above

expression

for

the

variance,

this

gives: 

Var [Z]



4 200002

m24

+

9 20000 *

m24 500

+

10 m24 12500000



1 m24. 500000



*

When

dTV(p, C)



,

Lemma

1

and

m



20000

n 2

give:

E [Z]



1 m2 5



 4000 n.

Combining this with our expression for variance we get:

Var [Z]



4 40002

E

[Z

]2

+

9 4000

E

[Z

]2

+

2 E [Z]2 5 4000



1 100

E

[Z

]2

.

C Details on Testing Monotonicity

In this section, we prove Lemma 3 necessary for our monotonicity testing result.
Our analysis starts with a structural lemma about monotone distributions. In [13], Birge showed that any monotone distribution p over [n] can be obliviously decomposed into O(log(n)/) intervals, such that the flattening p (recall Definition 3) of p over these intervals is -close to p in total variation distance. [32] extend this result, giving a bound between the 2-distance of p and p. We strengthen these results by extending them to monotone distributions over [n]d. In particular, we partition the domain [n]d of p into O((d log(n)/2)d) rectangles, and compare it with p, the flattening over these rectangles. The following result is proved in Section C.1.
Lemma 5. Let d  1. There is an oblivious decomposition of [n]d into O((d log(n)/2)d) rectangles such that for any monotone distribution p over [n]d, its flattening p over these rectangles satisfy 2(p, p)  2.

This effectively reduces the support size to logarithmic in n. At this point, we can apply the Laplace estimator (along the lines of [33]) and learn a q such that if p was monotone, then q will be O(2)-close in 2-distance.

The following result is proved in Section C.2.

Lemma 6. Let d  1, and p be a monotone distribution over [n]d. There is an algorithm which outputs a

distribution q such that E

2(p, q)



2 500

.

The

time

and sample

complexity

are

both

O((d log(n)/2)d/2).

Applying Markov's inequality gives the 2 distance guarantee in Lemma 3.

The final step before we apply our 2-tester is to compute the distance between q and Mdn. This subroutine is similar to the one introduced by [6]. The key idea is to write a linear program, which searches for any
distribution f which is close to q in total variation distance. We note that the desired properties of f (i.e.,
monotonicity, normalization, and -closeness to q) are easy to enforce as linear constraints. Note that the
linear program operates over the oblivious decomposition used in our structural result, so the complexity is polynomial in (d log(n)/)d, rather than the naive nd.

These results when combined, give precisely the guarantees of Lemma 3.

C.1 A Structural Result for Monotone Distributions on the Hypergrid

Birge [13] showed that any monotone distribution is estimated to a total variation  with a O(log(n)/)-
piecewise constant distribution. Moreover, the intervals over which the output is constant is independent of the
distribution p. This result, was strengthened to the Kullback-Leibler divergence by [32] to study the compression of monotone distributions. They upper bound the KL divergence by 2 distance and then bound the 2 distance. We extend this result to [n]d. We divide [n]d into bd rectangles as follows. Let {I1, . . . , Ib} be a
partition of [n] into consecutive intervals defined as:

|Ij | =

1 2(1 + )j-b/2

for

1



j



b 2

,

for

b 2

<

j



b.

12

For j = (j1, . . . , jd)  [b]d, let Ij d=ef Ij1 x Ij2 x . . . x Ijd .

The 2 distance between p and p can be bounded as



2(p, p) = 

p2i  - 1

j[b]d iIj pi



  p+j |Ij| - 1
j[b]d

For j = (j1, . . . , jd)  Slarge, let j = (j1, . . . , jb) be

ji =

ji ji - 1

if ji  b/2 + 1 otherwise.

We bound the expression above as follows.

Let T  [d] be any subset of d. Suppose the size of T is . Let T be the set of all j that satisfy ji = b/2 + 1 for i  T . In other words, over the dimensions determined by T , the value of the index is equal to d/2 + 1. The map j  j restricted to T is one-to-one, and since at most d - of the coordinates drop,

|Ij|  |Ij | * (1 + )d- .

Since there are coordinates that do not change, and each of them have 2(1 + ) coordinates, we obtain

pj  p-j * |Ij| * (2(1 + )) * (1 + )d-

jT

jT

= p-j * |Ij | * 2 (1 + )d.
jT

Since the mapping is one-to-one, the probability of observing as element in T is the probability of observing b/2 + 1 in coordinates, which is at most (2/(b + 2)) under any monotone distribution. Therefore,

pj 

2 b+2

jT

For any there are d choices for T . Therefore,

* 2 (1 + )d.

d
2(p, p) 

d

=0

= (1 + )d

4 (1 + )d - 1 b+2

4 1+

d
-1

b+2

= 1 +  + 4 + 4

d
-1

b+2 b+2

Recall that  = 2 log(n)/b > 1/b, implies that the expression above is at most (1 + 2)d - 1. This implies Lemma 5.

C.2 Monotone Learning

Our algorithm requires a distribution q satisfying the properties discussed earlier. We learn a monotone distribution from samples as follows.
Before proving this result, we prove a general result for 2 learning of arbitrary discrete distributions, adapting the result from [33]. For a distribution p, and a partition of the domain into b intervals I1, . . . , Ib, let pi = p(Ii)/|Ii| be the flattening of p over these intervals. We saw that for monotone distributions there exists a partition of the domain such that p is close to the underlying distribution in 2 distance.

Suppose we are given m samples from a distribution p and a partition I1, . . . , Ib. Let mj be the number of

samples that fall in Ij. For i  Ij, let

qi

d=ef

1 |Ij

|

mj m

+1 +b

.

13

Let Sj = iIj p2i . The expected 2 distance between p and q can be bounded as follows.


b
E 2(p, q) = 

m

j=1 iIj =0



m

(p(Ij )) (1 - p(Ij ))m-

(

+

p2i 1)/(|Ij

|(m

+

b))



-

1

m+b b =

Sj

m + 1 j=1 p(Ij )/|Ij |

m

m+1 +1

(p(Ij )) +1(1 - p(Ij ))m+1- +1

=0

=

m+b b

Sj

m + 1 j=1 p(Ij )/|Ij |

1 - (1 - p(Ij )m+1

-1

 m+b b

Sj

-1

m + 1 j=1 p(Ij )/|Ij |

= m + b 2(p, p) + 1 - 1 m+1

= m + b * 2(p, p) + b .

m+1

m+1

-1 (10)

Suppose  = O(log(n)/b), and b = O(d * log(n)/2). Then, by Lemma 5, 2(p, p)  2.

(11)

Combining this with (10) gives Lemma 3.

D Details on testing Unimodality

One striking feature of Birge's result is that the decomposition of the domain is oblivious to the samples, and therefore to the unknown distribution. However, such an oblivious decomposition will not work for the unimodal distribution, since the mode is unknown. Suppose we know where the mode of the unknown distribution might be, then the problem can be decomposed into monotone functions over two intervals. Therefore, in theory, one can modify the monotonicity testing algorithm by iterating over all the possible n modes. Indeed, by applying a union bound, it then follows that

Theorem 6. (Follows from [n] with sample complexity

OMonontolnoeg)nF/or2

> .

1/n1/4,

there

exists

an

algorithm

for

testing

unimodality

over

Our main result for testing unimodality is the following theorem. Tcohmepolreexmity7O. (Supnp/os2e). > n-1/4. Then there exists an algorithm for testing unimodality over [n] with sample

Recall that to circumvent Birge's decomposition, we want to decompose the interval into disjoint intervals such that the probability of each interval is about O(1/b), where b is a parameter, specified later. In particular we consider a decomposition of [n] with the following properties:

1. For each element i with probability at least 1/b, there is an I = {i}.

2. There are at most two intervals with p(I)  1/2b.

3. Every other interval I satisfies p(I) 

1 2b

,

2 b

.

Let I1, . . . , IL denote the partition of [n] corresponding to these intervals. Note that L = O(b).
Claim 1. There is an algorithm that takes O(b log b) samples and outputs I1, . . . , IL satisfying the properties above.

The first step in our algorithm is to estimate the total probability within each of these intervals. In particular,
Lemma 7. There is an algorithm that takes m = O(b log b/2) samples from a distribution p, and with probability at least 9/10 outputs a distribution q that is constant on each IL. Moreover, for any j such that p(Ij) > 1/2b, q(Ij)  (1  )p(Ij).

14

Proof. Consider any interval Ij with p(Ij)  1/2b. The number of samples NIj that fall in that interval is distributed Binomial(m , p(Ij). Then by Chernoff bounds for m > 12b log b/2,

Pr |NIj - m p(Ij)| > m p(Ij) 2 exp 2m p(Ij)/2

(12)



1 b2

,

(13)

where the last inequality uses the fact that p(Ij)  1/2b.

The next step is estimate the distance of q from Un. This is possible by a simple dynamic program, similar to the one used for monotonicity. If the estimated distance is more than /2, we output REJECT.
Our next step is to remove certain intervals. This will be to ensure that when the underlying distribution is unimodal, we are able to estimate the distribution multiplicatively over the remaining intervals. In particular, we do the following preprocessing step:

* A = . * For interval Ij,
- If q(Ij ) / ((1 - ) * q(Ij+1), (1 + ) * q(Ij+1)) OR q(Ij ) / ((1 - ) * q(Ij-1), (1 + ) * q(Ij-1)) ,
add Ij to A. * Add the (at most 2) intervals with mass at most 1/2b to A. * Add all intervals j with q(Ij)/|Ij| < /50n to A
If the distribution is unimodal, we can prove the following about the set of intervals Ac. Lemma 8. If p is unimodal then,

(14) (15)

* p(IAc )  1 - /25 - 1/b - O (log n/(b)) .

* Except at most one interval in Ac every other interval Ij satisfies,

p+j p-j

 (1 + ).

If this holds, then the 2 distance between p and q constrained to Ac, is at most 2. This lemma follows from the following result.

Lemma 9.

Let C

> 2.

For a unimodal distribution over [n], there are at most

4 log(50n/) C

intervals Ij that

satisfy

p+j p-j

< (1 + /C).

Proof.

To

the

contrary,

if there

are more

than

4 log(50n/) C

intervals,

then

at

least

half

of

them are

on one

side of the mode, however this implies that the ratio of the largest probability and smallest probability is at

least (1 + /C)j, and if j

>

2

log(50n/) C

,

is

at

least

50n/,

contradicting

that

we

have

removed

all

such

elements.

We have one additional pre-processing step here. We compute q(Ac) and if it is smaller than 1 - /25, we output REJECT.
Suppose there are L intervals in Ac. Then, except at most one interval in L we know that the 2 distance between p and q is at most 2 when p is unimodal, and the TV distance between p and q is at least /2 over Ac. We propose the following simple modification to take into account, the one interval that might introduce a high 2 distance in spite of having a small total variation. If we knew the interval, we can simply remove it and proceed. Since we do not know where the interval lies, we do the following.
1. Let Zj be the 2 statistic over the ith interval in Ac, computed with O(n/2) samples.
2. Let Zl be the largest among all Zj's. 3. If j,j=l Zj > m2/10, output REJECT.
4. Output ACCEPT.
The objective of removing the largest 2 statistic is our substitute for not knowing the largest interval. We now prove the correctness of this algorithm.

15

Case 1 p  U Mn: We only concentrate on the final step. The 2 statistic over all but one interval are at most c * m2, and the variance is bounded as before. Since we remove the largest statistic, the expected value of the new statistic is strictly dominated by that of these intervals. Therefore, the algorithm outputs ACCEPT with at least the same probability as if we removed the spurious interval.

Case 2 p / U Mn: This is the hard case to prove for unimodal distributions. We know that the 2 statistic is large in this case, and we therefore have to prove that it remains large even after removing the largest test
statistic Zl.

We invoke Kolmogorov's Maximal Inequality to this end.

Lemma 10 (Kolmogorov's Maximal Inequality). For independent zero mean random variables X1, . . . , XL with finite variance, let S = X1 + . . . X . Then for any  > 0,

Pr

max |S |  
1 L



1 2

* V ar (SL) .

(16)

As a corollary, it follows that Pr (max

|X

| > 2) 

1 2

* V ar (SL).

In the case we are interested in, we let Xi = Z - E [Z ]. Then, similar to the computations before, and the fact

that each Taking 

interval has = E SL -

a small m2/3

mass, it 2 /100,

follows that that the variance of the summation it follows that the statistic does not fall below to

is at n.

most This

E [Z ]2 /100. completes the

proof of Theorem 7.

E Learning product distributions in 2 distance

In this section we prove Lemma 4, thus proving Theorem 3. The proof is analogous to the proof for learning monotone distributions, and hinges on the following result of [33].

Given m samples from a distribution q over n elements, the add-1 estimator (Laplace estimator) q satisfies:

E

2(p, q)



n .

m+1

To handle the 2 distribution of product distributions, we first bound the 2-distance between product distributions in terms of the individual coordinates. Lemma 11. Let p = p1 x p2 . . . x pd, and q = q1 x q2 . . . x qd be two distributions in d. Then
d
2(p, q) = (1 + 2(p , q )) - 1.
=1

Proof. By the definition of 2-distance and exchanging the product and summation,

2(p, q) =

(pi - qi)2 =

p2i - 1 =

iX qi

iX qi


d

=1 i[n ]

pi

2 -1=

qi

d =1

1 + 2(p , q )

- 1.

Now, suppose p is a product distribution over X = [n1] x * * * x [nd]. We simply perform the add-1 estimation over each coordinate independently, giving a distribution q1 x * * * x qd. Since p is a product distribution the estimates in each coordinate is independent. Therefore, a simple application of the previous result and independence of the coordinates implies

d

E 2(p, q) =

1 + E 2(pl, ql) - 1

l=1

d


1+

nl

-1

m+1

l=1

 exp

l nl - 1,

m+1

where (17) follows from ex  1 + x. Using ex  1 + 2x for 0  x  1, we have

(17)

E

2(p, q)

 2 l nl , m+1

when m  l nl. Therefore, following an application of Markov's inequality, when m = (( Lemma 4 is proved.

(18) l nl)/2),

16

F Details on Testing Log-Concavity

Our main result for testing log-concavity is as follows:

TOheorne/m2

8. +

There exists 1/5 and time

an algorithm for complexity poly(n,

testing 1/).

log-concavity

over

[n]

with

sample

complexity

In particular, this implies the following optimal tester for this class: sCaomrpollelacroym3p.leSxuitpypOosen>/21/.n1/5. Then there exists an algorithm for testing log-concavity over [n] with

Our algorithm will fit into the structure of our general framework. We first perform a very particular type of learning algorithm, whose guarantees are summarized in the following lemma:
Lemma 12. Given  > 0 and sample access to p, there exists an algorithm such that:
- If p  LCDn, the algorithm outputs a distribution q  LCDn and an O()-effective support S of p such that 2(pS, qS)  2/500 with probability at least 5/6;
- If dTV(p, LCDn)  , the algorithm either outputs a distribution q  LCDn or REJECT. The sample complexity is O(1/5) and the time complexity is poly(n, 1/).
We note that as a corollary, one immediately obtains a O(1/5) proper learning algorithm for log-concave distributions. The result is immediate from the first item of Lemma 12 and Proposition 1. We can actually do a bit better - in the proof of Lemma 12, we partition [n] into intervals of probability mass (3/2). If one instead partitions into intervals of probability mass (/ log(1/)) and works directly with total variation distance instead of 2 distance, one can show that O(1/4) samples suffice.
Corollary 4. Given  > 0 and sample access to a distribution p  LCDn, there exists an algorithm which outputs a distribution q  LCDn such that dTV(p, q)  . The sample complexity is O(1/4) and the time complexity is poly(n, 1/).

Then, given the guarantees of Lemma 12, Theorem 8 follows from Theorem 14. The details of these results are presented in Section F.

It will suffice to prove Lemma 12.

Proof of Lemma 12: We first draw samples from p and obtain a O(1/3/2)-piecewise constant distribution f by appropriately flattening the empirical distribution. The proof is now in two parts. In the first part, we show that if p  LCDn then f will be close to p in 2 distance over its effective support. The second part involves proper learning of p. We will use a linear program on f to find a distribution q  LCDn. This distribution is such that if p  LCDn, then 2(p, q) is small, and otherwise the algorithm will either output some q  LCDn (with no other relevant guarantees) or REJECT.
We first construct f . Let p be the empirical distribution obtained by sampling O(1/5) samples from p.

The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality gives a generic algorithm for learning any distribution with respect to the Kolmogorov distance.

Lemma 13. (See [34]) Suppose we have n i.i.d. samples X1, . . . Xn from a distribution with CDF F . Let

Fn(x) d=ef

1 n

n i=1

1{Xi x}

be the empirical CDF. Then Pr[dK(F, Fn)



]



2e-2n2 .

In particular, if

n = ((1/2) * log(1/)), then Pr[dK(F, Fn)  ]  .

This implies that with probability at least 5/6, dK(p, p)  5/2/10. In particular, note that |pi -pi|  5/2/10. Condition on this event in the remainder of the proof.
Let a be the minimum i such that pi  3/2/5, and let b be the maximum i satisfying the same condition. Let M = {a, . . . , b} or  if a and b are undefined. By the guarantee provided by the DKW inequality, pi  3/2/10 for all i  M . Furthermore, pi  pi  3/2/10  (1  ) * pi. For each i  M , let fi = pi. We note that |M | = O(1/), so this contributes O(1/) constant pieces to f .
We now divide the rest of the domain into t intervals, all but constantly many of measure (3/2) (under p). This is done via the following iterative procedure. As a base case, set r0 = 0. Define Ij as [lj, rj], where lj = rj-1 + 1 and rj is the largest j  [n] such that p(Ij)  93/2/10. The exception is if Ij would intersect M - in this case, we "skip" M : set rj = a - 1 and lj+1 = b + 1. If such a j exists, denote it by
4To be more precise, we require the modification of Theorem 8 which is described in Section 4, in order to handle the case where the 2-distance guarantees only hold for a known effective support.

17

j. We note that p(Ij)  p(Ij) + 5/2/10  3/2. Furthermore, for all j except j and t, rj + 1  M , so

p(Ij )



93/2/10 - 3/2/5 - 5/2/10



33/2/5.

Observe that this lower bound implies that t



2 3/2

for 

sufficiently small.

Part 1. For this part of the algorithm, we only care about the guarantees when p  LCDn, so we assume this is the case.
For the domain [n] \ M , we let f be the flattening of p over the intervals I1, . . . It. To analyze f , we need a structural property of log-concave distributions due to Chan, Diakonikolas, Servedio, and Sun [16]. This essentially states that a log-concave distribution cannot have a sudden increase in probability.
Lemma 14 (Lemma 4.1 in [16]). Let p be a distribution over [n] that is non-decreasing and log-concave on [1, x]  [n]. Let I = [x, y] be an interval of mass P (I) =  , and suppose that the interval J = [1, x - 1] has mass p(J) =  > 0. Then
p(y)/p(x)  1 +  /.

Recall that any log-concave distribution is unimodal, and suppose the mode of p is at i0. We will first focus on
the intervals I1, . . . , ItL which lie entirely to the left of i0 and M . We will refer to Ij as Lj for all j  tL. Note that p is non-decreasing over these intervals.

The next steps to the analysis are as follows. First we show that the flattening of p over Lj is a multiplicative (1 + O(1/j)) estimate for each pi  Lj. Then, we show that flattening the empirical distribution p over Lj is a multiplicative (1 + O(1/j)) estimate of p(i) for each i  Lj. Finally, we exclude a small number of intervals (those corresponding to O() mass at the left and right side of the domain, as well as j) in order to get the 2
approximation we desire on an effective support.

* First, recall that p(Lj)  3/2 for all j. Also, letting Jj = [1, rj-1], we have that p(Jj)  (j - 1) *

33/2/5. Thus by Lemma 14, p(rj)  p(lj)(1 + 2/(j - 1)). Since the distribution is non-decreasing

in

Lj ,

the

flattening

p

of

p

is

such

that

p(i)



p(i)(1



2 j-1

)

for

all

i



Lj .

*

We have that hence p(i) 

p(Lj )  33/2/5, and p(Lj )  p(Lj )  5/2/10, so p(Lj )  p(Lj ) * (1 

p(i)

*

(1



 6

)

for

all

i



Lj .

Combining

with

the

previous

point,

we

have

that

 6

),

and

p(i)  p(i) *

1

2 3(j -

1)

+

 6

+

j

2 -

1

 p(i) *

1



11 3(j -

1)

.

A symmetric statement holds for the intervals that lie entirely to the right of i0 and M . We will refer to Ij as Rt-j for all j > tL.
To summarize, we have the following guarantees for the distribution f :

* For all i  M , f (i)  p(i) * (1  );

* For all i  Lj (except L1 and Lj ), f (i)  p(i) *

1



22 3j

;

* For all i  Rj (except R1), f (i)  p(i) *

1



22 3j

;

Note that, in particular, we have multiplicative estimates for all intervals, except those in L1, Lj , R1 and the

interval containing i0. Let S be the set of containing i0 Then, since each interval has

all intervals except probability mass at

Lj , most

Lj and O(3/2)

Rj for j  1/ , and theone and we are excluding O(1/ )

intervals, p(S) > 1 - O().

We now compute the 2-distance induced by this approximation for elements in S. For an element i  Lj  S,

we have

(f (i) - p(i))2  60p(i) . p(i) j2

Summing over all i  Lj  S gives

603/2

since

the

probability

mass

of

Lj

is

at

most

3/2.

j2 Summing

this

over

all

Lj

for

j



 1/ 

and

j

=

j

gives

2/3/2

603/2



1 j2

 603/2

j=1/ 

1

 1/ 

x2 dx

= 603/2()

as desired.

= O(2)

18

Part 2. To obtain a distribution q  LCDn, we write a linear program. We will work in the log domain, so our variables will be Qi, representing log q(i) for i  [n]. We will use Fi = log f (i) as parameters in our LP.
There will be no objective function, we simply search for a feasible point. Our constraints will be

Qi-1 + Qi+1  2Qi i  [n - 1]

Qi  0 i  [n]

log(1 + )  |Qi - Fi|  log(1 + ) for i  M

log 1 - 22 3j

 |Qi - Fi|  log

22 1+
3j

for

i



Lj , j



 1/ 

and

j

=

j

log 1 - 22 3j

 |Qi - Fi|  log

1 + 22 3j

 for i  Rj, j  1/ 

If we run the linear program, then after a rescaling and summing the error over all the intervals in the LP gives us that the distance between p and q to be O(2) 2-distance in a set S which has measure p(S)  1 - 4, as
desired.

If the linear program finds a feasible point, then we obtain a q  LCDn. Furthermore, if p  LCDn, this also

tells us that (after a rescaling of ), summing the error over all intervals implies that 2(pS, qS)



2 500

for a

known set S with p(S)  1 - O(), as desired. If M = , this algorithm works as described. The issue is if

M = , then we don't know when the L intervals end and the R intervals begin. In this case, we run O(1/)

LPs, using each interval as the one containing i0, and thus acting as the barrier between the L intervals (to its

left) and the R intervals (to its right). If p truly was log-concave, then one of these guesses will be correct and

the corresponding LP will find a feasible point.

G Details on MHR testing

In this section, we give our main result for testing for monotone hazard rate:

TOheorne/m29+.

There exists an algorithm for testing monotone hazard log(n/)/4 and time complexity poly(n, 1/).

rate

over

[n]

with

sample

complexity

This implies the following optimal tester for the class:

Corollary 5. over [n] with

sSaumppploesceom>plexiltoy gO(n/n)//n12/4. .

Then

there

exists

an

algorithm

for

testing

monotone

hazard

rate

We obey the same framework as before, first applying a 2-learner with the following guarantees: Lemma 15. Given  > 0 and sample access to p, there exists an algorithm such that: - If p  MHRn, the algorithm outputs a distribution q  MHRn and an O()-effective support S of p such that 2(pS, qS)  2/500 with probability at least 5/6; - If dTV(p, MHRn)  , the algorithm either outputs a distribution q  MHRn and a set S  [n] or REJECT. The sample complexity is O(log(n/)/4) and the time complexity is poly(n, 1/).

As with log-concave distributions, this implies the following proper learning result:
Corollary 6. Given  > 0 and sample access to a distribution p  MHRn, there exists an algorithm which outputs a distribution q  MHRn such that dTV(p, q)  . The sample complexity is O(log(n/)/4) and the time complexity is poly(n, 1/).

Proof of Lemma 15: As with log-concave distributions, our method for MHR distributions can be split into two parts. In the first step, if p  MHRn, we obtain a distribution q which is O(2)-close to p in 2 distance on a set A of intervals such that p(A)  1 - O(). q will achieve this by being a multiplicative (1 + O())
approximation for each element within these intervals. This step is very similar to the decomposition used for
unimodal distributions (described in Section D), so we sketch the argument and highlight the key differences.

The second step will be to find a feasible point in a linear program. If p  MHRn, there should always be a feasible point, indicating that q is close to a distribution in MHRn (leveraging the particular guarantees for our algorithm for generating q). If dTV(p, MHRn)  , there may or may not be a feasible point, but when there is, it should imply the existence of a distribution p  MHRn such that dTV(q, p)  /2.

The analysis will rely on the following lemma from [16], which roughly states that an MHR distribution is "almost" non-decreasing.

Lemma 16 (Lemma 5.1 in [16]). Let p be an MHR distribution over [n]. Let I = [a, b]  [n] be an interval,

and

R

=

[b

+ 1, n]

be

the

elements

to

the

right

of

I.

Let



=

p(I )/p(R).

Then

p(b

+

1)



1 1+

p(a).

19

Part 1.

As

before,

with

unimodal

distributions,

we

start

by

taking

O(

b

log 2

b

)

samples,

with

the

goal

of

par-

titioning the domain into intervals of mass approximately (1/b). First, we will ignore the left and rightmost

intervals of mass (). For all "heavy" elements with mass  (1/b), we consider them as singletons. We

note that Lemma 16 implies that there will be at most O(1/) contiguous intervals of such elements. The rest

of the domain is greedily divided (from left to right) into intervals of mass (1/b), cutting an interval short if

we reach one of the heavy elements. This will result in the guarantee that all but potentially O(1/) intervals

have (1/b) mass.

Next, similar to unimodal distributions, considering the flattened distribution, we discard all intervals for which the per-element probability is not within a (1  O()) multiplicative factor of the same value for both neighbor-
ing intervals. The claim is that all remaining intervals will have the property that the per-element probability is within a (1  O()) multiplicative factor of the true probability. This is implied by Lemma 16. If there
were a point in an interval which was above this range, the distribution must decrease slowly, and the next
interval would have a much larger per-element weight, thus leading to the removal of this interval. A similar
argument forbids us from missing an interval which contains a point that lies outside this range. Relying on
the fact that truncating the left and rightmost intervals eliminates elements with low probability mass, similar
to the unimodal case, one can show that we will remove at most log(n/)/ intervals, and thus a log(n/)/b probability mass. Choosing b = (2/ log(n/)) limits this to be O(), as desired. At this point, if p is indeed MHR, the multiplicative estimates guarantee that the result is O(2)-close in 2-distance among the remaining
intervals.

Part 2. We note that an equivalent condition for distribution f being MHR is log-concavity of log(1 - F ), where F is the CDF of f . Therefore, our approach for this part will be similar to the approach used for log-concave distributions.

Given the output distribution q from the previous part of this algorithm, our goal will be check if there exists an MHR distribution f which is O()-close to q. We will run a linear program with variables fi = log(1 - Fi). First, we ensure that f is a distribution. This can be done with the following constraints:

fi  0 fi  fi+1 fn = -

i  [n] i  [n - 1]

To ensure that f is MHR, we use the following constraint:

fi-1 + fi-1  2fi

i  [2, n - 1]

Now, ideally, we would like to ensure f and q are -close in total variation distance by ensuring they are pointwise within a multiplicative (1  ) factor of each other:

(1 - )  fi/qi  (1 + )

We note that this is a stronger condition than f and q being -close, but if p  MHRn, the guarantees of the previous step would imply the existence of such an f .
We have a separate treatment for the identified singletons (i.e., those with probability  1/b) and the remainder of the support. For each element qi identified to have  1/b mass, we add two constraints:

log((1 - b/2)(1 - Qi))  fi  log((1 + b/2)(1 - Qi))

log((1 - b/2)(1 - Qi-1))  fi-1  log((1 + b/2)(1 - Qi-1)) If we satisfy these constraints, it implies that

qi - b  fi  qi + b.

Since qi  1/b, this implies

(1 - )qi  fi  (1 + )qi

as desired.

Now, the remaining elements each have  1/b mass. For each such element qi, we create a constraint

(1

-

O())

1

qi - Qi-1

 fi-1 - fi



(1

+

O())

1

qi - Qi-1

Note that the middle term is

- log 1 - Fi = - log 1 - fi

 fi (1  2) ,

1 - Fi-1

1 - Fi-1

1 - Fi-1

20

where the second equality uses the Taylor expansion and the facts that fi  1/b and 1 - Fi-1   (since during the previous part, we ignored the rightmost O() probability mass). If we satisfy the desired constraints,

it implies that

fi



1 (1  2)

1 1

- -

Fi-1 Qi-1

(q

+ O())qi.

Since we are taking (1/4) samples and 1-Fi-1  (), Lemma 13 implies that fi is indeed a multiplicative (1  ) approximation for these points as well.

We note that all points which do not fall into these two cases make up a total of O() probability mass. Therefore, f may be arbitrary at these points and only incur O() cost in total variation distance.

If we find a feasible point for this linear program, it implies the existence of an MHR distribution within O() total variation distance. In this case, we continue to the testing portion of the algorithm. Furthermore, if p  MHRn, our method for generating q certifies that such a distribution exists, and we continue on to the testing portion of the algorithm.

H Details of the Lower Bounds

In C,

this section, for the class we show that dTV(C, Q)

of 

d,istthruibsuitmiopnlsyiQngdaeslocrwibeerdboinunddisocuf ssi(onno/nl2o)wfeorr

bounds and testing C.

a

class

of

interest

H.1 Monotone distributions

We first consider d = 1 and prove that for appropriately chosen c, any monotone distribution over [n] is -far from all distributions in Q. Consider any q  Q. For this distribution, we say that i  [n] is a raise-point if qi < qi+1. Let Rq be the set of raise points of q. For q  Q, (2) implies at least one in every four consecutive integers in [n] is a raise point, and therefore, |Rq|  n/4. Moreover, note that if i is a raise-point, then i + 1 is not a raise point. For any monotone (decreasing) distribution p, pi  pi+1. For any raise-point i  Rq, by the
triangle inequality,

|pi

-

qi|

+

|pi+1

-

qi+1|



|pi

-

pi+1

+

qi+1

-

qi|



qi+1

-

qi

=

2c .
n

(19)

Summing over the set Rq, we obtain dTV(p, q) 

1 2

|Rq

|

*

2c n

 c/4.

Therefore, if c  4, then

dTV(Mn, q)  . This proves the lower bound for d = 1.

This argument can be extended to [n]d. Consider the following class of distributions on [n]d. For each point

i = (i1, . . . , id)  [n]d, where i1 is even, generate a random z  {-1, 1}, and assign to i a probability of

(1 + zc)/nd. Let e1 d=ef (1, 0, . . . , 0). Similar to d = 1, assign a probability (1 - zc)/nd to the point

nd/2
i + e1 = (i1 + 1, i2, . . . , id). This class consists of 2 2 distributions, and Paninski's arguments extend to

give a lower bound of (nd/2/2) samples to distinguish this class from the uniform distribution over [n]d. It

remains to show that all these distributions are  far from Mdn. Call a point i as a raise point if pi < pi+e1 . For any i, one of the points i, i + e1, i + 2e1, i + 3e1 is a raise point, and the number of raise points is at least nd/4.

Invoking the triangle inequality (identical to (19)) over the raise-points, in the first dimension shows that any

monotone

distribution

over

[n]d

is

at

a

distance

c 4

from

any

distribution

in

this

class.

Choosing

c

=

4

yields

a

bound of .

H.2 Testing Product Distributions
Our idea for testing independence is similar to the previous section. We sketch the construction of a class of distributions on X = [n1] x * * * x [nd]. Then |X | = n1 * n2 . . . * nd. For each element in X assign a value (1  c) and then for each such assignment, normalize the values so that they add to 1, giving rise to a distribution. This gives us a class of 2|X| distributions. The key argument is to show that a large fraction of these distributions are far from being a product distribution. This follows since the degrees of freedom of a product distribution is exponentially smaller than the number of possible distributions. The second step is to simply apply Paninski's argument, now over the larger set of distributions, where we show that distinguishing the collection of distributions we constructed from the uniform distribution over X (which is a product distribution) requires |X |/2 samples.

H.3 Log-concave and Unimodal distributions
We will show that any log-concave or unimodal distribution is -far from all distributions in Q. Since LCDn  Un, it will suffice to show this for every unimodal distribution. Consider any unimodal distribution p, with

21

mode . Then, p is monotone non-decreasing over the interval [ ] and non-increasing over { + 1, . . . , n}.

By the argument for monotone distributions, the total variation distance between p and any distribution q over

elements greater than

is

at

least

n- -1 n

c 4

,

and

over

elements

less

than

is at least

-1 n

c 4

.

Summing

these

two gives the desired bound.

H.4 Monotone Hazard distributions

We will show that any monotone hazard rate distribution is -far from all distributions in Q.

Let p be any monotone-hazard distribution. Any distribution q  Q has mass at least 1/2 over the interval

I = [n/4, 3n/4]. Therefore, by Lemma 16, for any i  I, pi+1

1

+

pi 1/4

 pi. As noted before, at least

n/8 of the raise-points are in I.

For any i  I  Rq, qi = (1 + c)/n, qi+1 = (1 - c)/n

di = |pi - qi| + |pi+1 - qi+1|.

(20)

If pi  (1 + 2c)/n or pi  1/n, then the first term, and therefore di is at least c/n. If pi  (1/n, (1 +

2c)/n), then for n > 5/(c)

pi+1 

1 n

*

1

1 +

4 n



1 - c/2 .
n

Therefore the second term of di is at c/2n. Since there are at least n/8 raise points in I,

dTV(p, q)



1 2

n 8

*

c 2n



c . 16

(21)

Thus any MHR distribution is -far from Q for c  16.

22

