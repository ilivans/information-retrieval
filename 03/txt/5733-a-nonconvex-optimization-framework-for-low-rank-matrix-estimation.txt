A Nonconvex Optimization Framework for Low Rank Matrix Estimation

Tuo Zhao Johns Hopkins University

Zhaoran Wang

Han Liu

Princeton University

Abstract
We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation, nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation. However, the understanding of its theoretical guarantees are limited. In this paper, we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences of this general framework for matrix sensing. In particular, we prove that a broad class of nonconvex optimization algorithms, including alternating minimization and gradient-type methods, geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.
1 Introduction
Let M  2 Rmn be a rank k matrix with k much smaller than m and n. Our goal is to estimate M  based on partial observations of its entires. For example, matrix sensing is based on linear measurements hAi, M i, where i 2 {1, . . . , d} with d much smaller than mn and Ai is the sensing matrix. In the past decade, significant progress has been established on the recovery of low rank matrix [4, 5, 23, 18, 15, 16, 12, 22, 7, 25, 19, 6, 14, 11, 13, 8, 9, 10, 27]. Among all these existing works, most are based upon convex relaxation with nuclear norm constraint or regularization. Nevertheless, solving these convex optimization problems can be computationally prohibitive in high dimensional regimes with large m and n [27]. A computationally more efficient alternative is nonconvex optimization. In particular, we reparameterize the m  n matrix variable M in the optimization problem as U V > with U 2 Rmk and V 2 Rnk, and optimize over U and V . Such a reparametrization automatically enforces the low rank structure and leads to low computational cost per iteration. Due to this reason, the nonconvex approach is widely used in large scale applications such as recommendation systems [17]. Despite the superior empirical performance of the nonconvex approach, the understanding of its theoretical guarantees is relatively limited in comparison with the convex relaxation approach. Only until recently has there been progress on coordinate descent-type nonconvex optimization methods, which is known as alternating minimization [14, 8, 9, 10]. They show that, provided a desired initialization, the alternating minimization algorithm converges at a geometric rate to U  2 Rmk and V  2 Rnk, which satisfy M = U V >. Meanwhile, [15, 16] establish the convergence of gradient-type methods, and [27] further establish the convergence of a broad class of nonconvex algorithms including both gradient-type and coordinate descent-type methods. However, [15, 16, 27] only establish the asymptotic convergence for an infinite number of iterations, rather than the explicit rate of convergence. Besides these works, [18, 12, 13] consider projected gradient-type methods, which optimize over the matrix variable M 2 Rmn rather than U 2 Rmk and V 2 Rnk. These methods involve calculating the top k singular vectors of an m  n matrix at each iteration. For
Research supported by NSF IIS1116730, NSF IIS1332109, NSF IIS1408910, NSF IIS1546482-BIGDATA, NSF DMS1454377-CAREER, NIH R01GM083084, NIH R01HG06841, NIH R01MH102339, and FDA HHSF223201000072C.
1

k much smaller than m and n, they incur much higher computational cost per iteration than the aforementioned methods that optimize over U and V . All these works, except [27], focus on specific algorithms, while [27] do not establish the explicit optimization rate of convergence.

In this paper, we propose a general framework that unifies a broad class of nonconvex algorithms for low rank matrix estimation. At the core of this framework is a quantity named projected oracle divergence, which sharply captures the evolution of generic optimization algorithms in the presence of nonconvexity. Based on the projected oracle divergence, we establish sufficiently conditions under which the iteration sequences geometrically converge to the global optima. For matrix sensing, a direct consequence of this general framework is that, a broad family of nonconvex algorithms, including gradient descent, coordinate gradient descent and coordinate descent, converge at a geometric rate to the true low rank matrices U  and V . In particular, our general framework covers alternating minimization as a special case and recovers the results of [14, 8, 9, 10] under standard conditions. Meanwhile, our framework covers gradient-type methods, which are also widely used in practice [28, 24]. To the best of our knowledge, our framework is the first one that establishes exact recovery guarantees and geometric rates of convergence for a broad family of nonconvex matrix sensing algorithms.

To achieve maximum generality, our unified analytic framework significantly differs from previous

works. In detail, [14, 8, 9, 10] view alternating minimization as a perturbed version of the power

method. However, their point of view relies on the closed form solution of each iteration of alternating

minimization, which makes it hard to generalize to other algorithms, e.g., gradient-type methods.

Meanwhile, [27] take a geometric point of view. In detail, they show that the global optimum of the

optimization problem is the unique stationary point within its neighborhood and thus a broad class of

algorithms succeed. However, such geometric analysis of the objective function does not characterize

the convergence rate of specific algorithms towards the stationary point. Unlike existing analytic

frameworks, we analyze nonconvex optimization algorithms as perturbed versions of their convex

counterparts. For example, under our framework we view alternating minimization as a perturbed

version of coordinate descent on convex objective functions. We use the key quantity, projected oracle

divergence, to characterize such a perturbation effect, which results from the local nonconvexity

at intermediate solutions. This framework allows us to establish explicit rate of convergence in an

analogous way as existing convex optimization analysis.

Notation: For a vector v

=

(v1, . . . , vd)T

2 Rd, let the vector q

norm be kvkqq

=

P
j

vjq .

For

a

matrix A 2 Rmn, we use Aj = (A1j, ..., Amj)> to denote the j-th column of A, and Ai =

(Ai1, ..., nonzero

Ain)> to denote the singular values of A.

i-th We

row of A. Let max(A) and min(A) be define the following matrix norms: kAk2F

t=hePlajrgkeAstjakn22d,

smallest kAk2 =

max(A). Moreover, we B 2 Rmn, we define

define kAk to be the inner product

aths ehAsu,mBoi f=allPsini,gj uAlaijr

values of A. Given another matrix Bij. We define ei as an indicator

vector, where the i-th entry is one, and all other entries are zero. For a bivariate function f (u, v), we

define ruf (u, v) to be the gradient with respect to u. Moreover, we use the common notations of (*), O(*), and o(*) to characterize the asymptotics of two real sequences.

2 Problem Formulation and Algorithms

Let M  2 Rmn be the unknown low rank matrix of interest. We have d sensing matrices Ai 2 Rmn with i 2 {1, . . . , d}. Our goal is to estimate M  based on bi = hAi, M i in the high dimensional regime with d much smaller than mn. Under such a regime, a common assumption is rank(M ) = k  min{d, m, n}. Existing approaches generally recover M  by solving the following convex optimization problem

M

min
2Rmn

kM

k

subject to b = A(M ),

(2.1)

where b = [b1, ..., bd]> 2 Rd, and A(M ) : Rmn ! Rd is an operator defined as

A(M ) = [hA1, M i, ..., hAi, M i]> 2 Rd.

(2.2)

Existing convex optimization algorithms for solving (2.1) are computationally inefficient, in the sense

that they incur high per-iteration computational cost, and only attain sublinear rates of convergence to

the global optimum [14]. Instead, in large scale settings we usually consider the following nonconvex

2

optimization problem

min F(U, V ).
U 2Rmk,V 2Rnk

where

F (U,

V

)

=

1 2

kb

A(U V >)k22.

(2.3)

The reparametrization of M = U V >, though making the optimization problem in (2.3) nonconvex, significantly improves the computational efficiency. Existing literature [17, 28, 21, 24] has established convincing empirical evidence that (2.3) can be effectively solved by a board variety of gradient-based nonconvex optimization algorithms, including gradient descent, alternating exact minimization (i.e., alternating least squares or coordinate descent), as well as alternating gradient descent (i.e., coordinate gradient descent), which are shown in Algorithm 1.

It is worth noting the QR decomposition and rank k singular value decomposition in Algorithm 1 can be accomplished efficiently. In particular, the QR decomposition can be accomplished in O(k2 max{m, n}) operations, while the rank k singular value decomposition can be accomplished in O(kmn) operations. In fact, the QR decomposition is not necessary for particular update schemes, e.g., [14] prove that the alternating exact minimization update schemes with or without the QR decomposition are equivalent.

Algorithm 1 A family of nonconvex optimization algorithms for matrix sensing. Here (U , D, V )

KSVD(M ) is the rank k singular value decomposition of M . Here D is a diagonal matrix containing

the top k singular values of M in decreasing order, and U and V contain the corresponding top k left

acnodrrerisgphotnsdiinngguolarrthvoencotormrsaolfmMat.riHxearned(VR,VRiVs

) the

QR(V ) is the corresponding

QR decomposition, where upper triangular matrix.

V

is the

IP(FnUoaprr(u0:a)mtt,: D=e{tb(e0i0r},):di,.=S.V.t1.e,(,p0T{)sA)izie}1di=K,1STVotDal(nPumdi=b1ebrioAfii)te, rVat(i0o)ns

T V

(0) D (0) ,

U

(0)

U (0)D(0) 9

Alternating Exact Minimization : V (t+0.5) argminV F (U (t), V )

(V (t+1), RV(t+0.5))

QR(V (t+0.5))

Alternating Gradient Descent : V (t+0.5)

V (t) rV F (U (t), V (t))

(V (t+1), RV(t+0.5))

QR(V (t+0.5)), U (t)

U (t)RV(t+0.5)>

Gradient Descent : V (t+0.5) V (t) rV F (U (t), V (t))

(V (t+1), RV(t+0.5))

QR(V (t+0.5)), U (t+1)

U (t)RV(t+0.5)>

>>>>>>>>>= >>>>>>>>>; Updating V 9

Alternating Exact Minimization : U (t+0.5) argminU F (U, V (t+1))

(U (t+1), R(t+0.5)) QR(U (t+0.5)) U
Alternating Gradient Descent : U (t+0.5)

U (t) rU F (U (t), V (t+1))

(U (t+1), RU(t+0.5))

QR(U (t+0.5)), V (t+1)

V t+1RU(t+0.5)>

Gradient Descent : U (t+0.5) U (t) rU F (U (t), V (t))

End

(U (t+1), for

RU(t+0.5) )

QR(U (t+0.5)), V (t+1)

V tRU(t+0.5)>

>>>>>>>>>= >>>>>>>>>;

Updating U

Output: M (T ) U (T 0.5)V (T )> (for gradient descent we use U (T )V (T )>)

3 Theoretical Analysis

We analyze the convergence properties of the general family of nonconvex optimization algorithms illustrated in 2. Before we present the main results, we first introduce a unified analytic framework based on a key quantity named projected oracle divergence. Such a unified framework equips our theory with the maximum generality. Without loss of generality, we assume m  n throughout the rest of this paper.

3.1 Projected Oracle Divergence

We first provide an intuitive explanation for the success of nonconvex optimization algorithms, which forms the basis of our later proof for the main results. Recall that (2.3) is a special instance of the following optimization problem,

min f (U, V ).
U 2Rmk,V 2Rnk

(3.1)

A key observation is that, given fixed U , f (U, *) is strongly convex and smooth in V under suitable conditions, and the same also holds for U given fixed V correspondingly. For the convenience of

3

discussion, we summarize this observation in the following technical condition, which will be later verified for matrix sensing under suitable conditions.

Condition 3.1 (Strong Biconvexity and Bismoothness). There exist universal constants + > 0 and  > 0 such that

 2

kU 0

U k2F  f (U 0, V )

f (U, V )

hU 0

U, rU f (U, V )i



+ 2

kU

0

U k2F for all U, U 0,

 2

kV 0

V k2F  f (U, V 0)

f (U, V )

hV 0

V, rV

f (U, V

)i



+ 2

kV

0

V k2F for all V, V 0.

For the simplicity of discussion, for now we assume U  and V  are the unique global minimizers to

the generic optimization problem in (3.1). Assuming U  is given, we can obtain V  by

V  = argmin f (U , V ).

(3.2)

V 2Rnk

Condition 3.1 implies the objective function in (3.2) is strongly convex and smooth. Hence, we can

choose any gradient-based algorithm to obtain V . For example, we can directly solve for V  in

rV f (U , V ) = 0,

(3.3)

or iteratively solve for V  using gradient descent, i.e.,

V (t) = V (t 1) rV f (U , V (t 1)),

(3.4)

where  is the step size. For the simplicity of discussion, we put aside the renormalization issue for

now. In the example of gradient descent, by invoking classical convex optimization results [20], it is

easy to prove that

kV (t) V kF  kV (t 1) V kF for all t = 0, 1, 2, . . . ,

where  2 (0, 1) is a contraction coefficient, which depends on + and  in Condition 3.1.

However, the first-order oracle rV f (U , V (t 1)) is not accessible in practice, since we do not know

U . Instead, we only have access to rV f (U, V (t 1)), where U is arbitrary. To characterize the

divergence between the ideal first-order oracle rV f (U , V (t 1)) and the accessible first-order oracle

rV

f

(U,

V

(t

1)), D(V,

we define V 0, U) =

arkeVyfq(uUant,iVty0n) amredVpfro(Uje,cVted0 ),oVracleVdiv/e(rkgVence,VwhkiFch)t,akes

the

form (3.5)

where V 0 is the point for evaluating the gradient. In the above example, it holds for V 0 = V (t 1). Later we will illustrate that, the projection of the difference of first-order oracles onto a specific one dimensional space, i.e., the direction of V V , is critical to our analysis. In the above example of

gradient descent, we will prove later that for V (t) = V (t 1) rV f (U, V (t 1)), we have

kV (t) V kF  kV (t 1) V kF + 2/+ * D(V (t), V (t 1), U ).

(3.6)

In other words, the projection of the divergence of first-order oracles onto the direction of V (t) V 

captures the perturbation effect of employing the accessible first-order oracle rV f (U, V (t 1)) instead of the ideal rV f (U , V (t 1)). For V (t+1) = argminV f (U, V ), we will prove that

kV (t) V kF  1/ * D(V (t), V (t), U ).

(3.7)

According to the update schemes shown in Algorithm 1, for alternating exact minimization, we set

U = U (t) in (3.7), while for gradient descent or alternating gradient descent, we set U = U (t 1) or

U = U (t) in (3.6) respectively. Correspondingly, similar results hold for kU (t) U kF.

To establish the geometric rate of convergence towards the global minima U  and V , it remains to establish upper bounds for the projected oracle divergence. In the example of gradient decent we will prove that for some  2 (0, 1 ),

2/+ * D(V (t), V (t 1), U (t 1))  kU (t 1) U kF, which together with (3.6) (where we take U = U (t 1)) implies

kV (t) V kF  kV (t 1) V kF + kU (t 1) U kF.

(3.8)

Correspondingly, similar results hold for kU (t) U kF, i.e.,
kU (t) U kF  kU (t 1) U kF + kV (t 1) Combining (3.8) and (3.9) we then establish the contraction

V kF.

(3.9)

max{kV (t) V kF, kU (t) U kF}  ( + ) * max{kV (t 1) V kF, kU (t 1) U kF},

4

which further implies the geometric convergence, since  2 (0, 1 ). Respectively, we can establish similar results for alternating exact minimization and alternating gradient descent. Based upon such a unified analytic framework, we now simultaneously establish the main results. Remark 3.2. Our proposed projected oracle divergence is inspired by previous work [3, 2, 1], which analyzes the Wirtinger Flow algorithm for phase retrieval, the expectation maximization (EM) Algorithm for latent variable models, and the gradient descent algorithm for sparse coding. Though their analysis exploits similar nonconvex structures, they work on completely different problems, and the delivered technical results are also fundamentally different.

3.2 Matrix Sensing Before we present our main results, we first introduce an assumption known as the restricted isometry property (RIP). Recall that k is the rank of the target low rank matrix M . Assumption 3.3. The linear operator A(*) : Rmn ! Rd defined in (2.2) satisfies 2k-RIP with parameter 2k 2 (0, 1), i.e., for all 2 Rmn such that rank( )  2k, it holds that
(1 2k)k k2F  kA( )k22  (1 + 2k)k k2F.

Several random matrix ensembles satisfy k-RIP for a sufficiently large d with high probability. For

example, suppose that each entry of Ai is independently drawn from a sub-Gaussian distribution,

A(*) satisfies 2k-RIP with parameter

2k

with high

probability for

d

=

(

2 2k

kn

log

n).

The following theorem establishes the geometric rate of convergence of the nonconvex optimization algorithms summarized in Algorithm 1.

Theorem 3.4. Assume there exists a sufficiently small constant C1 such that A(*) satisfies 2k-RIP with 2k  C1/k, and the largest and smallest nonzero singular values of M  are constants, which do not scale with (d, m, n, k). For any pre-specified precision , there exist an  and universal constants

C2 and C3 such that for all T C2 log(C3/), we have kM (T ) M kF  .

The proof of Theorems 3.4 is provided in Appendices 4.1, A.1, and A.2. Theorem 3.4 implies that all

three nonconvex optimization algorithms geometrically converge to the global optimum. Moreover,

assuming that each entry of Ai is independently drawn from a sub-Gaussian distribution with mean zero and variance proxy one, our result further suggests, to achieve exact low rank matrix recovery,

our algorithm requires the number of measurements d to satisfy

d = (k3n log n),

(3.10)

since we assume that 2k  C1/k. This sample complexity result matches the state-of-the-art result for nonconvex optimization methods, which is established by [14]. In comparison with their result,

which only covers the alternating exact minimization algorithm, our results holds for a broader variety

of nonconvex optimization algorithms.

Note that the sample complexity in (3.10) depends on a polynomial of max(M )/ min(M ), which is treated as a constant in our paper. If we allow max(M )/ min(M ) to increase with the dimension, we can plug the nonconvex optimization algorithms into the multi-stage framework proposed by [14]. Following similar lines to the proof of Theorem 3.4, we can derive a new sample complexity, which is independent of max(M )/ min(M ). See more details in [14].

4 Proof of Main Results

Due to space limitation, we only sketch the proof of Theorem 3.4 for alternating exact minimization.

The proof of Theorem 3.4 for alternating gradient descent and gradient descent, and related lemmas

are provided in the appendix. For notational simplicity, let 1 = max(M ) and k = min(M ). Before we proceed with the main proof, we first introduce the following lemma, which verifies

Condition 3.1.

Lemma 4.1. Suppose that A(*) satisfies 2k-RIP with parameter 2k. Given an arbitrary orthonormal matrix U 2 Rmk, for any V, V 0 2 Rnk, we have

1

+ 2

2k

kV

0

V k2F

F (U , V 0) F (U , V ) hrV F (U , V ), V 0

Vi

1 2 2k kV 0

V k2F.

The proof of Lemma 4.1 is provided in Appendix B.1. Lemma 4.1 implies that F(U , *) is strongly convex and smooth in V given a fixed orthonormal matrix U , as specified in Condition 3.1. Equipped with Lemma 4.1, we now lay out the proof for each update scheme in Algorithm 1.

5

4.1 Proof of Theorem 3.4 (Alternating Exact Minimization)

Proof. Throughout the proof of alternating exact minimization, we define a constant  2 (1, 1) for notational simplicity. We assume that at the t-th iteration, there exists a matrix factorization of

M  = U (t) D(V (t+0.5),

V V

(t)>, where U(t) is orthonormal. We (t+0.5), U (t))= rV F (U (t), V (t+0.5)

choose the ) rV F (U

projected oracle divergence

(t)

,

V

(t+0.5)),

V (t+0.5) kV (t+0.5)

V V

as
(t)
(t)kF

.

Remark 4.2. Note that the matrix factorization is not necessarily unique. Because given a factoriza-

tion of M  = U V >, we can always obtain a new factorization of M  = UeVe >, where Ue = U O and

Ve = V O for an arbitrary unitary matrix O 2 Rkk. However, this is not a issue to our convergence analysis. As will be shown later, we can prove that there always exists a factorization of M  satisfying the desired computational properties for each iteration (See Lemma 4.5, Corollaries 4.7 and 4.8).

The following lemma establishes an upper bound for the projected oracle divergence.

Lemma 4.3.

Sup2kposep4th2ak(t(112+k a2n2kdk))2U

(t)
k 1

satisfy and

kU (t)

U (t)kF



(1 4(1

+

2k )
2k

k
)

1

.

Then we have D(V (t+0.5), V (t+0.5), U (t))  (1

2k ) 2

k kU (t)

U (t)kF.

(4.1)

The proof of Lemma 4.3 is provided in Appendix B.2. Lemma 4.3 shows that the projected oracle divergence for updating V diminishes with the estimation error of U (t).The following lemma quantifies the progress of an exact minimization step using the projected oracle divergence.
Lemma 4.4. We have kV (t+0.5) V (t)kF  1/(1 2k) * D(V (t+0.5), V (t+0.5), U (t)).

The proof of Lemma 4.4 is provided in Appendix B.3. Lemma 4.4 illustrates that the estimation error of V (t+0.5) diminishes with the projected oracle divergence. The following lemma characterizes the effect of the renormalization step using QR decomposition.

Lemma 4.5. Suppose that V (t+0.5) satisfies

kV (t+0.5) V (t)kF  k/4.

(4.2)

Then there exists a factorization of M  = U (t+1)V (t+1) such that V (t+0.5) 2 Rnk is an orthonormal matrix, and satisfies kV (t+1) V (t+1)kF  2/ k * kV (t+0.5) V (t)kF.

The proof of Lemma 4.5 is provided in Appendix B.4. The next lemma quantifies the accuracy of the

initialization U (0).

Lemma 4.6. Suppose that 2k satisfies

2k



(1

2k )2

4 k

1922k(1 + 2k)2

4.
1

(4.3)

Then there exists a factorization of M  = U (0)V (0)> such that U (0) 2 Rmk is an orthonormal

matrix, and satisfies kU (0)

U kF



.(1 2k) k
4(1+ 2k) 1

The proof of Lemma 4.6 is provided in Appendix B.5. Lemma 4.6 implies that the initial solution U (0) attains a sufficiently small estimation error.

Combining the above Lemmas, we obtain the next corollary for a complete iteration of updating V .

Corollary 4.7. Suppose that 2k and U (t) satisfy

2k



(1

2k )2

4 k

1922k(1 + 2k)2

4 1

and kU (t)

U (t)kF



(1 4(1

+

2k )
2k

)

k

1

.

(4.4)

We then have kV (t+1)

V (t+1)kF



(1 2k) k 4(1+ 2k) 1

.

Moreover,

we

also

have

kV

(t+1)

1 

kU

(t)

U (t)kF and kV (t+0.5)

V (t)kF



k
2

kU

(t)

U (t)kF.

V (t+1)kF 

6

The proof of Corollary 4.7 is provided in Appendix B.6. Since the alternating exact minimization algorithm updates U and V in a symmetric manner, we can establish similar results for a complete iteration of updating U in the next corollary.

Corollary 4.8. Suppose that 2k and V (t+1) satisfy

2k



(1

2k )2

4 k

1922k(1 + 2k)2

4 1

and kV (t+1)

V (t+1)kF



(1 4(1

+

2k )
2k

k
)

1

.

(4.5)

Then there exists a factorization of M  = U (t+1)V (t+1)> such U (t+1) is an orthonormal matrix,

and satisfies kU (t+1)

U (t+1)kF



.(1 2k) k
4(1+ 2k) 1

Moreover,

we

also

have

kU (t+1)

1 

kV

(t+1)

V (t+1)kF and kU (t+0.5)

U (t+1)kF



k
2

kV

(t+1)

V (t+1)kF.

U (t+1)kF 

The proof of Corollary 4.8 directly follows Appendix B.6, and is therefore omitted.

We then proceed with the proof of Theorem 3.4 for alternating exact minimization. Lemma 4.6

ensures that (4.4) of Corollary 4.7 holds for U (0). Then Corollary 4.7 ensures that (4.5) of Corollary

4.8 holds for V (1). By induction, Corollaries 4.7 and 4.8 can be applied recursively for all T iterations.

Thus we obtain

kV (T )

V (T )kF



1 

kU

(T

1)

U (T

1)kF



1 2

kV

(T

1)

V (T 1)kF



***



1 2T

1 kU (0)

U (0)kF



(1 42T (1

2k) k + 2k)

,
1

(4.6)

where the lastlinequalitycomes fromLemma 4m.6. Therefore, for a pre-specified accuracy , we need

at most T =

1/2 * log

(1 2k) k 2(1+ 2k) 1

log 1 

iterations such that

kV (T ) Moreover, Corollary 4.8 implies

V (T )kF



(1 2k) k 42T (1 + 2k)

1



 2

.

(4.7)

kU (T 0.5)

U (T )kF



k
2

kV

(T

)

V (T )kF 

(1

2k )

2 k

82T +1(1 + 2k)

,
1

where

the

last

inequality

comTes=from1/2(4*.6lo).gTh4e(r1ef(o1re+2,kw)2ekk2n)eedloagt

most 1

iterations such that

kU (T 0.5)

U kF



(1 2k) 82T +1(1 +

2 k
2k )

1

 21 .

Then combining (4.7) and (4.8), we obtain

(4.8)

kM (T ) M k = kU (T 0.5)V (T )> U (T )V (T )>kF

 kV (T )k2kU (T 0.5) U (T )kF + kU (T )k2kV (T ) V (T )kF  , (4.9)

where the last inequality is from kV (T )k2 = 1 (since V (T ) is orthonormal) and kU k2 = kM k2 = 1 (since U (T )V (T )> = M  and V (T ) is orthonormal). Thus we complete the proof.

5 Extension to Matrix Completion

Under the same setting as matrix sensing, we observe a subset of the entries of M , namely, W  {1, . . . , m}  {1, . . . , n}. We assume that W is drawn uniformly at random, i.e., Mi,j is observed independently with probability  2 (0, 1]. To exactly recover M , a common assumption is the incoherence of M , which will be specified later. A popular approach for recovering M  is to solve the following convex optimization problem

M

min
2Rmn

kM

k

subject to PW (M ) = PW (M ),

(5.1)

where PW (M ) : Rmn ! Rmn is an operator defined as [PW (M )]ij = Mij if (i, j) 2 W, and 0 otherwise. Similar to matrix sensing, existing algorithms for solving (5.1) are computationally

7

inefficient. Hence, in practice we usually consider the following nonconvex optimization problem

min FW (U, V ),
U 2Rmk,V 2Rnk

where FW (U, V ) = 1/2 * kPW (M )

PW (U V >)k2F.

(5.2)

Similar to matrix sensing, (5.2) can also be efficiently solved by gradient-based algorithms. Due to

space limitation, we present these matrix completion algorithms in Algorithm 2 of Appendix D. For

the convenience of later convergence analysis, we partition the observation set W into 2T + 1 subsets

W0,...,W2T using Algorithm 4 in Appendix D. However, in practice we do not need the partition scheme, i.e., we simply set W0 = * * * = W2T = W.

Before we present the main results, we introduce an assumption known as the incoherence property.

Assumption 5.1. The target rank k matrix M  is incoherent with parameter , i.e., given the rank k

singular

value

decommpiaoxsiktiUoniokf2M=pUk/m

V

>, and

we have max kV
j

jk2



p  k/n.

The incoherence assumption guarantees that M  is far from a sparse matrix, which makes it feasible to complete M  when its entries are missing uniformly at random. The following theorem establishes the iteration complexity and the estimation error under the Frobenius norm.

Theorem 5.2. Suppose that there exists a universal constant C4 such that  satisfies

 C42k3 log n log(1/)/m,

(5.3)

where  is the pre-specified precision. Then there exist an  and universal constants C5 and C6 such

that for any T C5 log(C6/), we have kM (T ) M kF   with high probability.

Due to space limit, we defer the proof of Theorem 5.2 to the longer version of this paper. Theorem

5.2 implies that all three nonconvex optimization algorithms converge to the global optimum at a

geometric rate. Furthermore, our results indicate that the completion of the true low rank matrix M  up to -accuracy requires the entry observation probability  to satisfy

 = (2k3 log n log(1/)/m).

(5.4)

This result matches the result established by [8], which is the state-of-the-art result for alternating

minimization. Moreover, our analysis covers three nonconvex optimization algorithms.

6 Experiments
We present numerical experiments for matrix sensing to support our theoretical analysis. We choose m = 30, n = 40, and k = 5, and vary d from 300 to 900. Each entry of Ai's are independent sampled from N (0, 1). We then generate M = U V >, where Ue 2 Rmk and Ve 2 Rnk are two matrices with all their entries independently sampled from N (0, 1/k). We then generate d measurements by bi = hAi, M i for i = 1, ..., d. Figure 1 illustrates the empirical performance of the alternating exact minimization and alternating gradient descent algorithms for a single realization. The step size for the alternating gradient descent algorithm is determined by the backtracking line search procedure. We see that both algorithms attain linear rate of convergence for d = 600 and d = 900. Both algorithms fail for d = 300, because d = 300 is below the minimum requirement of sample complexity for the exact matrix recovery.

Estimation Error Estimation Error

100 100

d = 300

10-5

d = 600

d = 900

d = 300

d = 600

10-5

d = 900

0 10 20 30 Number of Iterations
(a) Alternating Exact Minimization

40

0 10 20 30 Number of Iterations
(b) Alternating Gradient Descent

40

Figure 1: Two illustrative examples for matrix sensing. The vertical axis corresponds to estimation error kM (t) M kF. The horizontal axis corresponds to numbers of iterations. Both the alternating exact minimization and alternating gradient descent algorithms attain linear rate of convergence for d = 600 and d = 900. But both algorithms fail for d = 300, because d = 300 is below the minimum requirement of sample complexity for the exact matrix recovery.

8

References
[1] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. arXiv preprint arXiv:1503.00778, 2015.
[2] Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. arXiv preprint arXiv:1408.2156, 2014.
[3] Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985-2007, 2015.
[4] Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9(6):717-772, 2009.
[5] Emmanuel J Candes and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053-2080, 2010.
[6] Yudong Chen. Incoherence-optimal matrix completion. arXiv preprint arXiv:1310.0154, 2013. [7] David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions on
Information Theory, 57(3):1548-1566, 2011. [8] Moritz Hardt. Understanding alternating minimization for matrix completion. In Symposium on Founda-
tions of Computer Science, pages 651-660, 2014. [9] Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz. Computational limits for matrix
completion. arXiv preprint arXiv:1402.2331, 2014. [10] Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. arXiv preprint
arXiv:1407.4070, 2014. [11] Trevor Hastie, Rahul Mazumder, Jason Lee, and Reza Zadeh. Matrix completion and low-rank SVD via
fast alternating least squares. arXiv preprint arXiv:1410.2596, 2014. [12] Prateek Jain, Raghu Meka, and Inderjit S Dhillon. Guaranteed rank minimization via singular value
projection. In Advances in Neural Information Processing Systems, pages 937-945, 2010. [13] Prateek Jain and Praneeth Netrapalli. Fast exact matrix completion with finite samples. arXiv preprint
arXiv:1411.1087, 2014. [14] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating
minimization. In Symposium on Theory of Computing, pages 665-674, 2013. [15] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries.
IEEE Transactions on Information Theory, 56(6):2980-2998, 2010. [16] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy entries.
Journal of Machine Learning Research, 11:2057-2078, 2010. [17] Yehuda Koren. The Bellkor solution to the Netflix grand prize. Netflix Prize Documentation, 81, 2009. [18] Kiryung Lee and Yoram Bresler. Admira: Atomic decomposition for minimum rank approximation. IEEE
Transactions on Information Theory, 56(9):4402-4416, 2010. [19] Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with noise and
high-dimensional scaling. The Annals of Statistics, 39(2):1069-1097, 2011. [20] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer, 2004. [21] Arkadiusz Paterek. Improving regularized singular value decomposition for collaborative filtering. In
Proceedings of KDD Cup and workshop, volume 2007, pages 5-8, 2007. [22] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,
12:3413-3430, 2011. [23] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM Review, 52(3):471-501, 2010. [24] Benjamin Recht and Christopher Re. Parallel stochastic gradient algorithms for large-scale matrix comple-
tion. Mathematical Programming Computation, 5(2):201-226, 2013. [25] Angelika Rohde and Alexandre B Tsybakov. Estimation of high-dimensional low-rank matrices. The
Annals of Statistics, 39(2):887-930, 2011. [26] Gilbert W Stewart, Ji-guang Sun, and Harcourt B Jovanovich. Matrix perturbation theory, volume 175.
Academic press New York, 1990. [27] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. arXiv preprint
arXiv:1411.8003, 2014. [28] Gabor Takacs, Istvan Pilaszy, Bottyan Nemeth, and Domonkos Tikk. Major components of the gravity
recommendation system. ACM SIGKDD Explorations Newsletter, 9(2):80-83, 2007.
9

