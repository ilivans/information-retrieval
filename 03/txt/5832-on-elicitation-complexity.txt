On Elicitation Complexity

Rafael Frongillo University of Colorado, Boulder
raf@colorado.edu

Ian A. Kash Microsoft Research
iankash@microsoft.com

Abstract
Elicitation is the study of statistics or properties which are computable via empirical risk minimization. While several recent papers have approached the general question of which properties are elicitable, we suggest that this is the wrong question--all properties are elicitable by first eliciting the entire distribution or data set, and thus the important question is how elicitable. Specifically, what is the minimum number of regression parameters needed to compute the property? Building on previous work, we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation. We establish several general results and techniques for proving upper and lower bounds on elicitation complexity. These results provide tight bounds for eliciting the Bayes risk of any loss, a large class of properties which includes spectral risk measures and several new properties of interest.
1 Introduction
Empirical risk minimization (ERM) is a domininant framework for supervised machine learning, and a key component of many learning algorithms. A statistic or property is simply a functional assigning a vector of values to each distribution. We say that such a property is elicitable, if for some loss function it can be represented as the unique minimizer of the expected loss under the distribution. Thus, the study of which properties are elicitable can be viewed as the study of which statistics are computable via ERM [1, 2, 3].
The study of property elicitation began in statistics [4, 5, 6, 7], and is gaining momentum in machine learning [8, 1, 2, 3], economics [9, 10], and most recently, finance [11, 12, 13, 14, 15]. A sequence of papers starting with Savage [4] has looked at the full characterization of losses which elicit the mean of a distribution, or more generally the expectation of a vector-valued random variable [16, 3]. The case of real-valued properties is also now well in hand [9, 1]. The general vector-valued case is still generally open, with recent progress in [3, 2, 15]. Recently, a parallel thread of research has been underway in finance, to understand which financial risk measures, among several in use or proposed to help regulate the risks of financial institutions, are computable via regression, i.e., elicitable (cf. references above). More often than not, these papers have concluded that most risk measures under consideration are not elicitable, notable exceptions being generalized quantiles (e.g. value-at-risk, expectiles) and expected utility [13, 12].
Throughout the growing momentum of the study of elicitation, one question has been central: which properties are elicitable? It is clear, however, that all properties are "indirectly" elicitable if one first elicits the distribution using a standard proper scoring rule. Therefore, in the present work, we suggest replacing this question with a more nuanced one: how elicitable are various properties? Specifically, heeding the suggestion of Gneiting [7], we adapt to our setting the notion of elicitation complexity introduced by Lambert et al. [17], which captures how many parameters one needs to maintain in an ERM procedure for the property in question. Indeed, if a real-valued property is found not to be elicitable, such as the variance, one should not abandon it, but rather ask how many parameters are required to compute it via ERM.
1

Our work is heavily inspired by the recent progress along these lines of Fissler and Ziegel [15], who show that spectral risk measures of support k have elicitation complexity at most k + 1. Spectral risk measures are among those under consideration in the finance community, and this result shows that while not elicitable in the classical sense, their elicitation complexity is still low, and hence one can develop reasonable regression procedures for them. Our results extend to these and many other risk measures (see  4.6), often providing matching lower bounds on the complexity as well.
Our contributions are the following. We first introduce an adapted definition of elicitation complexity which we believe to be the right notion to focus on going forward. We establish a few simple but useful results which allow for a kind of calculus of elicitation; for example, conditions under which the complexity of eliciting two properties in tandem is the sum of their individual complexities. In  3, we derive several techniques for proving both upper and lower bounds on elicitation complexity which apply primarily to the Bayes risks from decision theory, or optimal expected loss functions. The class includes spectral risk measures among several others; see  4. We conclude with brief remarks and open questions.

2 Preliminaries and Foundation

Let  be a set of outcomes and P  () be a convex set of probability measures. The goal of elicitation is to learn something about the distribution p  P, specifically some function (p) such
as the mean or variance, by minimizing a loss function.

Definition 1. A property is a function  report value to each distribution.1 We let

: rP=. {pRk

, for P |r

some k  N, which = (p)} denote the

associates a desired set of distributions p

corresponding to report value r.

Given a property , we want to ensure that the best result is to reveal the value of the property using a loss function that evaluates the report using a sample from the distribution.

Definition 2. (p) = arginf

A loss r L(r,

function L : Rk p), where L(r, p)

x=.

R Ep[L(r,

elicits a property  : P  *)]. A property is elicitable if

Rk if some

for all p  P loss elicits it.

,

For example, when  = R, the mean (p) = Ep[] is elicitable via squared loss L(r, ) = (r-)2.

A well-known necessary condition for elicitability is convexity of the level sets of .

Proposition 1 (Osband [5]). If  is elicitable, the level sets r are convex for all r  (P).
One can easily check that the mean (p) = Ep[] has convex level sets, yet the variance (p) = Ep[( - Ep[])2] does not, and hence is not elicitable [9].

It is often useful to work with a stronger condition, that not only is r convex, but it is the intersection of a linear subspace with P. This condition is equivalent the existence of an identification function,
a functional describing the level sets of  [17, 1].

Definition 3. A function V : Rx  Rk is an identification function for  : P  Rk, or identifies

, if for all r write V (r, p)

=. E(pP[V)

it holds (r, )].

that  is

p  r  identifiable if

V (r, there

p) = 0  exists a V

Rk, where as with identifying it.

L(r,

p)

above

we

One can check for example that V (r, ) =  - r identifies the mean.

We can now define the classes of identifiable and elicitable properties, along with the complexity of identifying or eliciting a given property. Naturally, a property is k-identifiable if it is the link of a k-dimensional identifiable property, and k-elicitable if it is the link of a k-dimensional elicitable property. The elicitation complexity of a property is then simply the minimum dimension k needed for it to be k-elicitable.
Definition 4. Let Ik(P) denote the class of all identifiable properties  : P  Rk, and Ek(P) denote the class of all elicitable properties  : P  Rk. We write I(P) = kN Ik(P) and E(P) = kN Ek(P).
Definition 5. A property  is k-identifiable if there exists   Ik(P) and f such that  = f  . The identification complexity of  is defined as iden() = min{k :  is k-identifiable}.

1We will also consider  : P  RN.

2

Definition 6. A property  is k-elicitable if there exists   Ek(P) and f such that  = f  . The elicitation complexity of  is defined as elic() = min{k :  is k-elicitable}.
To make the above definitions concrete, recall that the variance 2(p) = Ep[(Ep[]-)2] is not elicitable, as its level sets are not convex, a necessary condition by Prop. 1. Note however that we may write 2(p) = Ep[2]-Ep[]2, which can be obtained from the property (p) = (Ep[], Ep[2]). It is well-known [4, 7] that  is both elicitable and identifiable as the expectation of a vector-valued random variable X() = (, 2), using for example L(r, ) = r -X() 2 and V (r, ) = r -X(). Thus, we can recover 2 as a link of the elicitable and identifiable  : P  R2, and as no such  : P  R exists, we have iden(2) = elic(2) = 2.
In this example, the variance has a stronger property than merely being 2-identifiable and 2elicitable, namely that there is a single  that satisfies both of these simultaneously. In fact this is quite common, and identifiability provides geometric structure that we make use of in our lower bounds. Thus, most of our results use this refined notion of elicitation complexity.
Definition 7. A property  has (identifiable) elicitation complexity elicI() = min{k : , f such that   Ek(P)  Ik(P) and  = f  }.
Note that restricting our attention to elicI effectively requires elicI()  iden(); specifically, if  is derived from some elicitable , then  must be identifiable as well. This restriction is only relevant for our lower bounds, as our upper bounds give losses explicitly.2 Note however that some restriction on Ek(P) is necessary, as otherwise pathological constructions giving injective mappings from R to Rk would render all properties 1-elicitable. To alleviate this issue, some authors require continuity (e.g. [1]) while others like we do require identifiability (e.g. [15]), which can be motivated by the fact that for any differentiable loss L for , V (r, ) = rL(*, ) will identify  provided Ep[L] has no inflection points or local minima. An important future direction is to relax this identifiability assumption, as there are very natural (set-valued) properties with iden > elic.3
Our definition of elicitation complexity differs from the notion proposed by Lambert et al. [17], in that the components of  above do not need to be individually elicitable. This turns out to have a large impact, as under their definition the property (p) = max p({}) for finite  has elicitation complexity || - 1, whereas under our definition elicI() = 2; see Example 4.3. Fissler and Ziegel [15] propose a closer but still different definition, with the complexity being the smallest k such that  is a component of a k-dimensional elicitable property. Again, this definition can lead to larger complexities than necessary; take for example the squared mean (p) = Ep[]2 when  = R, which has elicI() = 1 with (p) = Ep[] and f (x) = x2, but is not elicitable and thus has complexity 2 under [15]. We believe that, modulo regularity assumptions on Ek(P), our definition is better suited to studying the difficulty of eliciting properties: viewing f as a (potentially dimensionreducing) link function, our definition captures the minimum number of parameters needed in an ERM computation of the property in question, followed by a simple one-time application of f .
2.1 Foundations of Elicitation Complexity
In the remainder of this section, we make some simple, but useful, observations about iden() and elicI(). We have already discussed one such observation after Definition 7: elicI()  iden().
It is natural to start with some trivial upper bounds. Clearly, whenever p  P can be uniquely determined by some number of elicitable parameters then the elicitation complexity of every property is at most that number. The following propositions give two notable applications of this observation.4
Proposition 2. When || = n, every property  has elicI()  n - 1.
Proof. The probability distribution is determined by the probability of any n - 1 outcomes, and the probability associated with a given outcome is both elicitable and identifiable.
2Our main lower bound (Thm 2) merely requires  to have convex level sets, which is necessary by Prop. 1. 3One may take for example (p) = argmaxi p(Ai) for a finite measurable partition A1, . . . , An of . 4Note that these restrictions on  may easily be placed on P instead; e.g. finite  is equivalent to P having support on a finite subset of , or even being piecewise constant on some disjoint events.
3

Proposition 3. When  = R,5 every property  has elicI()   (countable).6

One well-studied class of properties are those where  is linear, i.e., the expectation of some vector-
valued random variable. All such properties are elicitable and identifiable (cf. [4, 8, 3]), with elicI()  k, but of course the complexity can be lower if the range of  is not full-dimensional.
Lemma 1. Let X :   Rk be P-integrable and (p) = Ep[X]. Then elicI() = dim(affhull((P))), the dimension of the affine hull of the range of .

It1isanedasy2tothcerpearoteperretdyund=a.n{t pr1o,pe2r,tie1s

in +

various ways. For example, given elicitable properties 2} clearly contains redundant information. A concrete

case is  = {mean squared, variance, 2nd moment}, which, as we have seen, has elicI() = 2. The

following definitions and lemma capture various aspects of a lack of such redundancy.

Definition 8. Property  : P  Rk in I(P) is of full rank if iden() = k.

Note that there are two ways for a property to fail to be full rank. First, as the examples above suggest,  can be "redundant" so that it is a link of a lower-dimensional identifiable property. Full rank can also be violated if more dimensions are needed to identify the property than to specify it. This is the case with, e.g., the variance which is a 1 dimensional property but has iden(2) = 2.
Definition 9. Properties ,   I(P) are independent if iden({,  }) = iden() + iden( ).
Lemma 2. If ,   E(P) are full rank and independent, then elicI({,  }) = elicI()+elicI( ).
To illustrate the lemma, elicI(variance) = 2, yet  = {mean,variance} has elicI() = 2, so clearly the mean and variance are not both independent and full rank. (As we have seen, variance is not full rank.) However, the mean and second moment satisfy both by Lemma 1.
Another important case is when  consists of some number of distinct quantiles. Osband [5] essentially showed that quantiles are independent and of full rank, so their elicitation complexity is the number of quantiles being elicited.
Lemma 3. Let  = R and P be a class of probability measures with continuously differentiable and invertible CDFs F , which is sufficiently rich in the sense that for all x1, . . . , xk  R, span({F -1(x1), . . . , F -1(xk)}, F  P) = Rk. Let q, denote the -quantile function. Then if 1, . . . , k are all distinct,  = {q1 , . . . , qk } has elicI () = k.
The quantile example in particular allows us to see that all complexity classes, including , are occupied. In fact, our results to follow will show something stronger: even for real-valued properties  : P  R, all classes are occupied; we give here the result that follows from our bounds on spectral risk measures in Example 4.4, but this holds for many other P; see e.g. Example 4.2.
Proposition 4. Let P as in Lemma 3. Then for all k  N there exists  : P  R with elicI() = k.

3 Eliciting the Bayes Risk

In this section we prove two theorems that provide our main tools for proving upper and lower bounds respectively on elicitation complexity. Of course many properties are known to be elicitable, and the losses that elicit them provide such an upper bound for that case. We provide such a construction for properties that can be expressed as the pointwise minimum of an indexed set of functions. Interestingly, our construction does not elicit the minimum directly, but as a joint elicitation of the value and the function that realizes this value. The form (1) is that of a scoring rule for the linear property p  Ep[Xa], except that here the index a itself is also elicited.7
Theorem 1. Let {Xa :   R}aA be a set of P-integrable functions indexed by A  Rk. Then if infa Ep[Xa] is attained, the property (p) = mina Ep[Xa] is (k + 1)-elicitable. In particular,

L((r, a), ) = H(r) + h(r)(Xa - r)

(1)

elicits

p



{((p), a)

:

Ep[Xa] = (p)}

for

any

strictly

decreasing

h

:

R



R+

with

d dr

H

=

h.

5Here and throughout, when  = Rk we assume the Borel -algebra. 6Omitted proofs can be found in the appendix of the full version of this paper. 7As we focus on elicitation complexity, we have not tried to characterize all ways to elicit this joint property,
or other properties we give explicit losses for. See  4.1 for an example where additional losses are possible.

4

Proof. We will work with gains instead of losses, and show that S((r, a), ) = g(r) + dgr(Xa - r) elicits p  {((p), a) : Ep[Xa] = (p)} for (p) = maxa Ep[Xa]. Here g is convex with strictly
increasing and positive subgradient dg.

For any fixed a, we have by the subgradient inequality,

S((r, a), p) = g(r) + dgr(Ep[Xa] - r)  g(Ep[Xa]) = S((Ep[Xa], a), p) ,

and as dg is strictly increasing, g is strictly convex, so r = Ep[Xa] is the unique maximizer. Now letting S(a, p) = S((Ep[Xa], a), p), we have

argmax S(a, p) = argmax g(Ep[Xa]) = argmax Ep[Xa] ,

aA

aA

aA

because g is strictly increasing. We now have

argmax S((r, a), p) = (Ep[Xa], a) : a  argmax Ep[Xa] .

aA,rR

aA

One natural way to get such an indexed set of functions is to take an arbitrary loss function L(r, ), in which case this pointwise minimum corresponds to the Bayes risk, which is simply the minimum possible expected loss under some distribution p.
Definition 10. Given loss function L : A x   R on some prediction set A, the Bayes risk of L is defined as L(p) := infaA L(a, p).

One illustration of the power of Theorem 1 is that the Bayes risk of a loss eliciting a k-dimensional property is itself (k + 1)-elicitable.
Corollary 1. If L : Rk x   R is a loss function eliciting  : P  Rk, then the loss

L((r, a), ) = L (a, ) + H(r) + h(r)(L(a, ) - r)

(2)

elicits {L, }, where h : R  R+ is any positive strictly decreasing function, H(r) =

r 0

h(x)dx,

and L is any surrogate loss eliciting .8 If   Ik(P), elicI(L)  k + 1.

We now turn to our second theorem which provides lower bounds for the elicitation complexity of the Bayes risk. A first observation, which follows from standard convex analysis, is that L is concave, and thus it is unlikely to be elicitable directly, as the level sets of L are likely to be nonconvex. To show a lower bound greater than 1, however, we will need much stronger techniques. In particular, while L must be concave, it may not be strictly so, thus enabling level sets which are potentially amenable to elicitation. In fact, L must be flat between any two distributions which share a minimizer. Crucial to our lower bound is the fact that whenever the minimizer of L differs between two distributions, L is essentially strictly concave between them.
Lemma 4. Suppose loss L with Bayes risk L elicits  : P  Rk. Then for any p, p  P with (p) = (p ), we have L(p + (1 - )p ) > L(p) + (1 - )L(p ) for all   (0, 1).

With this lemma in hand we can prove our lower bound. The crucial insight is that an identification function for the Bayes risk of a loss eliciting a property can, through a link, be used to identify that property. Corollary 1 tells us that k + 1 parameters suffice for the Bayes risk of a k-dimensional property, and our lower bound shows this is often necessary. Only k parameters suffice, however, when the property value itself provides all the information required to compute the Bayes risk; for example, dropping the y2 term from squared loss gives L(x, y) = x2 - 2xy and L(p) = -Ep[y]2, giving elic(L) = 1. Thus the theorem splits the lower bound into two cases.
Theorem 2. If a loss L elicits some   Ek(P) with elicitation complexity elicI() = k, then its Bayes risk L has elicI(L)  k. Moreover, if we can write L = f   for some function f : Rk  R, then we have elicI(L) = k; otherwise, elicI(L) = k + 1.

Proof. Let   E such that L = g   for some g : R  R.
8Note that one could easily lift the requirement that  be a function, and allow (p) to be the set of minimizers of the loss (cf. [18]). We will use this additional power in Example 4.4.

5

We show by contradiction that for all p, p  P, (p) = (p ) implies (p) = (p ). Otherwise, we have p, p with (p) = (p ), and thus L(p) = L(p ), but (p) = (p ). Lemma 4 would then give us some p = p + (1 - )p with L(p) > L(p). But as the level sets r are convex by Prop. 1, we would have (p) = (p), which would imply L(p) = L(p).
We now can conclude that there exists h : R  Rk such that  = h  . But as   E , this implies elicI()  , so clearly we need  k. Finally, if = k we have L = g   = g  h-1  . The upper bounds follow from Corollary 1.

4 Examples and Applications
We now give several applications of our results. Several upper bounds are novel, as well as all lower bounds greater than 2. In the examples, unless we refer to  explicitly we will assume  = R and write y   so that y  p. In each setting, we also make several standard regularity assumptions which we suppress for ease of exposition -- for example, for the variance and variantile we assume finite first and second moments (which must span R2), and whenever we discuss quantiles we will assume that P is as in Lemma 3, though we will not require as much regularity for our upper bounds.

4.1 Variance

In Section 2 we showed that elicI(2) = 2. As a warm up, let us see how to recover this statement using our results on the Bayes risk. We can view 2 as the Bayes risk of squared loss L(x, y) = (x- y)2, which of course elicits the mean: L(p) = minxR Ep[(x - y)2] = Ep[(Ep[y] - y)2] = 2(p). This gives us elicI(2)  2 by Corollary 1, with a matching lower bound by Theorem 2, as the

variance is not simply a function of the mean. Corollary 1 gives losses such as L((x, v), y) = e-v((x - y)2 - v) - e-v which elict {Ep[y], 2(p)}, but in fact there are losses which cannot

be represented by the form (2), showing that we do not have a full characterization; for example,

L((x, v), y) = v2 + v(x - y)(2(x + y) + 1) + (x - y)2 (x + y)2 + x + y + 1 . This L was

generated via squared loss

z-

y y2

2
with respect to the norm z 2 = z

1 -1/2 -1/2 1

z, which

elicits the first two moments, and link function (z1, z2)  (z1, z2 - z12).

4.2 Convex Functions of Means
Another simple example is (p) = G(Ep[X]) for some strictly convex function G : Rk  R and P-integrable X :   Rk. To avoid degeneracies, we assume dim affhull{Ep[X] : p  P} = k, i.e.  is full rank. Letting {dGp}pP be a selection of subgradients of G, the loss L(r, ) = -(G(r) + dGr(X() - r)) elicits  : p  Ep[X] (cf. [3]), and moreover we have (p) = -L(p). By Lemma 1, elicI() = k. One easily checks that L = G  , so now by Theorem 2, elicI() = k as well. Letting {Xk}kN be a family of such "full rank" random variables, this gives us a sequence of real-valued properties k(p) = Ep[X] 2 with elicI(k) = k, proving Proposition 4.

4.3 Modal Mass
With  = R consider the property (p) = maxxR p([x - , x + ]), namely, the maximum probability mass contained in an interval of width 2. Theorem 1 easily shows elicI()  2, as (p) = argmaxxR p([x - , x + ]) is elicited by L(x, y) = 1|x-y|>, and (p) = 1 - L(p). Similarly, in the case of finite , (p) = max p({}) is simply the expected score (gain rather than loss) of the mode (p) = argmax p({}), which is elicitable for finite  (but not otherwise; see Heinrich [19]).
In both cases, one can easily check that the level sets of  are not convex, so elicI() = 2; alternatively Theorem 2 applies in the first case. As mentioned following Definition 6, the result for finite  differs from the definitions of Lambert et al. [17], where the elicitation complexity of  is || - 1.

6

4.4 Expected Shortfall and Other Spectral Risk Measures

One important application of our results on the elicitation complexity of the Bayes risk is the elic-
itability of various financial risk measures. One of the most popular financial risk measures is expected shortfall ES : P  R, also called conditional value at risk (CVaR) or average value at risk (AVaR), which we define as follows (cf. [20, eq.(18)], [21, eq.(3.21)]):

ES(p) = inf
zR

Ep

1 

(z

-

y)1zy

-

z

= inf
zR

Ep

1 

(z

-

y)(1zy

-

)

-

y

.

(3)

Despite the importance of elicitability to financial regulation [11, 22], ES is not elicitable [7]. It

was recently shown by Fissler and Ziegel [15], however, that elicI(ES) = 2. They also consider the

broader class of spectral risk measures, which can be represented as (p) = [0,1] ES(p)d(),

where  is a probability measure on [0, 1] (cf. [20, eq. (36)]). In the case where  has finite support

=

k i=1

ii

for

point

distributions

,

i

>

0,

we

can

rewrite



using

the

above

as:

k

(p)

=

i=1

iESi (p)

=

inf
zRk

Ep

k i=1

i i

(zi

-

y)(1ziy

-

i)

-

y

.

(4)

They conclude elicI()  k + 1 unless ({1}) = 1 in which case elicI() = 1. We show how to recover these results together with matching lower bounds. It is well-known that the infimum in eq. (4) is attained by any of the k quantiles in q1 (p), . . . , qk (p), so we conclude elicI()  k + 1 by Theorem 1, and in particular the property {, q1 , . . . , qk } is elicitable. The family of losses from Corollary 1 coincide with the characterization of Fissler and Ziegel [15] (see  D.1). For a lower bound, as elicI({q1 , . . . , qk }) = k whenever the i are distinct by Lemma 3, Theorem 2 gives us elicI() = k + 1 whenever ({1}) < 1, and of course elicI() = 1 if ({1}) = 1.

4.5 Variantile

The  -expectile, a type of generalized quantile introduced by Newey and Powell [23], is defined as
the solution x =  to the equation Ep [|1xy -  |(x - y)] = 0. (This also shows   I1.) Here
we propose the  -variantile, an asymmetric variance-like measure with respect to the  -expectile: just as the mean is the solution x =  to the equation Ep[x - y] = 0, and the variance is 2(p) = Ep[( - y)2], we define the  -variantile 2 by 2(p) = Ep |1 y -  |( - y)2 .

It is well-known that  can be expressed as the minimizer of a asymmetric least squares problem: the loss L(x, y) = |1xy -  |(x - y)2 elicits  [23, 7]. Hence, just as the variance turned out to be a Bayes risk for the mean, so is the  -variantile for the  -expectile:

 = argmin Ep |1xy -  |(x - y)2
xR

=

2

=

min
xR

Ep

|1xy -  |(x - y)2

.

We now see the pair { , 2} is elicitable by Corollary 1, and by Theorem 2 we have elicI(2) = 2.

4.6 Deviation and Risk Measures

Rockafellar and Uryasev [21] introduce "risk quadrangles" in which they relate a risk R, deviation D, error E, and a statistic S, all functions from random variables to the reals, as follows:

R(X) = min{C + E(X - C)}, D(X) = min{E(X - C)}, S(X) = argmin{E(X - C)} .
C CC
Our results provide tight bounds for many of the risk and deviation measures in their paper. The most immediate case is the expectation quadrangle case, where E(X) = E[e(X)] for some e : R  R. In this case, if S(X)  I1(P) Theorem 2 implies elicI(R) = elicI(D) = 2 provided S is nonconstant and e non-linear. This includes several of their examples, e.g. truncated mean, log-exp, and rate-based. Beyond the expectation case, the authors show a Mixing Theorem, where they consider

kk

D(X) = min min

iEi(X - C - Bi)

C B1,..,Bk i=1

iBi = 0
i

= min
B1 ,..,Bk

iEi(X - Bi)
i=1

.

Once again, if the Ei are all of expectation type and Si  I1, Theorem 1 gives elicI(D) = elicI(R)  k + 1, with a matching lower bound from Theorem 2 provided the Si are all independent. The Reverting Theorem for a pair E1, E2 can be seen as a special case of the above where

7

one replaces E2(X) by E2(-X). Consequently, we have tight bounds for the elicitation complexity of several other examples, including superquantiles (the same as spectral risk measures), the quantile-radius quadrangle, and optimized certainty equivalents of Ben-Tal and Teboulle [24].
Our results offer an explaination for the existence of regression procedures for some of these risk/deviation measures. For example, a proceedure called superquantile regression was introduced in Rockafellar et al. [25], which computes spectral risk measures. In light of Theorem 1, one could interpret their procedure as simply performing regression on the k different quantiles as well as the Bayes risk. In fact, our results show that any risk/deviation generated by mixing several expectation quadrangles will have a similar procedure, in which the Bi variables are simply computed along side the measure of interest. Even more broadly, such regression procedures exist for any Bayes risk.
5 Discussion
We have outlined a theory of elicitation complexity which we believe is the right notion of complexity for ERM, and provided techniques and results for upper and lower bounds. In particular, we now have tight bounds for the large class of Bayes risks, including several applications of note such as spectral risk measures. Our results also offer an explanation for why procedures like superquantile regression are possible, and extend this logic to all Bayes risks.
There many natural open problems in elicitation complexity. Perhaps the most apparent are the characterizations of the complexity classes { : elic() = k}, and in particular, determining the elicitation complexity of properties which are known to be non-elicitabile, such as the mode [19] and smallest confidence interval [18].
In this paper we have focused on elicitation complexity with respect to the class of identifiable properties I, which we denoted elicI. This choice of notation was deliberate; one may define elicC := min{k :   Ek  C, f,  = f  } to be the complexity with respect to some arbitrary class of properties C. Some examples of interest might be elicE for expected values, of interest to the prediction market literature [8], and eliccvx for properties elicitable by a loss which is convex in r, of interest for efficiently performing ERM.
Another interesting line of questioning follows from the notion of conditional elicitation, properties which are elicitable as long as the value of some other elicitable property is known. This notion was introduced by Emmer et al. [11], who showed that the variance and expected shortfall are both conditionally elicitable, on Ep[y] and q(p) respectively. Intuitively, knowing that  is elicitable conditional on an elicitable  would suggest that perhaps the pair {,  } is elicitable; Fissler and Ziegel [15] note that it is an open question whether this joint elicitability holds in general. The Bayes risk L for  is elicitable conditioned on , and as we saw above, the pair {, L} is jointly elicitable as well. We give a counter-example in Figure 1, however, which also illustrates the subtlety of characterizing all elicitable properties.
p2 p2
p3 p3
p1 p1
Figure 1: Depictions of the level sets of two properties, one elicitable and the other not. The left is a Bayes risk together with its property, and thus elicitable, while the right is shown in [3] not to be elicitable. Here the planes are shown to illustrate the fact that these are both conditionally elicitable: the height of the plane (the intersept (p3, 0, 0) for example) is elicitable from the characterizations for scalar properties [9, 1], and conditioned on the plane, the properties are both linear and thus links of expected values, which are also elicitable.
8

References
[1] Ingo Steinwart, Chlo Pasin, Robert Williamson, and Siyu Zhang. Elicitation and Identification of Properties. In Proceedings of The 27th Conference on Learning Theory, pages 482-526, 2014.
[2] A. Agarwal and S. Agrawal. On Consistent Surrogate Risk Minimization and Property Elicitation. In COLT, 2015.
[3] Rafael Frongillo and Ian Kash. Vector-Valued Property Elicitation. In Proceedings of the 28th Conference on Learning Theory, pages 1-18, 2015.
[4] L.J. Savage. Elicitation of personal probabilities and expectations. Journal of the American Statistical Association, pages 783-801, 1971.
[5] Kent Harold Osband. Providing Incentives for Better Cost Forecasting. University of California, Berkeley, 1985.
[6] T. Gneiting and A.E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359-378, 2007.
[7] T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical Association, 106(494):746-762, 2011.
[8] J. Abernethy and R. Frongillo. A characterization of scoring rules for linear properties. In Proceedings of the 25th Conference on Learning Theory, pages 1-27, 2012.
[9] N.S. Lambert. Elicitation and Evaluation of Statistical Forecasts. Preprint, 2011. [10] N.S. Lambert and Y. Shoham. Eliciting truthful answers to multiple-choice questions. In Proceedings of
the 10th ACM conference on Electronic commerce, pages 109-118, 2009. [11] Susanne Emmer, Marie Kratz, and Dirk Tasche. What is the best risk measure in practice? A comparison
of standard measures. arXiv:1312.1645 [q-fin], December 2013. arXiv: 1312.1645. [12] Fabio Bellini and Valeria Bignozzi. Elicitable risk measures. This is a preprint of an article accepted for
publication in Quantitative Finance (doi 10.1080/14697688.2014. 946955), 2013. [13] Johanna F. Ziegel. Coherence and elicitability. Mathematical Finance, 2014. arXiv: 1303.1690. [14] Ruodu Wang and Johanna F. Ziegel. Elicitable distortion risk measures: A concise proof. Statistics &
Probability Letters, 100:172-175, May 2015. [15] Tobias Fissler and Johanna F. Ziegel. Higher order elicitability and Osband's principle. arXiv:1503.08123
[math, q-fin, stat], March 2015. arXiv: 1503.08123. [16] A. Banerjee, X. Guo, and H. Wang. On the optimality of conditional expectation as a Bregman predictor.
IEEE Transactions on Information Theory, 51(7):2664-2669, July 2005. [17] N.S. Lambert, D.M. Pennock, and Y. Shoham. Eliciting properties of probability distributions. In Pro-
ceedings of the 9th ACM Conference on Electronic Commerce, pages 129-138, 2008. [18] Rafael Frongillo and Ian Kash. General truthfulness characterizations via convex analysis. In Web and
Internet Economics, pages 354-370. Springer, 2014. [19] C. Heinrich. The mode functional is not elicitable. Biometrika, page ast048, 2013. [20] Hans Fllmer and Stefan Weber. The Axiomatic Approach to Risk Measures for Capital Determination.
Annual Review of Financial Economics, 7(1), 2015. [21] R. Tyrrell Rockafellar and Stan Uryasev. The fundamental risk quadrangle in risk management, optimiza-
tion and statistical estimation. Surveys in Operations Research and Management Science, 18(1):33-53, 2013. [22] Tobias Fissler, Johanna F. Ziegel, and Tilmann Gneiting. Expected Shortfall is jointly elicitable with Value at Risk - Implications for backtesting. arXiv:1507.00244 [q-fin], July 2015. arXiv: 1507.00244. [23] Whitney K. Newey and James L. Powell. Asymmetric least squares estimation and testing. Econometrica: Journal of the Econometric Society, pages 819-847, 1987. [24] Aharon Ben-Tal and Marc Teboulle. AN OLD-NEW CONCEPT OF CONVEX RISK MEASURES: THE OPTIMIZED CERTAINTY EQUIVALENT. Mathematical Finance, 17(3):449-476, 2007. [25] R. T. Rockafellar, J. O. Royset, and S. I. Miranda. Superquantile regression with applications to buffered reliability, uncertainty quantification, and conditional value-at-risk. European Journal of Operational Research, 234:140-154, 2014.
9

