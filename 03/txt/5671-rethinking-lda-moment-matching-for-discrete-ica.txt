Rethinking LDA: Moment Matching for Discrete ICA

Anastasia Podosinnikova Francis Bach Simon Lacoste-Julien INRIA - E cole normale superieure Paris

Abstract
We consider moment matching techniques for estimation in latent Dirichlet allocation (LDA). By drawing explicit links between LDA and discrete versions of independent component analysis (ICA), we first derive a new set of cumulantbased tensors, with an improved sample complexity. Moreover, we reuse standard ICA techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method. In an extensive set of experiments on both synthetic and real datasets, we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods.

1 Introduction
Topic models have emerged as flexible and important tools for the modelisation of text corpora. While early work has focused on graphical-model approximate inference techniques such as variational inference [1] or Gibbs sampling [2], tensor-based moment matching techniques have recently emerged as strong competitors due to their computational speed and theoretical guarantees [3, 4]. In this paper, we draw explicit links with the independent component analysis (ICA) literature (e.g., [5] and references therein) by showing a strong relationship between latent Dirichlet allocation (LDA) [1] and ICA [6, 7, 8]. We can then reuse standard ICA techniques and results, and derive new tensors with better sample complexity and new algorithms based on joint diagonalization.

2 Is LDA discrete PCA or discrete ICA?

Notation. Following the text modeling terminology, we define a corpus X = {x1, . . . , xN } as a

collection of convenient to

N documents. Each document is a collection {wn1, . . . , represent the -th token of the n-th document as a 1-of-M

wenncLond}inogf

Ln tokens. It is with an indicator

vadnseo2ccttuho{mer1ew,cn.ont.u.ins,2tNLv{n}e0c,t=to1o}rreMPxfenmwr tiox:t=hndmooPnc.ulyWm oweennnwetsni,lo2linnad-lRzewexMarmyo.s,

where M is the vocabulary size, and each document In such representation, the length Ln of the n-th use index k 2 {1, . . . , K} to refer to topics, index 2 {1, . . . , M } to refer to words from the vocabulary,

and index  2 {1, . . . , Ln} to refer to tokens of the n-th document. The plate diagrams of the models

from this section are presented in Appendix A.

Latent Dirichlet allocation [1] is a generative probabilistic model for discrete data such as text

corpora. In accordance to this model, the n-th document is modeled as an admixture over the vo-

cabulary of M words with K latent topics. Specifically, the latent variable n, which is sampled from the Dirichlet distribution, represents the topic mixture proportion over K topics for the n-th

document. Given n, the topic choice zn|n for the -th token is sampled from the multinomial dis-

tribution with the probability vector n. The token wn|zn, n is then sampled from the multinomial

distribution This vector

with dk is

the the

probability k-th topic,

vector that is

advznec,toorr

dk if k is the index of the non-zero of probabilities over the words from

subject to the simplex constraint, i.e., dk 2 M , where M := {d 2 RM : d  0,

This generative process of a document (the index n is omitted for simplicity) can be

element in zn. stPhuemmvmodacmraibz=uedla1ar}ys.

1

  Dirichlet(c),

z|  Multinomial(1, ),

(1)

w|z,   Multinomial(1, dz ). One can think of the latent variables z as auxiliary variables which were introduced for convenience of inference, but can in fact be marginalized out [9], which leads to the following model

  Dirichlet(c), x|  Multinomial(L, D),

LDA model (2)

where D 2 RM is the vector of

K is the topic parameters for

matrix with the k-th column equal to the k-th the Dirichlet distribution. While a document

topic dk, and c is represented

2 as

Ra K+se+t

of tokens w in the formulation (1), the formulation (2) instead compactly represents a document as the count vector x. Although the two representations are equivalent, we focus on the second one in

this paper and therefore refer to it as the LDA model.

Importantly, the LDA model does not model the length of documents. Indeed, although the original paper [1] proposes to model the document length as L|  Poisson( ), this is never used in practice and, in particular, the parameter is not learned. Therefore, in the way that the LDA model is typically used, it does not provide a complete generative process of a document as there is no rule to sample L| . In this paper, this fact is important, as we need to model the document length in order to make the link with discrete ICA.

Discrete PCA. The LDA model (2) can be seen as a discretization of principal component analysis (PCA) via replacement of the normal likelihood with the multinomial one and adjusting the prior [9] in the following probabilistic PCA model [10, 11]:   Normal(0, IK ) and x|  Normal(D, 2IM ), where D 2 RMK is a transformation matrix and is a parameter.

Discrete ICA (DICA). Interestingly, a small extension of the LDA model allows its interpreta-

tion as a discrete independent component analysis model. The extension naturally arises when the

document length for the LDA model is modeled as a random variable from the gamma-Poisson

mLDixAtuGmreaom(dwmehl ai(c2(hc)0wi,sbite)hq, uwsuihvceahrleednoct0ctuo:m=aePnnetgkleacntkigviteshbtihsineeosqmhuaiipvalaelrpeananrtda(omsmeeetevArapraipnaedbnlbed)i>,xiB.0e..i1,s)Ltht|oe

 Poisson( ) rate parameter.

and The

k  Gamma(ck, b), xm|  Poisson([D]m),

GP model (3)

where all 1, 2, . . . , K are mutually independent, the parameters ck coincide with the ones of the LDA model in (2), and the free parameter b can be seen (see Appendix B.2) as a scaling parameter for the document length when c0 is already prescribed.

This model was introduced by Canny [12] and later named as a discrete ICA model [13]. It is more natural, however, to name model (3) as the gamma-Poisson (GP) model and the model

1, . . . , K  mutually independent, xm|  Poisson([D]m)

DICA model (4)

as the discrete ICA (DICA) model. The only difference between (4) and the standard ICA model [6, 7, 8] (without additive noise) is the presence of the Poisson noise which enforces discrete, instead of continuous, values of xm. Note also that (a) the discrete ICA model is a semi-parametric model that can adapt to any distribution on the topic intensities k and that (b) the GP model (3) is a particular case of both the LDA model (2) and the DICA model (4).

Thanks to this close connection between LDA and ICA, we can reuse standard ICA techniques to derive new efficient algorithms for topic modeling.

3 Moment matching for topic modeling
The method of moments estimates latent parameters of a probabilistic model by matching theoretical expressions of its moments with their sample estimates. Recently [3, 4], the method of moments was applied to different latent variable models including LDA, resulting in computationally fast

2

learning algorithms with theoretical guarantees. For LDA, they (a) construct LDA moments with a particular diagonal structure and (b) develop algorithms for estimating the parameters of the model by exploiting this diagonal structure. In this paper, we introduce novel GP/DICA cumulants with a similar to the LDA moments structure. This structure allows to reapply the algorithms of [3, 4] for the estimation of the model parameters, with the same theoretical guarantees. We also consider another algorithm applicable to both the LDA moments and the GP/DICA cumulants.

3.1 Cumulants of the GP and DICA models

In this section, we derive and analyze the novel cumulants of the DICA model. As the GP model is a particular case of the DICA model, all results of this section extend to the GP model.

The first three cumulant tensors for the random vector x can be defined as follows

cum(x) := E(x),

cum(x,

x)

:=

cov(x,

x)

=

E

 (x

E(x))(x

E(x))> ,

cum(x, x, x) := E [(x E(x))  (x E(x))  (x E(x))] ,

(5) (6) (7)

where  denotes the tensor product (see some properties of cumulants in Appendix C.1). The essential property of the cumulants (which does not hold for moments) that we use in this paper is that the cumulant tensor for a random vector with independent components is diagonal.

Let y = D; then for the Poisson random variable xm|ym  Poisson(ym), the expectation is E(xm|ym) = ym. Hence, by the law of total expectation and the linearity of expectation, the expectation in (5) has the following form

E(x) = E(E(x|y)) = E(y) = DE().

(8)

Further, the variance of the Poisson random variable xm is var(xm|ym) = ym and, as x1, x2, . . . , xM are conditionally independent given y, then their covariance matrix is diagonal, i.e., cov(x, x|y) = diag(y). Therefore, by the law of total covariance, the covariance in (6) has the form

cov(x, x) = E [cov(x, x|y)] + cov [E(x|y), E(x|y)] = diag [E(y)] + cov(y, y) = diag [E(x)] + Dcov(, )D>,

(9)

where the last equality follows by the multilinearity property of cumulants (see Appendix C.1). Moving the first term from the RHS of (9) to the LHS, we define

S := cov(x, x) diag [E(x)] .

DICA S-cum. (10)

From (9) and by the independence of 1, . . . , K (see Appendix C.3), S has the following diagonal

structure

X

S = k var(k)dkd>k = Ddiag [var()] D>.

(11)

By analogy with the second order case, using the law of total cumulance, the multilinearity property of cumulants, and the independence of 1, . . . , K , we derive in Appendix C.2 expression (24), similar to (9), for the third cumulant (7). Moving the terms in this expression, we define a tensor T with the following element

[T ]m1m2m3 := cum(xm1 , xm2 , xm3 ) + 2 (m1, m2, m3)E(xm1 )

DICA T-cum. (12)

(m2, m3)cov(xm1 , xm2 ) (m1, m3)cov(xm1 , xm2 ) (m1, m2)cov(xm1 , xm3 ),

where is the Kronecker delta. By analogy with (11) (Appendix C.3), the diagonal structure of

tensor T :

X

T = k cum(k, k, k)dk  dk  dk.

(13)

In Appendix E.1, we recall (in our notation) the matrix S (39) and the tensor T (40) for the LDA model [3], which are analogues of the matrix S (10) and the tensor T (12) for the GP/DICA models. Slightly abusing terminology, we refer to the matrix S (39) and the tensor T (40) as the LDA moments and to the matrix S (10) and the tensor T (12) as the GP/DICA cumulants. The diagonal structure (41) & (42) of the LDA moments is similar to the diagonal structure (11) & (13) of the GP/DICA cumulants, though arising through a slightly different argument, as discussed at the end of

3

Appendix E.1. Importantly, due to this similarity, the algorithmic frameworks for both the GP/DICA cumulants and the LDA moments coincide.

The following sample complexity results apply to the sample estimates of the GP cumulants:1

Proposition 3.1. Under the GP model, the expected error for the sample estimator Sb (29) for the

GP cumulant S (10) is: h i rh
E kSb SkF  E kSb

i Sk2F



O

 p1 N

 max

L2, c0L ,

(14)

where := max k kdkk22, c0 := min(1, c0) and L := E(L).

A high probability bound could be derived using concentration inequalities for Poisson random variables [14]; but the expectation already gives the right order of magnitude for the error (for example via Markov's inequality). The expression (29) for an unbiased finite sample estimate Sb of S and the expression (30) for an unbiased finite sample estimate Tb of T are defined2 in Appendix C.4. A sketch of a proof for Proposition 3.1 can be found in Appendix D.

By following a similar analysis as in [15], we can rephrase the topic recovery error in term of the

error on the GP cumulant. Importantly, the whitening transformation (introduced in Section 4) redi-

vtbhiodattehsthteheacnoedrnrtcor0irb/ouLntiaoSrne(1fsr4mo)mabllyeSrLtt2oh,atwnhhe1icraehncdiosvcteahrneybsecerarvloeerroywfsiSlml (asslcela.eleWAaepspdeOon(nd1oi/xtppDrN.e5smefonartxtdh{eetaei,xlcsa0)c./tLTeh}xi)ps,rmewsehsaieonrnes

for we

the expected squared expect the analogous

ebroruonrdfoorftEhe[keTbstimaTtokrFo] fT1, /bputNdume atox{a

similar structure in the 3/2L3, c30/2L3/2}.

derivation,

Current

sample

complexity

results

of

the

LDA

moments

[3]

can

be

summarized

as

p O(1/ N

).

How-

ever, the proof (which can be found in the supplementary material [15]) analyzes only the case when

finite sample estimates of the LDA moments are constructed from one triple per document, i.e.,

w1  w2  w3 only, and not from the U-statistics that average multiple (dependent) triples per document as in the practical expressions (43) and (44). Moreover, one has to be careful when com-

paring upper bounds. Nevertheless, comparing the bound (14) with the current theoretical results

for the LDA moments, we see that the GP/DICA cumulants sample complexity contains the 2norm of the columns of the topic matrix D in the numerator, as opposed to the O(1) coefficient

for the LDA moments. This norm can be significantly smaller than 1 for vectors in the simplex

(e.g., = O(1/kdkk0) for sparse topics). This suggests that the GP/DICA cumulants may have better finite sample convergence properties than the LDA moments and our experimental results in

Section 5.2 are indeed consistent with this statement.

The GP/DICA cumulants have a somewhat more intuitive derivation than the LDA moments as they are expressed via the count vectors x (which are the sufficient statistics for the model) and not the tokens w's. Note also that the construction of the LDA moments depend on the unknown parameter c0. Given that we are in an unsupervised setting and that moreover the evaluation of LDA is a difficult task [16], setting this parameter is non-trivial. In Appendix G.4, we observe experimentally that the LDA moments are somewhat sensitive to the choice of c0.

4 Diagonalization algorithms

How is the diagonal structure (11) of S and (13) of T going to be helpful for the estimation of the model parameters? This question has already been thoroughly investigated in the signal processing (see, e.g., [17, 18, 19, 20, 21, 5] and references therein) and machine learning (see [3, 4] and references therein) literature. We review the approach in this section. Due to similar diagonal structure, the algorithms of this section apply to both the LDA moments and the GP/DICA cumulants.

For simplicity, let us rewrite expressions (11) and (13) for S and T as follows

XX

S=

k skdkd>k ,

T=

tkdk  dk  dk,

k

(15)

1Note that the expected squared error for the DICA cumulants is similar, but the expressions are less compact and, in general, depend on the prior on k.
2For completeness, we also present the finite sample estimates Sb (43) and Tb (44) of S (39) and T (40) for the LDA moments (which are consistent with the ones suggested in [4]) in Appendix F.4.

4

where sk := var(k) and tk := cum(k, k, k). Introducing the rescaled topics dek := pskdk, we can also rewrite S = DeDe>. Following the same assumption from [3] that the topic vectors are linearly independent (De is full rank), we can compute a whitening matrix W 2 RKM of S, i.e., a matrix such that W SW > = IK where IK is the K-by-K identity matrix (see Appendix F.1 for more details). As a result, the vectors zk := W dek form an orthonormal set of vectors.

Further, let us define a projection T (v) 2 RKK of a tensor T 2 RKKK onto a vector u 2 RK :

X

T (u)k1k2 :=

k3 Tk1k2k3 uk3 .

(16)

Applying the multilinear transformation (see, e.g., [4] for the definition) with W > to the tensor T

from (15) and projecting the resulting tensor T := T (W >, W >, W >) onto some vector u 2 RK ,

we obtain

X

T (u) =

k etkhzk, uizkzk>,

(17)

where etk := tk/s3k/2 is due to the rescaling of topics and h*, *i stands for the inner product. As the vectors zk are orthonormal, the pairs zk and k := etkhzk, ui are eigenpairs of the matrix T (u), which are uniquely defined if the eigenvalues k are all different. If they are unique, we can recover the GP/DICA (as well as LDA) model parameters via dek = W zk and etk = k/hzk, ui.

This procedure was referred to as the spectral algorithm for LDA [3] and the fourth-order3 blind identification algorithm for ICA [17, 18]. Indeed, one can expect that the finite sample estimates Sb (29) and Tb (30) possess approximately the diagonal structure (11) and (13) and, therefore, the reasoning from above can be applied, assuming that the effect of the sampling error is controlled.

This spectral algorithm, however, is known to be quite unstable in practice (see, e.g., [22]). To overcome this problem, other algorithms were proposed. For ICA, the most notable ones are probably the FastICA algorithm [20] and the JADE algorithm [21]. The FastICA algorithm, with appropriate choice of a contrast function, estimates iteratively the topics, making use of the orthonormal structure (17), and performs the deflation procedure at every step. The recently introduced tensor power method (TPM) for the LDA model [4] is close to the FastICA algorithm. Alternatively, the JADE algorithm modifies the spectral algorithm by performing multiple projections for (17) and then jointly diagonalizing the resulting matrices with an orthogonal matrix. The spectral algorithm is a special case of this orthogonal joint diagonalization algorithm when only one projection is chosen. Importantly, a fast implementation [23] of the orthogonal joint diagonalization algorithm from [24] was proposed, which is based on closed-form iterative Jacobi updates (see, e.g., [25] for the later).

In practice, the orthogonal joint diagonalization (JD) algorithm is more robust than FastICA (see, e.g., [26, p. 30]) or the spectral algorithm. Moreover, although the application of the JD algorithm for the learning of topic models was mentioned in the literature [4, 27], it was never implemented in practice. In this paper, we apply the JD algorithm for the diagonalization of the GP/DICA cumulants as well as the LDA moments, which is described in Algorithm 1. Note that the choice of a projection vector vp 2 RM obtained as vp = Wc>up for some vector up 2 RK is important and corresponds to the multilinear transformation of Tb with Wc> along the third mode. Importantly, in Algorithm 1, the joint diagonalization routine is performed over (P + 1) matrices of size KK, where the number of topics K is usually not too big. This makes the algorithm computationally fast (see Appendix G.1). The same is true for the spectral algorithm, but not for TPM.

In Section 5.1, we compare experimentally the performance of the spectral, JD, and TPM algorithms for the estimation of the parameters of the GP/DICA as well as LDA models. We are not aware of any experimental comparison of these algorithms in the LDA context. While already working on this manuscript, the JD algorithm was also independently analyzed by [27] in the context of tensor factorization for general latent variable models. However, [27] focused mostly on the comparison of approaches for tensor factorization and their stability properties, with brief experiments using a latent variable model related but not equivalent to LDA for community detection. In contrast, we provide a detailed experimental comparison in the context of LDA in this paper, as well as propose a novel cumulant-based estimator. Due to the space restriction the estimation of the topic matrix D and the (gamma/Dirichlet) parameter c are moved to Appendix F.6.

3See Appendix C.5 for a discussion on the orders.

5

Algorithm 1 Joint diagonalization (JD) algorithm for GP/DICA cumulants (or LDA moments)

1: Input: X 2 RMN , K, P (number of random projections); (and c0 for LDA moments)

2: Compute sample estimate Sb 2 RMM ((29) for GP/DICA / (43) for LDA in Appendix F)

3: Estimate whitening matrix Wc 2 RKM of Sb (see Appendix F.1)

option (a): Choose vectors {u1, u2, . . . , uP }  RK uniformly at random from the unit 2-

sphere and set vp = Wc>up 2 RM for all p = 1, . . . , P

(P = 1 yields the spectral algorithm)

option (b): Choose vectors {u1, u2, . . . , uP }  RK as the canonical basis e1, e2, . . . , eK of

RK and set vp = Wc>up 2 RM for all p = 1, . . . , K

4: For 8p, compute Bp = WcTb(vp)Wc> 2 RKK ((52) for GP/DICA / (54) for LDA; Appendix F)

5: Perform orthogonal joint diagonalization of matrices {WcSbWc> = IK , Bp, p = 1, . . . , P }

(see [24] and [23]) to find an orthogonal matrix V 2 RKK and vectors {a1, a2, . . . , aP }  RK such that

V WcSbWc>V > = IK , and V BpV >  diag(ap), p = 1, . . . , P

6: Estimate joint diagonalization matrix A = V Wc and values ap, p = 1, . . . , P 7: Output: Estimate of D and c as described in Appendix F.6

5 Experiments

In this section, (a) we compare experimentally the GP/DICA cumulants with the LDA moments and (b) the spectral algorithm [3], the tensor power method [4] (TPM), the joint diagonalization (JD) algorithm from Algorithm 1, and variational inference for LDA [1].

Real data: the associated press (AP) dataset, from D. Blei's web page,4 with N = 2, 243 documents and M = 10, 473 vocabulary words and the average document length Lb = 194; the NIPS papers dataset5 [28] of 2, 483 NIPS papers and 14, 036 words, and Lb = 1, 321; the KOS dataset,6 from the UCI Repository, with 3, 430 documents and 6, 906 words, and Lb = 136.

Semi-synthetic data are constructed by analogy with [29]: (1) the LDA parameters D and c are learned from the real datasets with variational inference and (2) toy data are sampled from a model of interest with the given parameters D and c. This provides the ground truth parameters D and c. For each setting, data are sampled 5 times and the results are averaged. We plot error bars that are the minimum and maximum values. For the AP data, K 2 {10, 50} topics are learned and, for the NIPS data, K 2 {10, 90} topics are learned. For larger K, the obtained topic matrix is illconditioned, which violates the identifiability condition for topic recovery using moment matching techniques [3]. All the documents with less than 3 tokens are resampled.

Sampling where c is

techniques. the learned

All the c from

sampling models have the parameter c the real dataset with variational LDA,

which and c0

is is

set to c = c0 a parameter

ct/haktckw1e,

can vary. The GP data are sampled from the gamma-Poisson model (3) with b = c0/Lb so that

the expected document length is Lb (see Appendix B.2). The LDA-fix(L) data are sampled from the

LDA model (2) with the document length being fixed to a given L. The LDA-fix2( ,L1,L2) data

are sampled as follows: (1 )-portion of the documents are sampled from the LDA-fix(L1) model

with a given document length L1 and -portion of the documents are sampled from the LDA-fix(L2)

model with a given document length L2.

Evaluation. Evaluation of topic recovery for semi-synthetic data is performed with the 1-

error between the recovered err1 (Db , D) := min2PERM

Db
1 2K

aPnd

true D k kdbk

topic matrices with the best permutation of columns: dkk1 2 [0, 1]. The minimization is over the possible

permutations  2 PERM of the columns of Db and can be efficiently obtained with the Hungarian

algorithm for bipartite matching. For the evaluation of topic recovery in the real data case, we use

an approximation of the log-likelihood for held out documents as the metric [16]. See Appendix G.6

for more details.

4http://www.cs.columbia.edu/blei/lda-c 5http://ai.stanford.edu/gal/data 6https://archive.ics.uci.edu/ml/datasets/Bag+of+Words

6

1-error 1-error

11

0.8 JD 0.8 JD(k)

0.6

JD(f)

0.6

Spec

0.4

TPM

0.4

0.2 0.2

0 1 10 20 30 40 50

0 1 10 20 30 40 50

Number of docs in 1000s

Number of docs in 1000s

Figure 1: Comparison of the diagonalization algorithms. The topic matrix D and Dirichlet parameter c are learned for K = 50 from AP; c is scaled to sum up to 0.5 and b is set to fit the expected document length

Lb = 200. The semi-synthetic dataset is sampled from GP; number of documents N varies from 1, 000 to 50, 000. Left: GP/DICA moments. Right: LDA moments. Note: a smaller value of the 1-error is better.

We use our Matlab implementation of the GP/DICA cumulants, the LDA moments, and the diagonalization algorithms. The datasets and the code for reproducing our experiments are available online.7 In Appendix G.1, we discuss implementation and complexity of the algorithms. We explain how we initialize the parameter c0 for the LDA moments in Appendix G.3.

5.1 Comparison of the diagonalization algorithms
In Figure 1, we compare the diagonalization algorithms on the semi-synthetic AP dataset for K = 50 using the GP sampling. We compare the tensor power method (TPM) [4], the spectral algorithm (Spec), the orthogonal joint diagonalization algorithm (JD) described in Algorithm 1 with different options to choose the random projections: JD(k) takes P = K vectors up sampled uniformly from the unit 2-sphere in RK and selects vp = W >up (option (a) in Algorithm 1); JD selects the full basis e1, . . . , eK in RK and sets vp = W >ep (as JADE [21]) (option (b) in Algorithm 1); JD(f ) chooses the full canonical basis of RM as the projection vectors (computationally expensive). Both the GP/DICA cumulants and LDA moments are well-specified in this setup. However, the LDA moments have a slower finite sample convergence and, hence, a larger estimation error for the same value N . As expected, the spectral algorithm is always slightly inferior to the joint diagonalization algorithms. With the GP/DICA cumulants, where the estimation error is low, all algorithms demonstrate good performance, which also fulfills our expectations. However, although TPM shows almost perfect performance in the case of the GP/DICA cumulants (left), it significantly deteriorates for the LDA moments (right), which can be explained by the larger estimation error of the LDA moments and lack of robustness of TPM. The running times are discussed in Appendix G.2. Overall, the orthogonal joint diagonalization algorithm with initialization of random projections as W > multiplied with the canonical basis in RK (JD) is both computationally efficient and fast.

5.2 Comparison of the GP/DICA cumulants and the LDA moments In Figure 2, when sampling from the GP model (top, left), both the GP/DICA cumulants and LDA moments are well specified, which implies that the approximation error (i.e., the error for the infinite number of documents) is low for both. The GP/DICA cumulants achieve low values of the estimation error already for N = 10, 000 documents independently of the number of topics, while the convergence is slower for the LDA moments. When sampling from the LDA-fix(200) model (top, right), the GP/DICA cumulants are mis-specified and their approximation error is high, although the estimation error is low due to the faster finite sample convergence. One reason of poor performance of the GP/DICA cumulants, in this case, is the absence of variance in document length. Indeed, if documents with two different lengths are mixed by sampling from the LDA-fix2(0.5,20,200) model (bottom, left), the GP/DICA cumulants performance improves. Moreover, the experiment with a changing fraction of documents (bottom, right) shows that a non-zero variance on the length improves the performance of the GP/DICA cumulants. As in practice real corpora usually have a non-zero variance for the document length, this bad scenario for the GP/DICA cumulants is not likely to happen.
7 https://github.com/anastasia-podosinnikova/dica

7

11 JD-GP(10)
0.8 0.8 JD-LDA(10)
0.6 0.6 JD-GP(90) JD-LDA(90)
0.4 0.4

1-error

1-error

0.2 0.2

0 1 10 20 30 40 50
Number of docs in 1000s
1

0 1 10 20 30 40 50
Number of docs in 1000s
1

0.8 0.8

1-error

1-error

0.6 0.6

0.4 0.4

0.2 0.2

0 1 10 20 30 40 50

0 0 0.2 0.4 0.6 0.8 1

Number of docs in 1000s

Fraction of doc lengths 

Figure 2: Comparison of the GP/DICA cumulants and LDA moments. Two topic matrices and parameters c1

and c2 are learned from the NIPS dataset for K = 10 and 90; c1 and c2 are scaled to sum up to c0 = 1. Four corpora of different sizes N from 1, 000 to 50, 000: top, left: b is set to fit the expected document length

Lb = 1300; sampling from the GP model; top, right: sampling from the LDA-fix(200) model; bottom, left: sampling from the LDA-fix2(0.5,20,200) model. Bottom, right: the number of documents here is fixed to

N = 20, 000; sampling from the LDA-fix2( ,20,200) model varying the values of the fraction from 0 to 1 with the step 0.1. Note: a smaller value of the 1-error is better.

Log-likelihood (in bits)

Log-likelihood (in bits)

-11.5

JD-GP

-10.5

-12

JD-LDA

-11

Spec-GP

-12.5

Spec-LDA

-11.5

-13 VI -12

VI-JD

-13.5

-12.5

10 50 100 150

10 50 100 150

Topics K

Topics K

Figure 3: Experiments with real data. Left: the AP dataset. Right: the KOS dataset. Note: a higher value of

the log-likelihood is better.

5.3 Real data experiments

In Figure 3, JD-GP, Spec-GP, JD-LDA, and Spec-LDA are compared with variational inference (VI) and with variational inference initialized with the output of JD-GP (VI-JD). We measure held out log-likelihood per token (see Appendix G.7 for details on the experimental setup). The orthogonal joint diagonalization algorithm with the GP/DICA cumulants (JD-GP) demonstrates promising performance. In particular, the GP/DICA cumulants significantly outperform the LDA moments. Moreover, although variational inference performs better than the JD-GP algorithm, restarting variational inference with the output of the JD-GP algorithm systematically leads to better results. Similar behavior has already been observed (see, e.g., [30]).

6 Conclusion
In this paper, we have proposed a new set of tensors for a discrete ICA model related to LDA, where word counts are directly modelled. These moments make fewer assumptions regarding distributions, and are theoretically and empirically more robust than previously proposed tensors for LDA, both on synthetic and real data. Following the ICA literature, we showed that our joint diagonalization procedure is also more robust. Once the topic matrix has been estimated in a semi-parametric way where topic intensities are left unspecified, it would be interesting to learn the unknown distributions of the independent topic intensities. Aknowledgements. This work was partially supported by the MSR-Inria Joint Center. The authors would like to thank Christophe Dupuy for helpful discussions.
8

References [1] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:903-1022,
2003. [2] T. Griffiths. Gibbs sampling in the generative model of latent Dirichlet allocation. Technical report,
Stanford University, 2002. [3] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet
allocation. In NIPS, 2012. [4] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning
latent variable models. J. Mach. Learn. Res., 15:2773-2832, 2014. [5] P. Comon and C. Jutten, editors. Handbook of blind sourse separation: independent component analysis
and applications. Academic Press, 2010. [6] C. Jutten. Calcul neuromimetique et traitement du signal: analyse en composantes independantes. PhD
thesis, INP-USM Grenoble, 1987. [7] C. Jutten and J. Herault. Blind separation of sources, part I: an adaptive algorithm based on neuromimetric
architecture. Signal Process., 24:1-10, 1991. [8] P. Comon. Independent component analysis, a new concept? Signal Process., 36:287-314, 1994. [9] W.L. Buntine. Variational extensions to EM and multinomial PCA. In ECML, 2002. [10] M.E. Tipping and C.M. Bishop. Probabilistic principal component analysis. J. R. Stat. Soc., 61:611-622,
1999. [11] S. Roweis. EM algorithms for PCA and SPCA. In NIPS, 1998. [12] J. Canny. GaP: a factor model for discrete data. In SIGIR, 2004. [13] W.L. Buntine and A. Jakulin. Applying discrete PCA in data analysis. In UAI, 2004. [14] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: a nonasymptotic theory of inde-
pendence. Oxford University Press, 2013. [15] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet
allocation. CoRR, abs:1204.6703, 2013. [16] H.M. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. Evaluation methods for topic models. In
ICML, 2009. [17] J.-F. Cardoso. Source separation using higher order moments. In ICASSP, 1989. [18] J.-F. Cardoso. Eigen-structure of the fourth-order cumulant tensor with application to the blind source
separation problem. In ICASSP, 1990. [19] J.-F. Cardoso and P. Comon. Independent component analysis, a survey of some algebraic methods. In
ISCAS, 1996. [20] A. Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE Trans.
Neural Netw., 10(3):626-634, 1999. [21] J.-F. Cardoso and A. Souloumiac. Blind beamforming for non Gaussian signals. In IEE Proceedings-F,
1993. [22] J.-F. Cardoso. High-order contrasts for independent component analysis. Neural Comput., 11:157-192,
1999. [23] J.-F. Cardoso and A. Souloumiac. Jacobi angles for simultaneous diagonalization. SIAM J. Mat. Anal.
Appl., 17(1):161-164, 1996. [24] A. Bunse-Gerstner, R. Byers, and V. Mehrmann. Numerical methods for simulataneous diagonalization.
SIAM J. Matrix Anal. Appl., 14(4):927-949, 1993. [25] J. Nocedal and S.J. Wright. Numerical optimization. Springer, 2nd edition, 2006. [26] F.R. Bach and M.I. Jordan. Kernel independent component analysis. J. Mach. Learn. Res., 3:1-48, 2002. [27] V. Kuleshov, A.T. Chaganty, and P. Liang. Tensor factorization via matrix factorization. In AISTATS,
2015. [28] A. Globerson, G. Chechik, F. Pereira, and N. Tishby. Euclidean embedding of co-occurrence data. J.
Mach. Learn. Res., 8:2265-2295, 2007. [29] S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu. A practical algorithm
for topic modeling with provable guarantees. In ICML, 2013. [30] S. Cohen and M. Collins. A provably correct learning algorithm for latent-variable PCFGs. In ACL, 2014.
9

