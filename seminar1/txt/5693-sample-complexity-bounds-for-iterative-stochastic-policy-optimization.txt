Sample Complexity Bounds for Iterative Stochastic Policy Optimization

Marin Kobilarov Department of Mechanical Engineering
Johns Hopkins University Baltimore, MD 21218 marin@jhu.edu

Abstract
This paper is concerned with robustness analysis of decision making under uncertainty. We consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration. In particular, we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs. A novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation. The bound serves as a high-confidence certificate for providing future performance or safety guarantees. The approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented.

1 Introduction

We consider a general class of stochastic optimization problems formulated as

 = arg min Ep(*|)[J ( )],


(1)

where  defines a vector of decision variables,  represents the system response defined through the density p( |), and J( ) defines a positive cost function which can be non-smooth and nonconvex. It is assumed that p( |) is either known or can be sampled from, e.g. in a black-box manner. The objective is to obtain high-confidence sample complexity bounds on the expected cost for a given decision strategy by observing past realizations of possibly different strategies. Such bounds are useful for two reasons: 1) for providing robustness guarantees for future executions, and 2) for designing new algorithms that directly minimize the bound and therefore are expected to have built-in robustness.

Our primary motivation arises from applications in robotics, for instance when a robot executes control policies to achieve a given task such as navigating to a desired state while perceiving the environment and avoiding obstacles. Such problems are traditionally considered in the framework of reinforcement learning and addressed using policy search algorithms, e.g. [1, 2] (see also [3] for a comprehensive overview with a focus on robotic applications [4]). When an uncertain system model is available the problem is equivalent to robust model-predictive control (MPC) [5].

Our specific focus is on providing formal guarantees on future executions of control algorithms in terms of maximum expected cost (quantifying performance) and maximum probability of constraint violation (quantifying safety). Such bounds determine the reliability of control in the presence of process, measurement and parameter uncertainties, and contextual changes in the task. In this work we make no assumptions about nature of the system structure, such as linearity, convexity, or Gaussianity. In addition, the proposed approach applies either to a physical system without an available

1

model, to an analytical stochastic model, or to a white-box model (e.g. from a high-fidelity opensource physics engine). In this context, PAC bounds have been rarely considered but could prove essential for system certification, by providing high-confidence guarantees for future performance and safety, for instance "with 99% chance the robot will reach the goal within 5 minutes", or "with 99% chance the robot will not collide with obstacles".
Approach. To cope with such general conditions, we study robustness through a statistical learning viewpoint [6, 7, 8] using finite-time sample complexity bounds on performance based on empirical runs. This is accomplished using concentration-of-measure inequalities [9] which provide only probabilistic bounds , i.e. they certify the algorithm execution in terms of statements such as: "in future executions, with 99% chance the expected cost will be less than X and the probability of collision will be less than Y". While such bounds are generally applicable to any stochastic decision making process, our focus and initial evaluation is on stochastic control problems.
Randomized methods in control analysis. Our approach is also inspired by existing work on randomized algorithms in control theory originally motivated by robust linear control design [10]. For example, early work focused on probabilistic root-locus design [11] and later applied to constraint satisfaction [12] and general cost functions [13]. High-confidence bounds for decidability of linear stability were refined in [14]. These are closely related to the concepts of randomized stability robustness analysis (RSRA) and randomized performance robustness analysis (RPRA) [13]. Finite-time probabilistic bounds for system identification problems have also been obtained through a statistical learning viewpoint [15].
2 Iterative Stochastic Policy Optimization
Instead of directly searching for the optimal  to solve (1) a common strategy in direct policy search and global optimization [16, 17, 18, 19, 20, 21] is to iteratively construct a surrogate stochastic model (|) with hyper-parameters   V, such as a Gaussian Mixture Model (GMM), where V is a vector space. The model induces a joint density p(, |) = p( |)(|) encoding natural stochasticity p( |) and artificial control-exploration stochasticity (|). The problem is then to find  to minimize the expected cost
J (v) E ,p(*|) [J ( )], iteratively until convergence, which in many cases also corresponds to (*|) shrinking close to a delta function around the optimal  (or to multiple peaks when multiple disparate optima exist as long as  is multi-modal).
The typical flow of the iterative policy optimization algorithms considered in this work is:
Iterative Stochastic Policy Optimization (ISPO)
0. Start with initial hyper-parameters 0 (i.e. a prior), set i = 0 1. Sample M trajectories (j, j)  p(*|i) for j = 1, . . . , M 2. Compute new policy i+1 using observed costs J(j) 3. Compute bound on expected cost and Stop if below threshold, else set i = i+1 and Goto 1
The purpose of computing probably-approximate bounds is two-fold: a) to analyze the performance of such standard policy search algorithms; b) to design new algorithms by not directly minimizing an estimate of the expected cost, but by minimizing an upper confidence bound on the expected cost instead. The computed policy will thus have "built-in" robustness in the sense that, with highprobability, the resulting cost will not exceed an a-priori known value. The present paper develops bounds applicable to both (a) and (b), but only explores their application to (a), i.e. to the analysis of existing iterative policy search methods.
Cost functions. We consider two classes of cost functions J. The first class encodes system performance and is defined as a bounded real-valued function such that 0  J( )  b for any  . The second are binary-valued indicator functions representing constraint violation. Assume that the variable  must satisfy the condition g( )  0. The cost is then defined as J( ) = I{g()>0} and its expectation can be regarded as the probability of constraint violation, i.e.
P(g( ) > 0) = Ep(*|)I{g()>0}. In this work, we will be obtain bounds for both classes of cost functions.
2

3 A Specific Application: Discrete-time Stochastic Control

We next illustrate the general stochastic optimization setting using a classical discrete-time non-
linear optimal control problem. Specific instances of such control problems will later be used for numerical evaluation. Consider a discrete-time dynamical model with state xk  X, where X is an n-dimensional manifold, and control inputs uk  Rm at time tk  [0, T ] where k = 0, . . . , N denotes the time stage. Assume that the system dynamics are given by

xk+1 = fk(xk, uk, wk),

subject to gk(xk, uk)  0, gN (xN )  0,

where fk and gk correspond either to the physical plant, to an analytical model, or to a "white-box"
high-fidelity physics-engine update step. The terms wk denotes process noise. Equivalently, such a formulation induces the process model density p(xk+1|xk, uk). In addition, consider the cost

J (x0:N , u0:N-1)

N -1
Lk(xk, uk) + LN (xN ),
k=0

where x0:N {x0, . . . , xN } denotes the complete trajectory and Lk are given nonlinear functions. Our goal is to design feedback control policies to optimize the expected value of J. For simplicity,
we will assume perfect measurements although this does not impose a limitation on the approach.

Assume that any decision variables in the problem (such as feedforward or feedback gains, obstacle avoidance terms, mode switching variables) are encoded using a finite-dimensional vector   Rn and define the control law uk = k(xk) using basis functions k(x)  Rmxn for all k = 0, . . . , N - 1. This representation captures both static feedback control laws as well as time-varying
optimal control laws of the form uk = uk + KkLQR(xk - xk) where uk = B(tk) is an optimized feedforward control (parametrized using basis functions B(t)  Rmxz such as B-splines), KkLQR is the optimal feedback gain matrix of the LQR problem based on the linearized dynamics and second-order cost expansion around the optimized nominal reference trajectory x, i.e. such that xk+1 = fk(xk, uk, 0).

The complete trajectory of the system is denoted by the random variable  = (x0:N , u0:N-1) and

has density p( |) = p(x0)Nk=-01p(xk+1|xk, uk)(uk - k(xk)), where (*) is the Dirac delta.

The trajectory constraint takes the form {g( )  0}

N -1 k=0

{gk (xk ,

uk )



0}



{gN

(xN

)



0}.

A simple example. As an example, consider a point-mass robot modeled as a double-integrator system with state x = (p, v) where p  Rd denotes position and v  Rd denotes velocity with d = 2
for planar workspaces and d = 3 for 3-D workspaces. The dynamics is given, for t = T /N , by

pk+1

=

pk

+

tvk

+

1 2

t2(uk

+

wk ),

vk+1 = vk + t(uk + wk),

where uk are the applied controls and wk is zero-mean white noise. Imagine that the constraint gk(x, u)  0 defines circular obstacles O  Rd and control norm bounds defined as

ro - p - po  0,

u  umax,

where ro is the radius of an obstacle at position po  Rd. The cost J could be arbitrary but a

typical choice is L(x, u) function defining a task.

=

1 2

The

u

2 R

+

q(x)

where

R

>

final cost could force the

0 is a given matrix and q(x) is system towards a goal state xf

a nonlinear  Rn (or a

region

Xf



Rn)

and

could

be

defined

according

to

LN (x)

=

1 2

x - xf

2 Qf

for

some

given

matrix

Qf  0. For such simple systems one can choose a smooth feedback control law uk = k(x) with

static positive gains  = (kp, kd, ko)  R3 and basis function

(x) = [ pf - p vf - v (x, O) ] ,

where (x, O) is an obstacle-avoidance force, e.g. defined as the gradient of a potential field or as a gyroscopic "steering" force (x, O) = s(x, O) x v that effectively rotates the velocity vector [22] . Alternatively, one could employ a time-varying optimal control law as described in 3.

3

4 PAC Bounds for Iterative Policy Adaptation

We next compute probabilistic bounds on the expected cost J () resulting from the execution of a new stochastic policy with hyper-parameters  using observed samples from previous policies 0, 1, . . . . The bound is agnostic to how the policy is updated (i.e. Step 2 in the ISPO algorithm).

4.1 A concentration-of-measure inequality for policy adaptation

The stochastic optimization setting naturally allows the use of a prior belief   (*|0) on what good control laws could be, for some known 0  V. After observing M executions based on such prior, we wish to find a new improved policy (*|) which optimizes the cost

(|) J () E,p(*|)[J ( )] = E,p(*|0) J ( ) (|0) ,

(2)

which can be approximated using samples j  (|0) and j  p( |j) by the empirical cost

1 M

M j=1

J

(j

)

(j | ) (j |0 )

.

(3)

The goal is to compute the parameters  using the sampled decision variables j and the corre-

sponding observed costs J(j). Obtaining practical bounds for J () becomes challenging since the

change-of-measure

likelihood

ratio

(|)  (|0 )

can

be

unbounded

(or

have

very

large

values)

[23]

and

a

standard bound, e.g. such as Hoeffding's or Bernstein's becomes impractical or impossible to apply.

To cope with this we will employ a recently proposed robust estimation [24] technique stipulating

that instead of estimating the expectation m = E[X] of a random variable X  [0, ) using its

empirical

mean

m

=

1 M

M j=1

Xj

,

a

more

robust

estimate

can

be

obtained

by

truncating

its

higher

moments, i.e. using m

1 M

M j=1

(Xj

)

for

some



>

0

where

(x) = log(1 + x + 1 x2). 2

(4)

What makes this possible is the key assumption that the (possibly unbounded) random variable must

have bounded second moment. We employ this idea to deal with the unboundedness of the policy

adaptation ratio by showing that in fact its second moment can be bounded and corresponds to an

information distance between the current and previous stochastic policies.

To obtain sharp bounds though it is useful to employ samples over multiple iterations of the ISPO

algorithm, i.e. from policies 0, 1, . . . , L-1 computed in previous iterations. To simplify notation

let z = (, ) and define i(z, )

J( )

(|)  (|i )

.

The

cost

(2)

of

executing



can

now

be

equivalently

expressed as

J () 

1 L-1 L Ezp(*|i)

i(z, )

i=0

using the computed policies in previous iterations i = 0, . . . , L - 1. We next state the main result:

Proposition 1. With probability 1 -  the expected cost of executing a stochastic policy with parameters   (*|) is bounded according to:

J ()  inf
>0

J() +

 2L

L-1
b2i eD2((*|)||(*|i))

+

1 LM

log

1 

i=0

,

(5)

where J() denotes a robust estimator defined by

J()

1 L-1 M

LM

 ( (zij, )) ,

i=0 j=1

computed after L iterations, with M samples zi1, . . . , ziM  p(*|i) obtained at iterations i = 0, . . . , L - 1, where D(p||q) denotes the Renyii divergence between p and q defined by

D (p||q)

=



1 -

1

log

p (x) q-1(x) dx.

The constants bi are such that 0  J( )  bi at each iteration i = 0, . . . , L - 1.

4

Proof. The bound is obtained by relating the mean to its robust estimate according to

P LM (J () - J())  t

= P eLM(J ()-J())  et ,

 E eLM(J ()-J()) e-t,

= e E e-t+LMJ ()

L-1 i=0

M j=1

-(

i(zij ,))


L-1 M



= e-t+LMJ E 

e-(

i(zij ,))


i=0 j=1

L-1 M

= e-t+LMJ

E zp(*|i)

1-

i(z, ) +

2 2

i(z, )2

i=0 j=1

L-1 M
= e-t+LM J ()

1 - J () +

2 2E

z p(*|i ) [

i(z, )2]

i=0 j=1

L-1 M

 e-t+LM J ()

e-J

()+

2 2

Ezp(*|i ) [

i (z , )2 ]



e-t+M

2 2

i=0 j=1

,L-1
i=0

Ezp(*|i ) [

i (z , )2 ]

(6) (7) (8)

using 1+x

Markov's inequality to obtain  ex in (8). Here, we adapted

(6),

the

identities

(x)



- log(1

-x

+

1 2

x2

)

in

the moment-truncation technique proposed by Catoni

(7) and [24] for

general unbounded losses to our policy adaptation setting in order to handle the possibly unbounded

likelihood ratio. These results are then combined with

E [ i(z, )2]  b2i E(*|i)

(|)2 (|i)2

= b2i eD2(||i),

where the relationship between the likelihood ratio variance and the Renyii divergence was established in [23].

Note that the Renyii divergence can be regarded as a distance between two distribution and can be
computed in closed bounded form for various distributions such as the exponential families; it is also closely related to the Kullback-Liebler (KL) divergence, i.e. D1(p||q) = KL(p||q).

4.2 Illustration using simple robot navigation

We next illustrate the application of these bounds using the simple scenario introduced in 3. The
stochasticity is modeled using a Gaussian density on the initial state p(x0), on the disturbances wk and on the goal state xf . Iterative policy optimization is performed using a stochastic model (|) encoding a multivariate Gaussian, i.e.

(|) = N (|, )

which is updated through reward-weighted-regression (RWR) [3], i.e. in Step 2 of the ISPO algorithm we take M samples, observe their costs, and update the parameters according to

M
 = w(j)j,
j=1

M
 = w(j)(j - )(j - )T ,
j=1

(9)

using the tilting weights w( ) = e-J() for some adaptively chosen  > 0 and where w(j)

w(j )/

M =1

w(

)

are

the

normalized

weights.

At each iteration i one can compute a bound on the expected cost using the previously computed 0, . . . , i-1. We have computed such bounds using (5) for both the expected cost and probability of

5

555 000

obstacles

goal

-5 -5 -5

obstacles

-10 -10 -10

-15 -15

-10

-5

0

iteration #1

sampled start states

-15 -15

5 -15

-10

-5

0

5 -15

-10

-5

0

5

iteration #4
8 7 6 5 4 3

iteration #9

Expected Cost

empirical J

robust

J


PAC bound J +

0.7 0.6

0.5

0.4

0.3

iteration #28
Probability of Collision
empirical P robust P PAC bound P +

2 0.2

1 0.1

00 0 5 10 15 20 25 30 0 5 10 15 20 25 30

iterations

iterations

a) b) c)
Figure 1: Robot navigation scenario based on iterative policy improvement and resulting predicted performance: a) evolution of the density p(|) over the decision variables (in this case the control gains); b) cost function J and its computed upper bound J + for future executions; c) analogous bounds on probability-ofcollision P ; snapshots of sampled trajectories (top). Note that the initial policy results in  30% collisions.
Surprisingly, the standard empirical and robust estimates are nearly identical.

collision, denoted respectively by J + and P + using M = 200 samples (Figure 1) at each iteration. We used a window of maximum L = 10 previous iterations to compute the bounds, i.e. to compute i+1 all samples from densities i-L+1, i-L+2, . . . , i were used. Remarkably, using our robust statistics approach the resulting bound eventually becomes close to the standard empirical estimate J . The collision probability bound P + decreses to less than 10% which could be further improved by employing more samples and more iterations. The significance of these bounds is that one can stop the optimization (regarded as training) at any time and be able to predict expected performance in future executions using the newly updated policy before actually executing the policy, i.e. using the samples from the previous iteration.
Finally, the Renyii divergence term used in these computations takes the simple form

D (N (*|0, 0)

N (*|1, 1))

=

 2

1 - 0

2 - 1

+

1 2(1 -

)

log

| | |0|1- |1|

,

where  = (1 - )0 + 1.

4.3 Policy Optimization Methods
We do not impose any restrictions on the specific method used for optimizing the policy (|). When complex constraints are present such computation will involve a global motion planning step combined with local feedback control laws (we show such an example in 5). The approach can be used to either analyze such policies computed using any method of choice or to derive new algorithms based on minimizing the right-hand side of the bound. The method also applies to modelfree learning. For instance, related to recent methods in robotics one could use reward-weightedregression (RWR) or policy learning by weighted samples with returns (PoWeR) [3], stochastic optimization methods such as [25, 26], or using the related cross-entropy optimization [16, 27].
6

5 Application to Aerial Vehicle Navigation

Consider an aerial vehicle such as a quadrotor navigating at high speed through a cluttered environ-
ment. We are interested in minimizing a cost metric related to the total time taken and control effort
required to reach a desired goal state, while maintaining low probability of collision. We employ an
experimentally identified model of an AscTec quadrotor (Figure 2) with 12-dimensional state space X = SE(3) x R6 with state x = (p, R, p, ) where p  R3 is the position, R  SO(3) is the rotation matrix, and   R3 is the body-fixed angular velocity. The vehicle is controlled with inputs u = (F, M )  R4 including the lift force F  0 and torque moments M  R3. The dynamics is

mp = Re3F + mg + (p, p),

(10)

R = R,

(11)

J = J x  + M,

(12)

where m is the mass, J-the inertia tensor, e3 = (0, 0, 1) and the matrix  is such that  =  x  for any   R3. The system is subject to initial localization errors and also to random disturbances, e.g. due to wind gusts and wall effects, defined as stochastic forces (p, p)  R3. Each component
in  is zero-mean and has standard deviation of 3 Newtons, for a vehicle with mass m  1 kg.

The objective is to navigate through a given urban environment at high speed to a desired goal state. We employ a two-stage approach consisting of an A*-based global planner which produces a sequence of local sub-goals that the vehicle must pass through. A standard nonlinear feedback backstepping controller based on a "slow" position control loop and a "fast" attitude control is employed [28, 29] for local control. In addition, and obstacle avoidance controller is added to avoid collisions since the vehicle is not expected to exactly follow the A* path. At each iteration M = 200 samples are taken with 1 -  = 0.95 confidence level. A window of L = 5 past iterations were used for the bounds. The control density (|) is a single Gaussian as specified in 4.2. The most sensitive gains in the controller are the position proporitional and derivative terms, and the obstacle gains, denoted by kp, kd, and ko, which we examine in the following scenarios:
a) fixed goal, wind gusts disturbances, virtual environment: the system is first tested in a cluttered simulated environment (Figure 2). The simulated vehicle travels at an average velocity of 20 m/s (see video in Supplement) and initially experiences more than 50% collisions. After a few iterations the total cost stabilizes and the probability of collision reduces to around 15%. The bound is close to the empirical estimate which indicates that it can be tight if more samples are taken. The collision probability bound is still too high to be practical but our goal was only to illustrate the bound behavior. It is also likely that our chosen control strategy is in fact not suitable for high-speed traversal of such tight environments.
b) sparser campus-like environment, randomly sampled goals: a more general evaluation was performed by adding the goal location to the stochastic problem parameters so that the bound will apply to any future desired goal in that environment (Figure 3). The algorithm converges to similar values as before, but this time the collision probability is smaller due to more expansive environment. In both cases, the bounds could be reduced further by employing more than M = 200 samples or by reusing more samples from previous runs according to Proposition 1.

6 Conclusion
This paper considered stochastic decision problems and focused on a probably-approximate bounds on robustness of the computed decision variables. We showed how to derive bounds for fixed policies in order to predict future performance and/or constraint violation. These results could then be employed for obtaining generalization PAC bounds, e.g. through a PAC-Bayesian approach which could be consistent with the proposed notion of policy priors and policy adaptation. Future work will develop concrete algorithms by directly optimizing such PAC bounds, which are expected to have built-in robustness properties.
References
[1] Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, pages 1057-1063, 1999.
[2] Csaba Szepesvari. Algorithms for Reinforcement Learning. Morgan and Claypool Publishers, 2010.
[3] M. P. Deisenroth, G. Neumann, and J. Peters. A survey on policy search for robotics. pages 388-403, 2013.

7

AscTec Pelican

simulated quadrotor motion

A* waypoint path

iteration #1
350 300 250 200

iteration #5

Expected Cost

empirical J

robust

J


PAC bound J +

1 0.8

0.6

iteration #17
Probability of Collision
empirical P robust P PAC bound P +

150 0.4
100 0.2
50

00 0 5 10 15 20 25 0 5 10 15 20 25

iterations

iterations

a) b) c) Figure 2: Aerial vehicle navigation using a simulated nonlinear quadrotor model (top). Iterative stochastic
policy optimization iterations (a,b,c) analogous to those given in Figure 1. Note that the initial policy results in
over 50% collisions which is reduced to less than 10% after a few policy iterations.

random Goals

campus map

Start iteration #1

iteration #4

iteration #10

Expected Cost 400

Probability of Collision 1

empirical J

empirical P

350

robust J PAC bound J + 0.8

robust P PAC bound P +

300

250 0.6 200
0.4 150

100 0.2
50

00 0 5 10 15 0 5 10 15

iterations

iterations

a) b) c) Figure 3: Analogous plot to Figure 2 but for a typical campus environment using uniformly at random sampled
goal states along the northern boundary. The vehicle must fly below 100 feet and is not allowed to fly above
buildings. This is a larger less constrained environment resulting in less collisions.

8

[4] S. Schaal and C. Atkeson. Learning control in robotics. Robotics Automation Magazine, IEEE, 17(2):20 -29, june 2010.
[5] Alberto Bemporad and Manfred Morari. Robust model predictive control: A survey. In A. Garulli and A. Tesi, editors, Robustness in identification and control, volume 245 of Lecture Notes in Control and Information Sciences, pages 207-226. Springer London, 1999.
[6] Vladimir N. Vapnik. The nature of statistical learning theory. Springer-Verlag New York, Inc., New York, NY, USA, 1995.
[7] David A. McAllester. Pac-bayesian stochastic model selection. Mach. Learn., 51:5-21, April 2003.
[8] J Langford. Tutorial on practical prediction theory for classification. Journal of Machine Learning Research, 6(1):273-306, 2005.
[9] Stphane Boucheron, Gbor Lugosi, Pascal Massart, and Michel Ledoux. Concentration inequalities : a nonasymptotic theory of independence. Oxford university press, Oxford, 2013.
[10] M. Vidyasagar. Randomized algorithms for robust controller synthesis using statistical learning theory. Automatica, 37(10):1515-1528, October 2001.
[11] Laura Ryan Ray and Robert F. Stengel. A monte carlo approach to the analysis of control system robustness. Automatica, 29(1):229-236, January 1993.
[12] Qian Wang and RobertF. Stengel. Probabilistic control of nonlinear uncertain systems. In Giuseppe Calafiore and Fabrizio Dabbene, editors, Probabilistic and Randomized Methods for Design under Uncertainty, pages 381-414. Springer London, 2006.
[13] R. Tempo, G. Calafiore, and F. Dabbene. Randomized algorithms for analysis and control of uncertain systems. Springer, 2004.
[14] V. Koltchinskii, C.T. Abdallah, M. Ariola, and P. Dorato. Statistical learning control of uncertain systems: theory and algorithms. Applied Mathematics and Computation, 120(13):31 - 43, 2001. ce:titleThe Bellman Continuum/ce:title.
[15] M. Vidyasagar and Rajeeva L. Karandikar. A learning theory approach to system identification and stochastic adaptive control. Journal of Process Control, 18(34):421 - 430, 2008. Festschrift honouring Professor Dale Seborg.
[16] Reuven Y. Rubinstein and Dirk P. Kroese. The cross-entropy method: a unified approach to combinatorial optimization. Springer, 2004.
[17] Anatoly Zhigljavsky and Antanasz Zilinskas. Stochastic Global Optimization. Spri, 2008.
[18] Philipp Hennig and Christian J. Schuler. Entropy search for information-efficient global optimization. J. Mach. Learn. Res., 98888:1809-1837, June 2012.
[19] Christian Igel, Nikolaus Hansen, and Stefan Roth. Covariance matrix adaptation for multi-objective optimization. Evol. Comput., 15(1):1-28, March 2007.
[20] Pedro Larraaga and Jose A. Lozano, editors. Estimation of distribution algorithms: A new tool for evolutionary computation. Kluwer Academic Publishers, 2002.
[21] Martin Pelikan, David E. Goldberg, and Fernando G. Lobo. A survey of optimization by building and using probabilistic models. Comput. Optim. Appl., 21:5-20, January 2002.
[22] Howie Choset, Kevin M. Lynch, Seth Hutchinson, George A Kantor, Wolfram Burgard, Lydia E. Kavraki, and Sebastian Thrun. Principles of Robot Motion: Theory, Algorithms, and Implementations. MIT Press, June 2005.
[23] Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning Bounds for Importance Weighting. In Advances in Neural Information Processing Systems 23, 2010.
[24] Olivier Catoni. Challenging the empirical mean and empirical variance: A deviation study. Ann. Inst. H. Poincar Probab. Statist., 48(4):1148-1185, 11 2012.
[25] E. Theodorou, J. Buchli, and S. Schaal. A generalized path integral control approach to reinforcement learning. Journal of Machine Learning Research, (11):3137-3181, 2010.
[26] Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Neural Information Processing Systems (NIPS), 2014.
[27] M. Kobilarov. Cross-entropy motion planning. International Journal of Robotics Research, 31(7):855- 871, 2012.
[28] Robert Mahony and Tarek Hamel. Robust trajectory tracking for a scale model autonomous helicopter. International Journal of Robust and Nonlinear Control, 14(12):1035-1059, 2004.
[29] Marin Kobilarov. Trajectory tracking of a class of underactuated systems with external disturbances. In American Control Conference, pages 1044-1049, 2013.
9

