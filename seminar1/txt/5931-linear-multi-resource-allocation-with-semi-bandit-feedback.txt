Linear Multi-Resource Allocation with Semi-Bandit Feedback

Tor Lattimore Department of Computing Science
University of Alberta, Canada tor.lattimore@gmail.com

Koby Crammer Department of Electrical Engineering
The Technion, Israel koby@ee.technion.ac.il

Csaba Szepesvari Department of Computing Science
University of Alberta, Canada szepesva@ualberta.ca

Abstract
We study an idealised sequential resource allocation problem. In each time step the learner chooses an allocation of several resource types between a number of tasks. Assigning more resources to a task increases the probability that it is completed. The problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy. Our main contribution is the new setting and an algorithm with nearly-optimal regret analysis. Along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise. We also present some new results for stochastic linear bandits on the hypercube that significantly improve on existing work, especially in the sparse case.

1 Introduction

Economist Thomas Sowell remarked that "The first lesson of economics is scarcity: There is never enough of anything to fully satisfy all those who want it."1 The optimal allocation of resources is
an enduring problem in economics, operations research and daily life. The problem is challenging
not only because you are compelled to make difficult trade-offs, but also because the (expected)
outcome of a particular allocation may be unknown and the feedback noisy.

We focus on an idealised resource allocation problem where the economist plays a repeated resource

allocation game with multiple resource types and multiple tasks to which these resources can be

assigned. Specifically, we consider a (nearly) linear model with D resources and K tasks. In each time step t the economist chooses an allocation of resources Mt  RDxK where Mtk  RD is the kth column and represents the amount of each resource type assigned to the kth task. We assume

that the kth task is completed successfully with probability min {1, Mtk, k } and k  RD is an

unknown non-negative vector that determines how the success rate of a given task depends on the

quantity and type of resources assigned to it. Naturally we will limit the availability of resources

by demanding that Mt satisfies

K k=1

Mtdk



1

for

all

resource

types

d.

At

the

end

of

each

time

step the economist observes which tasks were successful. The objective is to maximise the number

of successful tasks up to some time horizon n that is known in advance. This model is a natural

generalisation of the one used by Lattimore et al. [2014], where it was assumed that there was a

single resource type only.

1He went on to add that "The first lesson of politics is to disregard the first lesson of economics." Sowell [1993]

1

An example application might be the problem of allocating computing resources on a server between a number of Virtual Private Servers (VPS). In each time step (some fixed interval) the controller chooses how much memory/cpu/bandwidth to allocate to each VPS. A VPS is said to fail in a given round if it fails to respond to requests in a timely fashion. The requirements of each VPS are unknown in advance, but do not change greatly with time. The controller should learn which VPS benefit the most from which resource types and allocate accordingly.
The main contribution of this paper besides the new setting is an algorithm designed for this problem along with theoretical guarantees on its performance in terms of the regret. Along the way we present some additional results for the related problem of minimising regret for stochastic linear bandits on the hypercube. We also prove new concentration results for weighted least squares estimation, which may be independently interesting.
The generalisation of the work of Lattimore et al. [2014] to multiple resources turns out to be fairly non-trivial. Those with knowledge of the theory of stochastic linear bandits will recognise some similarity. In particular, once the nonlinearity of the objective is removed, the problem is equivalent to playing K linear bandits in parallel, but where the limited resources constrain the actions of the learner and correspondingly the returns for each task. Stochastic linear bandits have recently been generating a significant body of research (e.g., Auer [2003], Dani et al. [2008], Rusmevichientong and Tsitsiklis [2010], Abbasi-Yadkori et al. [2011, 2012], Agrawal and Goyal [2012] and many others). A related problem is that of online combinatorial optimisation. This has an extensive literature, but most results are only applicable for discrete action sets, are in the adversarial setting, and cannot exploit the additional structure of our problem. Nevertheless, we refer the interested reader to (say) the recent work by Kveton et al. [2014] and references there-in. Also worth mentioning is that the resource allocation problem at hand is quite different to the "linear semi-bandit" proposed and analysed by Krishnamurthy et al. [2015] where the action set is also finite (the setting is different in many other ways besides).
Given its similarity, it is tempting to apply the techniques of linear bandits to our problem. When doing so, two main difficulties arise. The first is that our payoffs are non-linear: the expected reward is a linear function only up to a point after which it is clipped. In the resource allocation problem this has a natural interpretation, which is that over-allocating resources beyond a certain point is fruitless. Fortunately, one can avoid this difficulty rather easily by ensuring that with high probability resources are never over-allocated. The second problem concerns achieving good regret regardless of the task specifics. In particular, when the number of tasks K is large and resources are at a premium the allocation problem behaves more like a K-armed bandit where the economist must choose the few tasks that can be completed successfully. For this kind of problem regret should scale in the worst case with K only [Auer et al., 2002, Bubeck and Cesa-Bianchi, 2012]. The standard linear bandits approach, on the other hand, would lead to a bound on the regret that depends linearly on K. To remedy this situation, we will exploit that if K is large and resources are scarce, then many tasks will necessarily be under-resourced and will fail with high probability. Since the noise model is Bernoulli, the variance of the noise for these tasks is extremely low. By using weighted least-squares estimators we are able to exploit this and thereby obtain an improved regret. An added benefit is that when resources are plentiful, then all tasks will succeed with high probability under the optimal allocation, and in this case the variance is also low. This leads to a poly-logarithmic regret for the resource-laden case where the optimal allocation fully allocates every task.

2 Preliminaries

If F is some event, then F is its complement (i.e., it is the event that F does not occur). If A is

positive definite and x is a vector, then

x

2 A

=

x

Ax stands for the weighted 2-norm. We write |x|

to be the vector of element-wise absolute values of x. We let   RDxK be a matrix with columns

1, . . . K. All entries in  are non-negative, but otherwise we make no global assumptions on . At

each time step t the learner chooses an allocation matrix Mt  M where

K
M = M  [0, 1]DxK : Mdk  1 for all d .

k=1

The assumption that each resource type has a bound of 1 is non-restrictive, since the units of any resource can be changed to accommodate this assumption. We write Mtk  [0, 1]D for the kth

2

column of Mt. The reward at time step t is Yt 1 where Ytk  {0, 1} is sampled from a Bernoulli distribution with parameter ( Mtk, k ) = min {1, Mtk, k }. The economist observes all Ytk, however, not just the sum. The optimal allocation is denoted by M  and defined by

K
M  = arg max ( Mk, k ) .
M M k=1
We are primarily concerned with designing an allocation algorithm that minimises the expected (pseudo) regret of this problem, which is defined by

K nK

Rn = n ( Mk, k ) - E

( Mtk, k ) ,

k=1

t=1 k=1

where the expectation is taken over both the actions of the algorithm and the observed reward.

Optimal Allocations

If  is known, then the optimal allocation can be computed by constructing an appropriate linear program. Somewhat surprisingly it may also be computed exactly in O(K log K + D log D) time using Algorithm 1 below. The optimal allocation is not so straight-forward as, e.g., simply allocating resources to the incomplete task for which the corresponding  is largest in some dimension. For example, for K = 2 tasks and d = 2 resource types:

=

1 2

=

0 1/2

1/2 1

=

M =

M1 M2

=

0 1/2

1 1/2

.

We see that even though 22 is the largest parameter, the optimal allocation assigns only half of the second resource (d = 2) to this task. The right approach is to allocate resources to incomplete tasks using the ratios as prescribed by Algorithm 1. The intuition for allocating in this way is that resources should be allocated as efficiently as possible, and efficiency is determined by the ratio of the expected success due to the allocation of a resource and the amount of resources allocated. Theorem 1. Algorithm 1 returns M .

Algorithm 1

Input:  M = 0  RDxK and B = 1  RD while  k, d s.t Mk, k < 1 and Bd > 0 do

A = {k : Mk, k < 1} and B = {d : Bd > 0}

k, d = arg max min

dk

(k,d)AxB iA\{k} di

Mdk = min
end while return M

1- Bd ,

Mk, k dk

The proof of Theorem 1 and an implementation of Algorithm 1 may be found in the supplementary material.

We are interested primarily in the case when  is unknown, so Algorithm 1 will not be directly applicable. Nevertheless, the algorithm is useful as a module in the implementation of a subsequent algorithm that estimates  from data.

3 Optimistic Allocation Algorithm

We follow the optimism in the face of uncertainty principle. In each time step t, the algorithm

constructs an estimator kt for each k and a corresponding confidence set Ctk for which k  Ctk holds with high probability. The algorithm then takes the optimistic action subject to the assumption

that k does indeed lie in Ctk for all k. The main difficulty is the construction of the confidence sets. Like other authors [Dani et al., 2008, Rusmevichientong and Tsitsiklis, 2010, Abbasi-Yadkori et al.,

2011] we define our confidence sets to be ellipses, but the use of a weighted least-squares estimator

means that our ellipses may be significantly smaller than the sets that would be available by using

these previous works in a straightforward way. The algorithm accepts as input the number of tasks

and resource types, the horizon and constants  > 0 and  where constant  is defined by

1 = ,
nK

N = 4n4D2 D ,

B  max
k

k

2 2

,

so that

 6nN 3nN

 = 1 + B + 2 log

log



2
.

(1)

3

Note that B must be a known bound on maxk k 22, which might seem like a serious restriction, until one realizes that it is easy to add an initialisation phase where estimates are quickly made while incurring minimal additional regret, as was also done by Lattimore et al. [2014]. The value of  determines the level of regularisation in the least squares estimation and will be tuned later to optimise the regret.

Algorithm 2 Optimistic Allocation Algorithm

1: Input K, D, n, ,  2: for t  1, . . . , n do 3: // Compute confidence sets for all tasks k: 4: Gtk = I + <t kMkMk
5: tk = G-tk1  <t  kM Y k

6:

Ctk =

k :

k - tk

2 Gtk





and Ctk =

k :

k - tk

2 Gtk



4

7: // Compute optimistic allocation: 8: Mt = arg maxMtM maxkCtk ( Mtk, k )
9: // Observe success indicators Ytk for all tasks k: 10: Ytk  Bernoulli(( Mtk, k ))

11: // Compute weights for all tasks k: 12: t-k1 = arg maxkCtk Mtk, k (1 - Mtk, k ) 13: end for

Computational Efficiency

We could not find an efficient implementation of Algorithm 2 because solving the bilinear optimisation problem in Line 8 is likely to be NP-hard (Bennett and Mangasarian [1993] and also Petrik and Zilberstein [2011]). In our experiments we used a simple algorithm based on optimising for M and  in alternative steps combined with random restarts, but for large D and K this would likely not be efficient. In the supplementary material we present an alternative algorithm that is efficient, but relies on the assumption that k 1  1 for all k. In this regime it is impossible to over-allocate resources and this fact can be exploited to obtain an efficient and practical algorithm with strong guarantees. Along the way, we are able to construct an elegant algorithm for linear bandits on the hypercube that enjoys optimal regret and adapts to sparsity.

Computing the weights tk (Line 12) is (somewhat surprisingly) straight-forward. Define

ptk = Mtk, tk + 2  Mtk G-tk1 Then the weights can be computed by

and ptk = Mtk, tk - 2  Mtk G-tk1 .

ptk(1 - ptk) t-k1 = ptk(1 - ptk)
1
4

if

ptk



1 2

if

ptk



1 2

otherwise .

(2)

A curious reader might wonder why the weights are computed by optimising within confidence set Ctk, which has double the radius of Ctk. The reason is rather technical, but essentially if the true parameter k were to lie on the boundary of the confidence set, then the corresponding weight could become infinite. For the analysis to work we rely on controlling the size of the weights. It is not clear whether or not this trick is really necessary.

4 Worst-case Regret for Algorithm 2
We now analyse the regret of Algorithm 2. First we offer a worst-case bound on the regret that depends on the time-horizon like O( n). We then turn our attention to the resource-laden case where the optimal allocation satisfies Mk, k = 1 for all k. In this instance we show that the dependence on the horizon is only poly-logarithmic, which would normally be unexpected when the

4

action-space is continuous. The improvement comes from the weighted estimation that exploits the fact that the variance of the noise under the optimal allocation vanishes.

Theorem 2.

Suppose Algorithm 2 is run with bound B  maxk

k

2 2

.

Then

Rn  1 + 4D

2nK

max
k

k

+4

/

log(1 + 4n2) .

Choosing  = B-1 log

6nN 

log

3nN 

and assuming that B  O(maxk

k

2 2

),

then

Rn  O

D3/2

nK max
k

k

2 log n

.

The proof of Theorem 2 will follow by carefully analysing the width of the confidence sets as the algorithm makes allocations. We start by proving the validity of the confidence sets, and then prove the theorem.

Weighted Least Squares Estimation

For this M1, . . . ,

sub-section we focus Mn be a sequence of

on the problem of estimating a allocations to task k with Mt 

single unknown  = RD. Let {Ft}nt=0 be a

k. Let filtration

with Ft containing information available at the end of round t, which means that Mt is Ft-1-

measurable. Let 1, . . . , n be the sequence of weights chosen by Algorithm 2. The sequence of

outcomes is Y1, . . . , Yn  {0, 1} for which E[Yt|Ft-1] = ( Mt,  ). The weighted regularised

gram matrix is Gt = I + <t  M M and the corresponding weighted least squares estimator

is

t = G-t 1 tM Y .
 <t

Theorem 3. If



2 2

 B and  is chosen as in Eq. (1), then

t - 

2 Gt



 for all t  n with

probability at least 1 -  = 1/(nK).

Similar results exist in the literature for unweighted least-squares estimators (for example, Dani et al. [2008], Rusmevichientong and Tsitsiklis [2010], Abbasi-Yadkori et al. [2011]). In our case, however, Gt is the weighted gram matrix, which may be significantly larger than an unweighted version when the weights become large. The proof of Theorem 3 is unfortunately too long to include in the main text, but it may be found in the supplementary material.

Analysing the Regret

We start with some technical lemmas. Let F be the failure event that

tk - k

2 Gtk

>  for some

t  n and 1  k  K.

Lemma 4 (Abbasi-Yadkori et al. [2012]). Let x1, . . . , xn be an arbitrary sequence of vectors with

xt

2 2



c

and

let

Gt

=

I

+

t-1 s=1

xsxs

.

Then

n t=1

min

1,

x2
t G-t 1

 2D log

1

+

c*n D

.

n

Corollary 5. If F does not hold, then

tk min

1,

Mtk

2 G-tk1

 8D log(1 + 4n2).

t=1

The proof is omitted, but follows rather easily by showing that tk can be moved inside the minimum at a price of increasing the loss at most by a factor of four, and then applying Lemma 4. See the
supplementary material for the formal proof.

K

Lemma 6. Suppose F does not hold, then

t-k1  D

max
k

k

+4

/ .

k=1

5

Proof. We exploit the fact that t-k1 is an estimate of the variance, which is small whenever Mtk 1 is small:

t-k1 = arg max Mtk, k (1 - Mtk, k )  arg max Mtk, k

k Ctk

k Ctk

(a)

= Mtk,  + arg max Mtk, k - 
k Ctk



Mtk 1 k  + 4

 Mtk G-tk1

(b) (c)
 Mtk 1 k  + 4  Mtk I/  Mtk 1 k  + 4 / ,

where (a) follows from Cauchy-Schwartz and the fact that k  Ctk, (b) since G-tk1  I/ and basic

linear algebra, (c) since Mtk I/ = 1/ Mtk 2  1/ Mtk 1. The result is completed

since the resource constraints implies that

K k=1

Mtk

1  D.

Proof of Theorem 2. By Theorem 3 we have that F holds with probability at most  = 1/(nK). If F does not hold, then by the definition of the confidence set we have k  Ctk for all t and k. Therefore

nK

nK

Rn = E

( Mk, k - ( Mtk, k ))  1 + E 1 {F }

Mk - Mtk, k .

t=1 k=1

t=1 k=1

Note that we were able to replace ( Mtk, k ) = Mtk, k , since if F does not hold, then Mtk will never be chosen in such a way that resources are over-allocated. We will now assume that F

does not hold and bound the argument in the expectation. By the optimism principle we have:

nK
Mk - Mtk, k
t=1 k=1

(a) n K
 min {1, Mtk, tk - k }

t=1 k=1

(b) n K
 min 1, Mtk G-tk1 tk - k Gtk
t=1 k=1

(c) n K

2

min 1, Mtk G-tk1

t=1 k=1



(d)
2

n
n
t=1

K
min 1, Mtk G-tk1
k=1

2

(e)
2

n
n
t=1

K
t-k1
k=1

K

tk min

1,

M2
tk G-tk1

k=1

(f ) 

2

nD

max
k

k

+4



n


K

tk min

1,

M2
tk G-tk1

t=1 k=1

(g)
 4D

2nK

max
k

k

+4

 

log(1 + 4n2) .

where (a) follows from the assumption that k  Ctk for all t and k and since Mt is chosen optimistically, (b) by the Cauchy-Schwarz inequality, (c) by the definition of kt, which lies inside Ctk, (d) by Jensen's inequality, (e) by Cauchy-Schwarz again, (f) follows from Lemma 6. Finally (g)
follows from Corollary 5.

5 Regret in Resource-Laden Case
We now show that if there are enough resources such that the optimal strategy can complete every task with certainty, then the regret of Algorithm 2 is poly-logarithmic (in contrast to O( n) otherwise). As before we exploit the low variance, but now the variance is small because Mtk, k is

6

close to 1, while in the previous section we argued that this could not happen too often (there is no

contradiction as the quantity maxk k appeared in the previous bound).

Theorem 7. If

K k=1

Mk, k

= K, then Rn  1 + 8KD log(1 + 4n2).

Proof. We start by showing that the weights are large:

t-k1

=

max
 Ctk

Mtk, 

(1 -

Mtk,  )  max (1 -
 Ctk

Mtk,  )

 max Mtk,  - 
, Ctk



Mtk

G-tk1

max
, Ctk

 -  Gtk 

Mtk 4G-tk1

.

Applying the optimism principle and using the bound above combined with Corollary 5 gives the result:

nK

ERn  1 + E 1 {F }

min {1, Mtk, kt - k }

t=1 k=1

nK

 1 + 2E 1 {F }

min 1, Mtk G-tk1 

t=1 k=1

nK

= 1 + 2E 1 {F }

min 1, t-k1tk Mtk G-tk1

t=1 k=1

nK

 1 + 8 E 1 {F }

min

1, tk

M2
tk G-tk1

t=1 k=1

 1 + 8KD log(1 + 4n2) .



6 Experiments

We present two experiments to demonstrate the behaviour of Algorithm 2. All code and data is
available in the supplementary material. Error bars indicate 95% confidence intervals, but sometimes
they are too small to see (the algorithm is quite conservative, so the variance is very low). We used B = 10 for all experiments. The first experiment demonstrates the improvements obtained by
using a weighted estimator over an unweighted one, and also serves to give some idea of the rate of learning. For this experiment we used D = K = 2 and n = 106 and

=

1

2

=

8/10 4/10

2/10 2

=

M =

1 1/2

0 1/2

K
and Mk, k = 2 ,
k=1

where the kth column is the parameter/allocation for the kth task. We ran two versions of the algorithm. The first, exactly as given in Algorithm 2 and the second identical except that the weights were fixed to tk = 4 for all t and k (this value is chosen because it corresponds to the minimum inverse variance for a Bernoulli variable). The data was produced by taking the average regret over 8 runs. The results are given in Fig. 1. In Fig. 2 we plot tk. The results show that tk is increasing linearly with t. This is congruent with what we might expect because in this regime the estimation error should drop with O(1/t) and the estimated variance is proportional to the estimation error.
Note that the estimation error for the algorithm with tk = 4 will be O( 1/t).

For the second experiment we show the algorithm adapting to the environment. We fix n = 5 x 105 and D = K = 2. For   (0, 1) we define

 =

1/2 1/2

/2 /2

=

M =

1 1

0 0

K
and Mk, k = 1 .
k=1

The unusual profile of the regret as  varies can be attributed to two factors. First, if  is small then the algorithm quickly identifies that resources should be allocated first to the first task. However, in the early stages of learning the algorithm is conservative in allocating to the first task to avoid overallocation. Since the remaining resources are given to the second task, the regret is larger for small

7

 because the gain from allocating to the second task is small. On the other hand, if  is close to 1, then the algorithm suffers the opposite problem. Namely, it cannot identify which task the resources should be assigned to. Of course, if  = 1, then the algorithm must simply learn that all resources can be allocated safely and so the regret is smallest here. An important point is that the algorithm never allocates all its resources at the start of the process because this risks over-allocation, so even in "easy" problems the regret will not vanish.

Regret 
Regret

Figure 1: Weighted vs unweighted estimation

Figure 2: Weights

80,000 60,000 40,000 20,000
0 0

40

Weighted Estimator Unweighted Estimator
1,000,000 t

20
0 0

t1 t2
1,000,000 t

Figure 3: "Gap" dependence
30,000
20,000
10,000
0 0.0 0.5 1.0 

7 Conclusions and Summary
We introduced the stochastic multi-resource allocation problem and developed a new algorithm that enjoys near-optimal worst-case regret. The main drawback of the new algorithm is that its computation time is exponential in the dimension parameters, which makes practical implementations challenging unless both K and D are relatively small. Despite this challenge we were able to implement that algorithm using a relatively brutish approach to solving the optimisation problem, and this was sufficient to present experimental results on synthetic data showing that the algorithm is behaving as the theory predicts, and that the use of the weighted least-squares estimation is leading to a real improvement.
Despite the computational issues, we think this is a reasonable first step towards a more practical algorithm as well as a solid theoretical understanding of the structure of the problem. As a consolation (and on their own merits) we include some other results:
* An efficient (both in terms of regret and computation) algorithm for the case where overallocation is impossible.
* An algorithm for linear bandits on the hypercube that enjoys optimal regret bounds and adapts to sparsity.
* Theoretical analysis of weighted least-squares estimators, which may have other applications (e.g., linear bandits with heteroscedastic noise).
There are many directions for future research. The most natural is to improve the practicality of the algorithm. We envisage such an algorithm might be obtained by following the program below:
* Generalise the Thompson sampling analysis for linear bandits by Agrawal and Goyal [2012]. This is a highly non-trivial step, since it is no longer straight-forward to show that such an algorithm is optimistic with high probability. Instead it will be necessary to make do with some kind of local optimism for each task.
* The method of estimation depends heavily on the algorithm over-allocating its resources only with extremely low probability, but this significantly slows learning in the initial phases when the confidence sets are large and the algorithm is acting conservatively. Ideally we would use a method of estimation that depended on the real structure of the problem, but existing techniques that might lead to theoretical guarantees (e.g., empirical process theory) do not seem promising if small constants are expected.
It is not hard to think up extensions or modifications to the setting. For example, it would be interesting to look at an adversarial setting (even defining it is not so easy), or move towards a non-parametric model for the likelihood of success given an allocation.
8

References
Yasin Abbasi-Yadkori, Csaba Szepesvari, and David Tax. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, pages 2312-2320, 2011.
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and application to sparse stochastic bandits. In AISTATS, volume 22, pages 1-9, 2012.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. arXiv preprint arXiv:1209.3352, 2012.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. The Journal of Machine Learning Research, 3:397-422, 2003.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47:235-256, 2002.
Kristin P Bennett and Olvi L Mangasarian. Bilinear separation of two sets inn-space. Computational Optimization and Applications, 2(3):207-227, 1993.
Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multiarmed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorporated, 2012. ISBN 9781601986269.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feedback. In COLT, pages 355-366, 2008.
Akshay Krishnamurthy, Alekh Agarwal, and Miroslav Dudik. Efficient contextual semi-bandit learning. arXiv preprint arXiv:1502.05890, 2015.
Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. arXiv preprint arXiv:1410.0949, 2014.
Tor Lattimore, Koby Crammer, and Csaba Szepesvari. Optimal resource allocation with semi-bandit feedback. In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence (UAI), 2014.
Marek Petrik and Shlomo Zilberstein. Robust approximate bilinear programming for value function approximation. The Journal of Machine Learning Research, 12:3027-3063, 2011.
Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395-411, 2010.
Thomas Sowell. Is Reality Optional?: And Other Essays. Hoover Institution Press, 1993.
9

