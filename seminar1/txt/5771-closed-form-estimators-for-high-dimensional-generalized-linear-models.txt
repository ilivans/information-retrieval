Closed-form Estimators for High-dimensional Generalized Linear Models

Eunho Yang IBM T.J. Watson Research Center
eunhyang@us.ibm.com

Aurelie C. Lozano IBM T.J. Watson Research Center
aclozano@us.ibm.com

Pradeep Ravikumar University of Texas at Austin pradeepr@cs.utexas.edu

Abstract
We propose a class of closed-form estimators for GLMs under high-dimensional sampling regimes. Our class of estimators is based on deriving closed-form variants of the vanilla unregularized MLE but which are (a) well-defined even under high-dimensional settings, and (b) available in closed-form. We then perform thresholding operations on this MLE variant to obtain our class of estimators. We derive a unified statistical analysis of our class of estimators, and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection, that surprisingly match those of the more complex regularized GLM MLEs, even while our closed-form estimators are computationally much simpler. We derive instantiations of our class of closed-form estimators, as well as corollaries of our general theorem, for the special cases of logistic, exponential and Poisson regression models. We corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations.
1 Introduction
We consider the estimation of generalized linear models (GLMs) [1], under high-dimensional settings where the number of variables p may greatly exceed the number of observations n. GLMs are a very general class of statistical models for the conditional distribution of a response variable given a covariate vector, where the form of the conditional distribution is specified by any exponential family distribution. Popular instances of GLMs include logistic regression, which is widely used for binary classification, as well as Poisson regression, which together with logistic regression, is widely used in key tasks in genomics, such as classifying the status of patients based on genotype data [2] and identifying genes that are predictive of survival [3], among others. Recently, GLMs have also been used as a key tool in the construction of graphical models [4]. Overall, GLMs have proven very useful in many modern applications involving prediction with high-dimensional data. Accordingly, an important problem is the estimation of such GLMs under high-dimensional sampling regimes. Under such sampling regimes, it is now well-known that consistent estimators cannot be obtained unless low-dimensional structural constraints are imposed upon the underlying regression model parameter vector. Popular structural constraints include that of sparsity, which encourages parameter vectors supported with very few non-zero entries, group-sparse constraints, and low-rank structure with matrix-structured parameters, among others. Several lines of work have focused on consistent estimators for such structurally constrained high-dimensional GLMs. A popular instance, for the case of sparsity-structured GLMs, is the 1 regularized maximum likelihood estimator (MLE), which has been shown to have strong theoretical guarantees, ranging from risk
1

consistency [5], consistency in the 1 and 2-norm [6, 7, 8], and model selection consistency [9]. Another popular instance is the 1/q (for q 2) regularized MLE for group-sparse-structured logistic regression, for which prediction consistency has been established [10]. All of these estimators solve general non-linear convex programs involving non-smooth components due to regularization. While a strong line of research has developed computationally efficient optimization methods for solving these programs, these methods are iterative and their computational complexity scales polynomially with the number of variables and samples [10, 11, 12, 13], making them expensive for very large-scale problems. A key reason for the popularity of these iterative methods is that while the number of iterations are some function of the required accuracy, each iteration itself consists of a small finite number of steps, and can thus scale to very large problems. But what if we could construct estimators that overall require only a very small finite number of steps, akin to a single iteration of popular iterative optimization methods? The computational gains of such an approach would require that the steps themselves be suitably constrained, and moreover that the steps could be suitably profiled and optimized (e.g. efficient linear algebra routines implemented in BLAS libraries), a systematic study of which we defer to future work. We are motivated on the other hand by the simplicity of such a potential class of "closed-form" estimators. In this paper, we thus address the following question: "Is it possible to obtain closed-form estimators for GLMs under high-dimensional settings, that nonetheless have the sharp convergence rates of the regularized convex programs and other estimators noted above?" This question was first considered for linear regression models [14], and was answered in the affirmative. Our goal is to see whether a positive response can be provided for the more complex statistical model class of GLMs as well. In this paper we focus specifically on the class of sparse-structured GLMs, though our framework should extend to more general structures as well. As an inkling of why closed-form estimators for high-dimensional GLMs is much trickier than that for high-dimensional linear models is that under small-sample settings, linear regression models do have a statistically efficient closed-form estimator -- the ordinary least-squares (OLS) estimator, which also serves as the MLE under Gaussian noise. For GLMs on the other hand, even under small-sample settings, we do not yet have statistically efficient closed-form estimators. A classical algorithm to solve for the MLE of logistic regression models for instance is the iteratively reweighted least squares (IRLS) algorithm, which as its name suggests, is iterative and not available in closedform. Indeed, as we show in the sequel, developing our class of estimators for GLMs requires far more advanced mathematical machinery (moment polytopes, and projections onto an interior subset of these polytopes for instance) than the linear regression case. Our starting point to devise a closed-form estimator for GLMs is to nonetheless revisit this classical unregularized MLE estimator for GLMs from a statistical viewpoint, and investigate the reasons why the estimator fails or is even ill-defined in the high-dimensional setting. These insights enable us to propose variants of the MLE that are not only well-defined but can also be easily computed in analytic-form. We provide a unified statistical analysis for our class of closed-form GLM estimators, and instantiate our theoretical results for the specific cases of logistic, exponential, and Poisson regressions. Surprisingly, our results indicate that our estimators have comparable statistical guarantees to the regularized MLEs, in terms of both variable selection and parameter estimation error, which we also corroborate via extensive simulations (which surprisingly even show a slight statistical performance edge for our closed-form estimators). Moreover, our closed-form estimators are much simpler and competitive computationally, as is corroborated by our extensive simulations. With respect to the conditions we impose on the GLM models, we require that the population covariance matrix of our covariates be weakly sparse, which is a different condition than those typically imposed for regularized MLE estimators; we discuss this further in Section 3.2. Overall, we hope our simple class of statistically as well as computationally efficient closed-form estimators for GLMs would open up the use of GLMs in large-scale machine learning applications even to lay users on the one hand, and on the other hand, encourage the development of new classes of "simple" estimators with strong statistical guarantees extending the initial proposals in this paper.
2

2 Setup

We consider the class of generalized linear models (GLMs), where a response variable y 2 Y, conditioned on a covariate vector x 2 Rp, follows an exponential family distribution:

P(y|x;

)

=

exp



h(y)

+

yh, xi c(

)

A h, xi

(1)

where 2 R > 0 is fixed and known scale parameter,  2 Rp is the GLM parameter of interest,

and A(h, xi) is the log-partition function or the log-normalization constant of the distribution. Our

goal is to estimate the GLM parameter  given exponential families, the conditional moment of

nthie.ir.des. psoanmspelgesiven(xt(hie) ,cyo(vi)a)riani=te1s.

By can

properties be written

of as

(h, xi)  E(y|x; ) = A0(h, xi).

Examples. Popular instances of (1) include the standard linear regression model, the logistic re-

gression model, and the Poisson regression model, among others. In the case of the linear re-

gexrepssniony2m/2o+dyehl, w,x2ei

have a
h ,xi2

resoponse variable y 2 R, with the conditional distribution P(y|x, ): /2 , where the log-partition function (or log-normalization constant)

A(a) of (1) in this specific case is given by A(a) = a2/2. Another popular GLM instance is

the exp

loyghistic, xriegrelsosgionexmpo(dehlP,(xyi|x) ,+e)x,pf(ohra,

xcia)tegorwichaelreoutthpeutlovga-rpiaabrtlietioyn

2Y  function

{ 1, 1}, A(a) =

log exp( a) + exp(a) . The exponential regression model P(y|x, ) in turn is given by:

exp yh, xi + log h, xi . Here, the domain of response variable Y = R+ is the set of non-negative real numbers (it is typically used to model time intervals between events for instance), and the log-partition function A(a) = log( a). Our final example is the Poisson regres-

sion model, P(y|x, ): exp log(y!) + yh, xi exp h, xi where the response variable is count-valued with domain Y  {0, 1, 2, ...}, and with log-partition function A(a) = exp(a).

Any exponential family distribution can be used to derive a canonical GLM regression model (1) of a response y conditioned on covariates x, by setting the canonical parameter of the exponential family distribution to h, xi. For the parameterization to be valid, the conditional density should be normalizable, so that A h, xi < +1.

High-dimensional Estimation Suppose that we are given n covariate vectors, x(i) 2 Rp, drawn

i.i.d. from some distribution, and corresponding response variables, y(i) 2 Y, drawn from the

distribution P(y|x(i), ) in (1). A key goal in statistical estimation is to estimate the parameters

 in

a2hRigph,-gdiivmeennjsuisotntahlerseagmimplee,swh(exr(ei)t,hye(id))imnie=n1s.ioSnucohf

estimation becomes particularly covariate vector p is potentially

challenging even larger

than the number of samples n. In such high-dimensional regimes, it is well understood that structural

constraints on  are necessary in order to find consistent estimators. In this paper, we focus on the

structural constraint of element-wise sparsity, so that the number of non-zero elements in  is less

than or equal to some value k much smaller than p: kk0  k.

Estimators: Regularized Convex Programs The 1 norm is known to encourage the esti-

mation of such sparse-structured parameters . Accordingly, a popular class of M -estimators

ffwtiooorrrintats(e1lpn)as.risamesG-:pslitivrmceuintciynt,uinmrwesidezaemGcopLlllMeastepta(hrx,ea(mn1in),ePytoe(bniri=s)s)e1irsyvni(=atith)1ixeo(fnirs)o1min+rePgvn1u(eyclPat|oxrinir,z=ea1dnA)d,mhmtahxae,itxmri(1xui)mirfeogrl+omuglsa-lrniwikkzheeldekirh1eMoo.wLdeEFesoosvtrcieamrnnlaoottaboader-

the notation y 2 Rn to denote the vector of n responses so that i-th element of y, yi, is

y(i), and X 2 Rnp to denote the design matrix whose i-th row is [x(i)]>. With this no-

tation we can rewrite optimization problem characterizing the 1-regularized MLE simply as

minimize 

1 n

>

X

>y

+

1 n

1>

A(X

)

+

nkk1

where we overload the notation A(*) for an

input vector  2 Rn to denote A()  A(1), A(2), . . . , A(n) >, and 1  (1, . . . , 1)> 2 Rn.

3

3 Closed-form Estimators for High-dimensional GLMs

The goal of this paper is to derive a general class of closed-form estimators for high-dimensional GLMs, in contrast to solving huge, non-differentiable 1 regularized optimization problems. Before introducing our class of such closed-form estimators, we first introduce some notation.

For any u 2 Rp, we use [S (u)]i = sign(ui) max(|ui| , 0) to denote the element-wise softthresholding operator, with thresholding parameter . For any given matrix M 2 Rpp, we denote by T(M ) : Rpp 7! Rpp a family of matrix thresholding operators that are defined point-wise, so that they can be written as [T(M )]ij := (Mij), for any scalar thresholding operator (*) that satisfies the following conditions: for any input a 2 R, (a) |(a)|  |a|, (b) |(a)| = 0 for |a|  , and (c) |(a) a|  . The standard soft-thresholding and hard-thresholding operators are both pointwise operators that satisfy these properties. See [15] for further discussion of such pointwise matrix thresholding operators.

For any  2 Rn, we let rA() denote the element-wise gradients: rA()  A0(1), A0(2), . . . , A0(n) >. We assume that the exponential family underlying the GLM is minimal, so that this map is invertible, and so that for any  2 Rn in the range of rA(*), we can denote [rA] 1() as an element-wise inverse map of rA(*): (A0) 1(1), (A0) 1(2), . . . , (A0) 1(n) >.

Consider the response moment polytope M := { :  = Ep[y], for some distribution p over y 2 Y}, and let Mo denote the interior of M. Our closed-form estimator will use a carefully selected subset

M  Mo.

(2)

Denote the projection of a response variable y 2 Y onto this subset as M (y) = arg min2M |y |, where the subset M is selected so that the projection step is always well-defined, and the minimum exists. Given a vector y 2 Yn, we denote the vector of element-wise projections of entries in y as M (y) so that:

[M (y)]i := M (yi).

(3)

As the conditions underlying our theorem will make clear, we will need the operator [rA] 1(*) defined above to be both well-defined and Lipschitz in the subset M of the interior of the response moment polytope. In later sections, we will show how to carefully construct such a subset M for different GLM models.

We now have the machinery to describe our class of closed-form estimators:

bElem = S n

h T



X

>X n

i

1 X>[rA] 1 n

M (y)

! ,

(4)

where the various mathematical terms were defined above. It can be immediately seen that the estimator is available in closed-form. In a later section, we will see instantiations of this class of estimators for various specific GLM models, and where we will see that these estimators take very simple forms. Before doing so, we first describe some insights that led to our particular construction of the high-dimensional GLM estimator above.

3.1 Insights Behind Construction of Our Closed-Form Estimator

We first revisit the classical unregularized MLE for GLMs:

b 2

aurngiqmueinm inimn1um>

Xin>gyen+ern1al1, >eAsp(eXcia)lly.

Note under

that this optimization problem does not have a high-dimensional sample settings where p > n.

Nonetheless, it is instructive to study why this unregularized MLE is either ill-suited or even

ill-defined under high-dimensional settings. The stationary condition of unregularized MLE

optimization problem can be written as:

X>y = X>rA(Xb).

(5)

There are two main caveats to solving for a unique b satisfying this stationary condition, which we clarify below.

4

(Mapping to mean parameters) In a high dimensional sampling regime where p n, (5) can

be seen to reduce to y = rA(Xb) (so long as XT has rank n). This then suggests solving for

Xb = [rA] 1(y), where we recall the definition of the operator rA(*) in terms of element-wise operations involving A0(*). The caveat however is that A0(*) is only onto the interior Mo of the response moment polytope [16], so that [A0(*)] 1 is well-defined only when given  2 Mo. When entries of the sample response vector y however lie outside of Mo, as will typically be the case and which we will illustrate for multiple instances of GLM models in later sections, the inverse mapping would not be well-defined. We thus first project the sample response vector y onto M  Mo to obtain M (y) as defined in (3). Armed with this approximation, we then consider the more amenable M (y)  rA(Xb), instead of the original stationary condition in (5).

(Sample covariance) We thus now have the approximate characterization of the MLE as Xb 

[rA] 1 M (y) . This then suggests solving for an approximate MLE b via least squares as b = [X>X] 1X>[rA] 1 M (y) . The high-dimensional regime with p > n poses a caveat here, since the sample covariance matrix (X>X)/n would then be rank-deficient, and hence not

invertible. Our in the previous

approach is to then subsection instead,

use a which

thresholded sample can be shown to be

cinovvearrtiiabnleceanmdatcroixnsTistenXt>ntoX

defined the popu-

lation covariance matrix  with high probability [15, 17]. In particular, recent work [15] has shown

that thresholded sample covariance T

X>X
n

qis consistent with respect to the spectral norm with

convergence rate

T

X>X n

 op  O c0

log p n

, under some mild conditions detailed in our

main theorem. Plugging in this thresholded sample covariance matrix, to get an approximate least

squares solution for the GLM parameters , and then performing soft-thresholding precisely yields

our closed-form estimator in (4).

Our class of closed-form estimators in (4) can thus be viewed as surgical approximations to the MLE so that it is well-defined in high-dimensional settings, as well as being available in closed-form. But would such an approximation actually yield rigorous consistency guarantees? Surprisingly, as we show in the next section, not only is our class of estimators consistent, but in our corollaries we show that the statistical guarantees are comparable to those of the state of the art iterative ways like regularized MLEs.

We note that our class of closed-form estimators in (4) can also be written in an equivalent form that is more amenable to analysis:

minimize


kk1

(6)

s. t



h T



X

>X n

i

1 X>[rA] 1 n

M (y)

 n.
1

The equivalence between (4) and (6) easily follows from the fact that the optimization problem (6)

is decomposable into independent element-wise sub-problems, and each sub-problem corresponds

to soft-thresholding. It can be seen that this form is also amenable to extending the framework in

this paper to structures beyond sparsity, by substituting in alternative regularizers. Due to space

constraints, the computational complexity is discussed in detail in the Appendix.

3.2 Statistical Guarantees
In this subsection, we provide an unified statistical analysis for the class of estimators (4) under the following standard conditions, namely sparse  and sub-Gaussian design X:
(C1) The parameter  in (1) is exactly sparse with k non-zero elements indexed by the support set S, so that Sc = 0. (C2) Each row of the design matrix X 2 Rnp is i.i.d. sampled from a zero-mean distribution with covariance matrix  such that for any v 2 Rp, the variable hv, Xii is sub-Gaussian with parameter at most ukvk2 for every row of X, Xi. Our next assumption is on the covariance matrix of the covariate random vector: (C3) The covariance matrix  of X satisfies that for all w 2 Rp, kwk1  kwk1 with fixed constant  > 0. Moreover,  is approximately sparse, along the lines of [17]: for some

5

pmoasxitiivPe pjc=on1s|tanijt|qD, ci0i.

 If

D for q = 0,

all diagonal entries, and moreover, for some then this condition will be equivalent with 

0  q < 1 and being sparse.

c0,

We also introduce some notations used in hav:e=th2atuwkithkh2ipghlopgrnob. aWbielitthye, n|hlet,Mx(i0)bi|e

the2foullkowikn2gptlhoegonremfo.r

Under the condition all samples, i = 1, . .

the subset of M such that

(C2), . , n.

we Let

no

M0 :=  :  = A0() , where  2 [  ,  ] .

(7)

We also define u,A and ,A on the upper bounds of A00(*) and (A 1)0(*), respectively:

max |A00()|
2[  , ]



u,A

,

max |(A
a2M0 [M

1)0(a)|  ,A.

(8)

Armed with these conditions and notations, we derive our main theorem:

Theorem 1. Consider any generalized linear model in (1) where all the conditions (C1), (C2) and

(C3) =

holqd. C1

Now,
log p0 n

suppose that we solve the estimation problem (4) setting the thresholding parameter

where

C1

:=

p 16(maxj jj) 10

for

any constant



> 2, q

and p0

:=

max{n, p}.

Fp2ruorjtehcuetriomn,oA(rp3e,),2saunupd,pAoiss+edeCafil1nskoedthakas1t:wEaen:=dsewtmhtahexerie=cEo1n,.d.s.et,rnpaeinnydt(sib)oonuntdheManp(pyars)oCxii2m4atuiloognn,pAe0 rq+rolrEogninwpd0h.uecreed

C2 by

:= the

(A) Then, as long as n >

2c1 c0 

2 1q

log p0

where

c1

is

a

constant

related

only

on



and

maxi

ii,

any optimal solution b of (4) is guaranteed to be consistent:

b

q



 1  2 C2

log p0 n

+E

,

b



2



p 4 k C2

q

log p0 n

 +E ,

b

q



 1  8k C2

log p0 n

+E

.

(B) Moreover, the support set of the estimate b correctly excludes all true zero values of . Moreover, when mins2S |s| 3 n, it correctly includes all non-zero true supports of , with probability at least 1 cp0 c0 for some universal constants c, c0 > 0 depending on  and u.

Remark 1. While our class of closed-form estimators and analyses consider sparse-structured parameters, these can be seamlessly extended to more general structures (such as group sparsity and low rank), using appropriate thresholding functions. Remark 2. The condition (C3) required in Theorem 1 is different from (and possibly stronger) than the restricted strong convexity [8] required for 2 error bound of 1 regularized MLE. A key facet of our analysis with our Condition (C3) however is that it provides much simpler and clearer identifying constants in our non-asymptotic error bounds. Deriving constant factors in the analysis of the 1-regularized MLE on the other hand, with its restricted strong convexity condition, involves many probabilistic statements, and is non-trivial, as shown in [8].

Another key facet of our analysis in Theorem 1 is that it also provides an 1 error bound, and guarantees the sparsistency of our closed-form estimator. For 1 regularized MLEs, this requires a separate sparsistency analysis. In the case of the simplest standard linear regression models, [18]

showed that the incoherence condition of |||S ||| * |||1 is the maximum of absolute row sum.

cASsdSiS1sc|||u1sse<d

1 is required for sparsistency, where in [18], instances of such incoherent

covariance matrices  include the identity, and Toeplitz matrices: these matrices can be seen to

also satisfy our condition (C3). On the other hand, not all matrices that satisfy our condition (C3)

need satisfy the stringent incoherence condition in turn. For example, consider  where SS = 0.95I3 + 0.05133 for a matrix 1 of ones, SSc is all zeros but the last column is 0.4131, and ScSc = I(p 3)(p 3). Then, this positive definite  can be seen to satisfy our Condition (C3),

sainndcecoenascehqruoewnthlya,sthoenliync4onhoenre-znecreosc.onHdoiwtioenverre,q|||uirSecdSfoSr S1th|||e1Liassesoquwaliltlon1o.t0b9e09saatnisdfileadr.geWr ethdaenfe1r,

relaxing our condition (C3) further as well as a deeper investigation of all the above conditions to

future work.

6

Remark 3. only kk2

The constant C2 is bounded, may

sicnaltehewsitthatepmke.ntOdneptheendosthoenr

kk1, which in the worst case where hand, our theorem does not require an

explicit sample complexity condition that n be larger than some function on k, while the analysis

of 1-regularized MLEs do additionally require that n c k log p for some constant c. In our experiments, we verify that our closed-form estimators outperform the 1-regularized MLEs even

when k is fairly large (for instant, when (n, p, k) = (5000, 104, 1000)).

In order to apply Theorem 1 to a specific instance of GLMs, we need to specify the quantities in (8),

as well as carefully construct a subset M of the interior of the response moment polytope. In case

of the simplest linear models described in Section 2, we have the identity mapping  = A0() = .

The inequalities in (8) can thus be seen to be satisfied with ,A = u,A = 1 . Moreover, we can set

M := Mo case. In the

=follRowsointghastectiMon(sy, )w=e

y, and trivially recover the previous results in [14] as will derive the consequences of our framework for the

a special complex

instances of logistic and Poisson regression models, which are also important members in GLMs.

4 Key Corollaries

In order to derive corollaries of our main Theorem 1, we need to specify the response polytope subsets M, M0 in (2) and (7) respectively, as well as bound the two quantities ,A and u,A in (8).

Logistic regression models described in

models. Section 2

cTahnebeexspeoennentotiablefAam(il)y =loglo-pgarteixtipo(n

fun) c+tioenxpo(fl)og.isCtiocnrseegqrueesnsitolyn,

its double derivative A00()

=

4 exp(2) (exp(2)+1)2



1 for any , so that (8) holds with u,A

=

1.

The response moment polytope for the binary response variable y 2 Y  { 1, 1} is the inter-

val M = [ 1, 1], so that its interior is given by Mo = ( 1, 1). For the subset of the interior,

we define M = [ 1 + , 1 ], for some 0 <  < 1. At the same time, the forward mapping

is given by A0() = exp(2)

1)/(exp(2) + 1), and hence M0 becomes [

a1 a+1

,

a1 a+1

]

where

a

:=

n 4pukk2 log n

.

The

inverse

mapping

of

logistic

models

is

given

by

(A0)

1()

=

1 2

log

1+ 1

, and

gtthhivae,Atecnmo:=rMaoxlmlia=aran1yx,d.b.n.Me,nl12o(0wy+, (.iit)12cna4npublkoeMgsnk(e2yen),1tih)/a=to(Ai,n0a)(n8d)1.(mNo)oretiesovtLheiarp,tswchMiitthz(ytfhioi)rs=Mseyttii[(n1gMof0tw)h,eiwthshucibcoshneswttaMenwt,liwellsesushtehaavinne

Poisson regression models. Another important instance of GLMs is the Poisson regression model,

that is becoming increasingly more relevant in modern big-data settings with varied multivariate

count upper  in [
u,A

=dbaotnua,n2. dFueko]d,r:AtkAh200e/0(0p(Pul)oo)gisn=so. enxTexprhepeg(2rruee)ssu.spkiooDnnesknem2opmotidnoleogmlgecnnast e:p==, otlhyn2et2odupuokekubfolkker22/tdphpeelrlooicvggoanntu,i,nvsetwo-voetafhltauhAtee(d(n*8)r)heiaisssvpnesooanttthsiuasefitnveiffaodorrirwamabintllhyye

y 2 Y  {0, 1, . . .} is given by M = [0, 1), so that its interior is given by Mo = (0, 1). For the

subset of the interior, we define M = [, 1) for some  s.t. 0 <  < 1. The forward mapping in this

case is simply given by A0() = exp(), and M0 in (7) becomes [a

1,

a]

where

a

is

n 2pukk2 log n

.

The

inverse mapping for the Poisson regression model then is given by (A0) 1() = log(), which can

be

seen

to

be

Lipschitz

for

M

with

constant

,A

=

max{n 2pukk2 log n

, 1/}

in

(8).

With

this

setting

of M, it can be seen that the projection operator is given by M (yi) = I(yi = 0) + I(yi 6= 0)yi.

Now, we are ready to recover the error bounds, as a corollary of Theorem 1, for logistic regression and Poisson models when condition (C2) holds:

Corollary 1. Consider any logistic regression model or a Poisson regression model where all con-

ditions in Theorem 1 hold. the thresholding parameter
q

Supposeqthat we

 = C1

log p0 n

,

solve our closed-form estimation problem (4), setting

and the constraint bound

n

=

2 

+n(1/c2pclo0/gppl0og n)

C1kk1

log p0 n

where c and c0 are some constants depending only on u, kk2 and . Then the

7

Table 1: Comparisons on simulated datasets when parameters are tuned to minimize 2 error on independent validation sets.

(n, p, k)
(n = 2000, p = 5000, k = 10)
(n = 4000, p = 5000, k = 10)
(n = 5000, p = 104 , k = 100)

METHOD 1 MLE1 1 MLE2 1 MLE3
ELEM 1 MLE1 1 MLE2 1 MLE3
ELEM 1 MLE1 1 MLE2 1 MLE3
ELEM

TP
1 1 1 0.9900 1 1 1 1 1 1 1 0.9975

FP
0.1094 0.0873 0.1000 0.0184 0.1626 0.1327 0.1112 0.0069 0.1301 0.1695 0.2001 0.3622

2 ERROR
4.5450 4.0721 3.4846 2.7375 4.2132 3.6569 2.9681 2.6213 18.9079 18.5567 18.2351 16.4148

TIME
63.9 133.1 348.3 26.5 155.5 296.8 829.3 40.2 500.1 983.8 2353.3 151.8

(n, p, k)
(n = 5000, p = 104 , k = 1000)
(n = 8000, p = 104 , k = 100)
(n = 8000, p = 104 , k = 1000)

METHOD 1 MLE1 1 MLE2 1 MLE3
ELEM 1 MLE1 1 MLE2 1 MLE3
ELEM 1 MLE1 1 MLE2 1 MLE3
ELEM

TP
0.7990 0.7935 0.7965 0.8295
1 1 1 0.9450 0.7965 0.7900 0.7865 0.7015

FP
1 1 1 1 0.1904 0.2181 0.2364 0.0359 1 1 1 0.5103

2 ERROR
65.1895 65.1165 65.1024 63.2359 18.6186 18.1806 17.6762 11.9881 65.0714 64.9650 64.8857 61.0532

TIME
520.7 1005.8 2560.1 152.1 810.6 1586.2 3568.9 221.1 809.5 1652.8 4196.6 219.4

optimal solution b of (4) is guaranteed to be consistent:

p

r

p

b 



1



4 

p

c n(1/2

log p0

c0

p / log

n)

+ C1kk1

r

log p0 n

,

b



2



8k 

p

r

c n(1/2

log p0

c0

p / log

n)

+ C1kk1

log p0 n

,

b



1



16k 

c n(1/2

log p0

c0

p / log

n)

+ C1kk1

log p0 n

,

with probability Moreover, when

at least mins2S

1 |s

c1p0

|

6 

c01nf(o1/rc2psoclo0m/gpepl0ougnni)ve+rsCal1ckonskt1aqntslocgn1p,0c01,

> 0 and p0 := max{n, b is sparsistent.

p}.

Remarkably, the rates in Corollary 1 are asymptotically comparable to those for the 1-regularized MLE (see for instance Theorem 4.2 and Corollary 4.4 in [7]). In Appendix A, we place slightly more stringent condition than (C2) and guarantee error bounds with faster convergence rates.

5 Experiments

We corroborate the performance of our elementary estimators on simulated data over varied regimes

of sample size n, number of covariates p, and sparsity size k. We consider two popular instances

of GLMs, logistic and Poisson regression models. We compare against standard 1 regularized MLE estimators with iteration bounds of 50, 100, and 500, denoted by 1 MLE1, 1 MLE2 and 1 MLE3 respectively. We construct the n  p design matrices X by sampling the rows independently

from N (0, ) where i,j = 0.5|i j|. For each simulation, the entries of the true model coefficient vector  are set to be 0 everywhere, except for a randomly chosen subset of k coefficients, which

are chosen independently and uniformly in the interval (1, 3). We report results averaged over 100

independent trials. Noting that our we simply report the results when

theoretical results were not sensitive  = 10 4 across all experiments.

to

the

setting

of



in

M

(y),

While our theorem specified an optimal setting of the regularization parameter n and , this optimal setting depended on unknown model parameters. Thpus, as is standard withphigh-dimensional regularized estimators, we set tuning parameters n = c log p/n and  = c0 log p/n by a holdoutvalidated fashion; finding a parameter that minimizes the 2 error on an independent validation set. Detailed experimental setup is described in the appendix.

Table 1 summarizes the performances of 1 MLE using 3 different stopping criteria and Elem-GLM. Besides 2 errors, the target tuning metric, we also provide the true and false positives for the support set recovery task on the new test set where the best tuning parameters are used. The computation times in second indicate the overall training computation time summing over the whole parameter tuning process. As we can see from our experiments, with respect to both statistical and computational performance our closed form estimators are quite competitive compared to the classical 1 regularized MLE estimators and in certain case outperform them. Note that 1 MLE1 stops prematurely after only 50 iterations, so that training computation time is sometimes comparable to closed-form estimator. However, its statistical performance measured by 2 is much inferior to other 1 MLEs with more iterations as well as Elem-GLM estimator. Due to the space limit, ROC curves, results for other settings of p and more experiments on real datasets are presented in the appendix.

8

References
[1] P. McCullagh and J.A. Nelder. Generalized linear models. Monographs on statistics and applied probability 37. Chapman and Hall/CRC, 1989.
[2] G. E. Hoffman, B. A. Logsdon, and J. G. Mezey. Puma: A unified framework for penalized multiple regression analysis of gwas data. Plos computational Biology, 2013.
[3] D. Witten and R. Tibshirani. Survival analysis with high-dimensional covariates. Stat Methods Med Res., 19:29-51, 2010.
[4] E. Yang, P. Ravikumar, G. I. Allen, and Z. Liu. Graphical models via generalized linear models. In Neur. Info. Proc. Sys. (NIPS), 25, 2012.
[5] S. Van de Geer. High-dimensional generalized linear models and the lasso. Annals of Statistics, 36(2): 614-645, 2008.
[6] F. Bach. Self-concordant analysis for logistic regression. Electron. J. Stat., 4:384-414, 2010. [7] S. M. Kakade, O. Shamir, K. Sridharan, and A. Tewari. Learning exponential families in high-dimensions:
Strong convexity and sparsity. In Inter. Conf. on AI and Statistics (AISTATS), 2010. [8] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high-dimensional
analysis of M-estimators with decomposable regularizers. Arxiv preprint arXiv:1010.2731v1, 2010. [9] F. Bunea. Honest variable selection in linear and logistic regression models via l1 and l1 + l2 penalization.
Electron. J. Stat., 2:1153-1194, 2008. [10] L. Meier, S. Van de Geer, and P. Buhlmann. The group lasso for logistic regression. Journal of the Royal
Statistical Society, Series B, 70:53-71, 2008. [11] Y. Kim, J. Kim, and Y. Kim. Blockwise sparse regression. Statistica Sinica, 16:375-390, 2006. [12] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordi-
nate descent. Journal of Statistical Software, 33(1):1-22, 2010. [13] K. Koh, S. J. Kim, and S. Boyd. An interior-point method for large-scale 1-regularized logistic regres-
sion. Jour. Mach. Learning Res., 3:1519-1555, 2007. [14] E. Yang, A. C. Lozano, and P. Ravikumar. Elementary estimators for high-dimensional linear regression.
In International Conference on Machine learning (ICML), 31, 2014. [15] A. J. Rothman, E. Levina, and J. Zhu. Generalized thresholding of large covariance matrices. Journal of
the American Statistical Association (Theory and Methods), 104:177-186, 2009. [16] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families and variational inference.
Foundations and Trends in Machine Learning, 1(1-2):1--305, December 2008. [17] P. J. Bickel and E. Levina. Covariance regularization by thresholding. Annals of Statistics, 36(6):2577-
2604, 2008. [18] M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using 1-constrained
quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183-2202, May 2009. [19] Daniel A. Spielman and Shang-Hua Teng. Solving sparse, symmetric, diagonally-dominant linear systems
in time 0(m1.31). In 44th Symposium on Foundations of Computer Science (FOCS 2003), 11-14 October 2003, Cambridge, MA, USA, Proceedings, pages 416-427, 2003. [20] Michael B. Cohen, Rasmus Kyng, Gary L. Miller, Jakub W. Pachocki, Richard Peng, Anup B. Rao, and Shen Chen Xu. Solving sdd linear systems in nearly mlog1/2n time. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, STOC '14, pages 343-352. ACM, 2014. [21] Daniel A. Spielman and Shang-Hua Teng. Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems. SIAM J. Matrix Analysis Applications, 35(3):835-885, 2014. [22] P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by minimizing 1-penalized log-determinant divergence. Electronic Journal of Statistics, 5:935-980, 2011. [23] E. Yang, A. C. Lozano, and P. Ravikumar. Elementary estimators for sparse covariance matrices and other structured moments. In International Conference on Machine learning (ICML), 31, 2014. [24] E. Yang, A. C. Lozano, and P. Ravikumar. Elementary estimators for graphical models. In Neur. Info. Proc. Sys. (NIPS), 27, 2014.
9

