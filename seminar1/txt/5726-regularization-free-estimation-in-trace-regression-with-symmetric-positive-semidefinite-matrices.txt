Regularization-Free Estimation in Trace Regression with Symmetric Positive Semidefinite Matrices

Martin Slawski

Ping Li

Department of Statistics & Biostatistics

Department of Computer Science

Rutgers University

Piscataway, NJ 08854, USA

{martin.slawski@rutgers.edu,

pingli@stat.rutgers.edu}

Matthias Hein Department of Computer Science Department of Mathematics Saarland University Saarbrucken, Germany hein@cs.uni-saarland.de

Abstract
Trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In this paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity.

1 Introduction

Trace regression models of the form

yi = tr(Xi) + i, i = 1, . . . , n,

(1)

where   Rm1xm2 is the parameter of interest to be estimated given measurement matrices Xi  Rm1xm2 and observations yi contaminated by errors i, i = 1, . . . , n, have attracted considerable

interest in high-dimensional statistical inference, machine learning and signal processing over the

past few years. Research in these areas has focused on a setting with few measurements n  m1*m2 and  being (approximately) of low rank r  min{m1, m2}. Such setting is relevant to problems

such as matrix completion [6, 23], compressed sensing [5, 17], quantum state tomography [11] and

phase retrieval [7]. A common thread in these works is the use of the nuclear norm of a matrix

as a convex surrogate for its rank [18] in regularized estimation amenable to modern optimization

techniques. This approach can be seen as natural generalization of 1-norm (aka lasso) regularization for the linear regression model [24] that arises as a special case of model (1) in which both  and {Xi}ni=1 are diagonal. It is inarguable that in general regularization is essential if n < m1 * m2.

The situation is less clear if  is known to satisfy additional constraints that can be incorporated in

estimation. Specifically, in the present paper we consider the case in which m1 = m2 = m and 

is known to semidefinite

bceonsyeminmtheetriscpapcoesiotifvseysmemmiedterificnrieteal(mspxd),mi.em. atricesSSm+mw. TithheSsm+et

denoting the Sm+ deserves

positive specific

interest as it includes covariance matrices and Gram matrices in kernel-based learning [20]. It is

rather common for these matrices to be of low rank (at least approximately), given the widespread

use of principal components analysis and low-rank kernel approximations [28]. In the present paper, we focus on the usefulness of the spd constraint for estimation. We argue that if  is spd and the

measurement matrices {Xi}ni=1 obey certain conditions, constrained least squares estimation

min
Sm+

1 2n

n
(yi - tr(Xi))2

i=1

(2)

may perform similarly well in prediction and parameter estimation as approaches employing nuclear

norm regularization with proper choice of the regularization parameter, including the interesting

1

regime n < m, where m = dim(Sm) = m(m + 1)/2. Note that the objective in (2) only consists of a data fitting term and is hence convenient to work with in practice since there is no free parameter. Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for linear regression [16, 21].

Related of these

work. papers

Model (1) with   Sm+ has been studied consider the setup of compressed sensing in

in several recent papers. which the {Xi}ni=1 can

A be

good deal chosen by

the user, with the goal to minimize the number of observations required to (approximately) recover

. For example, in [27], recovery of  being low-rank from noiseless observations (i = 0,

i = 1, . . . , n) by solving a feasibility problem over Sm+ is considered, which is equivalent to the

constrained least squares problem (1) in a noiseless setting.

In [3, 8], recovery from rank-one measurements is considered, i.e., for {xi}ni=1  Rm yi = xi xi + i = tr(Xi) + i, with Xi = xixi , i = 1, . . . , n.

(3)

As opposed to [3, 8], where estimation based on nuclear norm regularization is proposed, the present

work is devoted to regularization-free estimation. While rank-one measurements as in (3) are also in

the center of interest herein, our framework is not limited to this case. In [3] an application of (3) to

cisovd.aisIrnciaufnsacsceetd,m,thwaitshrifiexrtesestththieemm{aztoiiod}neni=lgu1ivnaedrneerois.nit.ludyd. yofrnwoemi-tdhiamodbeisnsetsrriivobanutatiiolonpnsrowjeitchtizoenrso{mxieazni}anin=d1

of the data covariance

points matrix

yi = (xi zi)2 = xi zizixi = xi xi + i, i = xi {zizi - }xi, i = 1, . . . , n. (4)

Specializing (3) to the case in which  = (), one obtains the quadratic model

yi = |xi |2 + i

(5)

which (with complex-valued ) is relevant to the problem of phase retrieval [14]. The approach

of [7] treats (5) as an instance of (1) and uses nuclear norm regularization to enforce rank-one

solutions. In follow-up work [4], the authors show a refined recovery result stating that imposing an

spd constraint - without regularization - suffices. A similar result has been proven independently

by [10]. However, the results in [4] and [10] only concern model (5). After posting an extended

version [22] of the present paper, a generalization of [4, 10] to general low-rank spd matrices has

been achieved in [13]. Since [4, 10, 13] consider bounded noise, whereas the analysis herein assumes

Gaussian noise, our results are not direclty comparable to those in [4, 10, 13].

Notation. Md denotes the space of real d x d matrices with inner product M, M  :=

tr(M M ). The subspace of symmetric matrices Sd has dimension d := d(d + 1)/2. M  Sd

has an eigen-decomposition M = U U  =

d j=1

j

(M

)uj

uj ,

where

1(M )

=

max(M )



2(M )  . . .  d(M ) = min(M ),  = diag(1(M ), . . . , d(M )), and U = [u1 . . . ud]. For

q  [1, ) and M  Sd, M q := (

d j=1

|j (M )|q)1/q

denotes

the

Schatten-q-norm

(q

=

1:

nu-

clear norm; q = 2 Frobenius norm M F , q = : spectral norm M  := max1jd |j(M )|).

Let S1(d) = {M  Sd : M 1 = 1} and S1+(d) = S1(d)  Sd+. The symbols , , ,  refer to the semidefinite ordering. For a set A and   R, A := {a, a  A}.

It is convenient to re-write model (1) as y = X () + , where y = (yi)ni=1,  = (i)ni=1 and

X : Mm sampling

 Rn is operator.

a linear map defined by (X (M ))i Its adjoint X  : Rn  Mm is given

= by

tr(XiM ), i the map v 

=

1, . . . , n,

n i=1

vi

Xi.

referred

to

as

Supplement. The appendix contains all proofs, additional experiments and figures.

2 Analysis
Preliminaries. Throughout this section, we consider a special instance of model (1) in which
yi = tr(Xi) + i, where   Sm+ , Xi  Sm, and i i.i.d. N (0, 2), i = 1, . . . , n. (6) The assumption that the errors {i}ni=1 are Gaussian is made for convenience as it simplifies the stochastic part of our analysis, which could be extended to sub-Gaussian errors. Note that w.l.o.g., we may assume that {Xi}ni=1  Sm. In fact, since   Sm, for any M  Mm we have that tr(M ) = tr(M sym), where M sym = (M + M )/2.

2

In the sequel, we study the statistical performance of the constrained least squares estimator

  argmin
Sm+

1 2n

y - X ()

2 2

(7)

under model (6). More specifically, under certain conditions on X , we shall derive bounds on

(a)

1 n

X () - X ()

2 2

,

and

(b)  -  1,

(8)

where (a) will be referred to as "prediction error" below. The most basic method for estimating 

is ordinary least squares (ols) estimation

ols  argmin
Sm

1 2n

y - X ()

22,

(9)

which is computationally simpler than (7). While (7) requires convex programming, (9) boils down
to solving a linear system of equations in m = m(m + 1)/2 variables. On the other hand, the prediction error of ols scales as OP(dim(range(X ))/n), where dim(range(X )) can be as large as min{n, m}, in which case the prediction error vanishes only if m/n  0 as n  . Moreover,
the estimation error ols -  1 is unbounded unless n  m. Research conducted over the past few years has thus focused on methods dealing successfully with the case n < m as long as the target  has additional structure, notably low-rankedness. Indeed, if  has rank r  m, the
intrinsic dimension of the problem becomes (roughly) mr  m. In a large body of work, nuclear norm regularization, which serves as a convex surrogate of rank regularization, is considered as a

computationally convenient alternative for which a series of adaptivity properties to underlying low-

rankedness has been established, e.g. [5, 15, 17, 18, 19]. Complementing (9) with nuclear norm

regularization yields the estimator

1



argmin
Sm

1 2n

y - X ()

2 2

+





1,

(10)

where  > 0 is a regularization parameter. In case an spd constraint is imposed (10) becomes

1+  argmin 1 Sm+ 2n

y - X ()

2 2

+



tr().

(11)

Our analysis aims at elucidating potential advantages of the spd constraint in the constrained least

squares problem (7) from a statistical point of view. It turns out that depending on properties of

X , the behaviour of  can range from a performance similar to the least squares estimator ols on

the one hand to a performance similar to the nuclear norm regularized estimator 1+ with properly

chosen/tuned  on the other hand. The latter case appears to be remarkable:  may enjoy similar

adaptivity properties as nuclear norm regularized estimators even though  is obtained from a pure

data fitting problem without explicit regularization.

2.1 Negative result

We first discuss a negative example of X for which the spd-constrained estimator  does not im-
prove (substantially) over the unconstrained estimator ols. At the same time, this example provides clues on conditions to be imposed on X to achieve substantially better performance.

Random Gaussian design. Consider the Gaussian orthogonal ensemble (GOE)
GOE(m) = {X = (xjk), {xjj }mj=1 i.i.d. N (0, 1), {xjk = xkj }1j<km i.i.d. N (0, 1/2)}. Gaussian measurements are common in compressed sensing. It is hence of interest to study measurements {Xi}ni=1 i.i.d. GOE(m) in the context of the constrained least squares problem (7). The following statement points to a serious limitation associated with such measurements.
Proposition 1. Consider Xi i.i.d. GOE(m), i = 1, . . . , n. For any  > 0, if n  (1 - )m/2, with probability at least 1 - 32 exp(-2m), there exists   Sm+ ,  = 0 such that X () = 0.

Proposition 1 implies that if the number of measurements drops below 1/2 of the ambient dimension

m, estimating  based on (7) becomes ill-posed; the estimation error  -  1 is unbounded,

irrespective of the rank of . Geometrically, the consequence of Proposition 1 is that the convex

cone CX = {z  Rn of CX (we conjecture

: z=X that this

(), event

  Sm+ } contains 0. Unless 0 has measure zero), this means

is contained in the boundary that CX = Rn, i.e. the spd

constraint becomes vacuous.

3

2.2 Slow Rate Bound on the Prediction Error

We present a positive result on the spd-constrained least squares estimator  under an additional condition on the sampling operator X . Specifically, the prediction error will be bounded as

1 n

X () - X ()

2 2

=

O(0



1 + 20),

where

0

=

1 n

X ()

,

(12)

with 0 typically being of the order O( m/n) (up to log factors). The rate in (12) can be a sig-
nificant improvement of what is achieved by ols if  1 = tr() is small. If 0 = o(  1) that rate coincides with those of the nuclear norm regularized estimators (10), (11) with regularization parameter   0, cf. Theorem 1 in [19]. For nuclear norm regularized estimators, the rate O(0  1) is achieved for any choice of X and is slow in the sense that the squared prediction error only decays at the rate n-1/2 instead of n-1.

Condition on X . In order to arrive at a suitable condition to be imposed on X so that (12) can be achieved, it makes sense to re-consider the negative example of Proposition 1, which states that as long as n is bounded away from m/2 from above, there is a non-trivial   Sm+ such that X () = 0. Equivalently, dist(PX , 0) = minS1+(m) X () 2 = 0, where

PX := {z  Rn : z = X (),   S1+(m)}, and S1+(m) := {  Sm+ : tr() = 1}.

In this situation, it is impossible to derive a non-trivial bound on the prediction error as dist(PX , 0) =

0 may imply CX = Rn so that

X () - X ()

2 2

=

 22. To rule this out, the condition

dist(PX , 0) > 0 is natural. More strongly, one may ask for the following:

There

exists

a

constant



>

0

such

that

02(X )

:=

min
S1+ (m)

1 n

X ()

2 2



2.

(13)

An analogous condition is sufficient for a slow rate bound in the vector case, cf. [21]. However, the condition for the slow rate bound in Theorem 1 below is somewhat stronger than (13).

Condition 1. There exist constants R > 1,  > 0 s.t.  2(X , R)  2, where for R  R

 2(X , R) = dist2(RPX , PX )/n = min
ARS1+ (m)

1 n

X (A) - X (B)

2 2

.

B S1+ (m)

The following condition is sufficient for Condition 1 and in some cases much easier to check. Proposition 2. Suppose there exists a  Rn, a 2  1, and constants 0 < min  max s.t.
min(n-1/2X (a))  min, and max(n-1/2X (a))  max. Then for any  > 1, X satisfies Condition 1 with R = (max/min) and 2 = ( - 1)22max.

The condition of Proposition 2 the unit ball under X , which,

caafnterbescpahlirnagsebdyas1/havnin, ghaaspiotssitsimvealdleesfitneitiegemnvaatrliuxeinbotuhnediemdaagwe aoyf

from zero and a bounded condition number. As a simple example, suppose that X1 = nI. In-

voking Proposition 2 with a = (1, 0, . . . , 0) and  = 2, we find that Condition 1 is satisfied with

R = 2 and 2 = 1. A more interesting example is random design where the {Xi}ni=1 are (sam-

ple) covariance matrices, where the underlying random vectors satisfy appropriate tail or moment

conditions.

Corollary 1. Let m be a probability distribution on Rm with second moment matrix  := Ezm [zz] satisfying min() > 0. Consider the random matrix ensemble

M(m, q) =

1 q

q k=1

zk zk ,

{zk}qk=1 i.i.d. m

.

(14)

Suppose

that

{Xi}ni=1

i.i.d.

M(m, q)

and

let

n

:=

1 n

n i=1

Xi

and

0

<

n

<

min().

Under

the

event {  - n   n}, X satisfies Condition 1 with

R

=

2(max() + n) min() - n

and 2 = (max() + n)2.

4

It is instructive to spell out Corollary 1 with m as the standard Gaussian distribution on Rm. The
matrix n equals the sample covariance matrix computed from N = n * q samples. It is well-known [9] that for m, N large, max(n) and min(n) concentrate sharply around (1+n)2 and (1-n)2, respectively, where n = m/N . Hence, for any  > 0, there exists C > 1 so that if N  Cm,
it holds that R  2 + . Similar though weaker concentration results for  - n  exist for the broader class of distributions m with finite fourth moments [26]. Specialized to q = 1, Corollary 1 yields a statement about X made up from random rank-one measurements Xi = zizi, i = 1, . . . , n, cf. (3). The preceding discussion indicates that Condition 1 tends to be satisfied in this case.

Theorem 1. Suppose that model (6) holds with X satisfying Condition 1 with constants R and 2. We then have

1 n

X () - X ()

2 2



max

2(1 + R)0  1, 20  1 + 8

0

R 

2

where, for any   0, with probability at least 1 - (2m)-

0  

(1

+

)2

log(2m)

V2
n
n

,

where Vn2 =

1 n

n i=1

Xi2

.

oRr2ed(meXra,OrRk():0cUanndbeer1et+vhaelus20ac)taealdisnbagynsnsRooulvnic=negdOaat(l1eth)aesatbnsedqguina2rneisn=gproofb(t1lhe)im,s tshweecittbhioonsu.pnddFocorofgnTisvthreeanoinrXetsm,. thH1eeinqscuoeafnitttihtiyes feasible to check in practice whether Condition 1 holds. For later reference, we evaluate the term Vn2 for M(m, q) with m as standard Gaussian distribution. As shown in the supplement, with high probability, Vn2 = O(m log n) holds as long as m = O(nq).

2.3 Bound on the Estimation Error

In the forth,

previous subsection, we did not make any we suppose that  is of low rank 1  r

assumptions about  apart from  m and study the performance

  of the

Sm+ . Henceconstrained

least squares estimator (7) for prediction and estimation in such setting.

Preliminaries. Let  = U U  be the eigenvalue decomposition of , where

U=

U U m x r m x (m - r)

r 0rx(m-r) 0(m-r)xr 0(m-r)x(m-r)

where r is diagonal with positive diagonal entries. Consider the linear subspace T = {M  Sm : M = UAU, A  Sm-r}.
From UU = 0, it follows that  is contained in the orthogonal complement T = {M  Sm : M = U B + BU , B  Rrxm},

of dimension mr - r(r - 1)/2  m if r  m. The image of T under X is denoted by T .

Conditions on X . We introduce the key quantities the bound in this subsection depends on. Separability constant.

 2(T)

=

1 n

dist2

(T

, PX ) ,

PX := {z  Rn : z = X (),   T  S1+(m)}

=

T,

min
S1+ (m)T

1 n

X () - X ()

2 2

Restricted eigenvalue.

2(T) = min
0=T

X

() 

22/n
2 1

.

As indicated by the following statement concerning the noiseless case, for bounding  -  , it is inevitable to have lower bounds on the above two quantities.

5

Proposition 3. Consider the trace regression model (1) with i = 0, i = 1, . . . , n. Then

argmin
Sm+

1 2n

X () - X ()

2 2

=

{}

for all   T  Sm+

if and only if it holds that  2(T) > 0 and 2(T) > 0.

Correlation constant. Moreover, we use of the following the quantity. It is not clear to us if it is intrinsically required, or if its appearance in our bound is for merely technical reasons.

(T) = max

1 n

X (), X () :

 1  1,   T,   S1+(m)  T .

We are now in position to provide a bound on  -  1.
Theorem 2. Suppose that model (6) holds with  as considered throughout this subsection and let 0 be defined as in Theorem 1. We then have

 -  1  max

80



(T) 2(T)2(T)

3 2

+

(T) 2(T)

+ 40

1 2(T)

+

1  2(T)

,

80 2(T)

1

+

(T) 2(T)

,

80  2(T)

.

Remark. Given Theorem 2 an improved bound on the prediction error scaling with 20 in place of 0 can be derived, cf. (26) in Appendix D.

The quality of the bound of Theorem 2 depends on how the quantities  2(T), 2(T) and (T) scale

with n, m and r, which is design-dependent. Accordingly, the estimation error in nuclear norm

can be non-finite in the worst case and O(0r) in the best case, which matches existing bounds for nuclear norm regularization (cf. Theorem 2 in [19]).

* The quantity  2(T) is specific to the geometry of the constrained least squares problem

(7) and hence of critical importance. For instance, it follows from Proposition 1 that for standard Gaussian measurements,  2(T) = 0 with high probability once n < m/2. The situation can be much better for random spd measurements (14) as exemplified for mea-

surements Xi =  2(T) = (1/r)

zi as

zi with zi i.i.d. N long as n = (m

(0, I * r).

)

in

Appendix

H.

Specifically,

it

turns out

that

* It is not restrictive to assume 2(T) is positive. Indeed, without that assumption, even an
oracle estimator based on knowledge of the subspace T would fail. Reasonable sampling
operators X have rank min{n, m} so that the nullspace of X only has a trivial intersection with the subspace T as long as n  dim(T) = mr - r(r - 1)/2.

* For fixed T, computing (T) entails solving a biconvex (albeit non-convex) optimization problem in   T and   S1+(m)T. Block coordinate descent is a practical approach to such optimization problems for which a globally optimal solution is out of reach. In this manner we explore the scaling of (T) numerically as done for  2(T). We find that
(T) = O(m/n) so that (T) = O(1) apart from the regime n/m  0, without ruling out the possibility of undersampling, i.e. n < m.

3 Numerical Results

In this section, we empirically study properties of the estimator . In particular, its performance relative to regularization-based methods is explored. We also present an application to spiked covariance estimation for the CBCL face image data set and stock prices from NASDAQ.

Comparison with regularization-based approaches. We here empirically evaluate  -  1

relative to well-known regularization-based methods.

Setup. 1, . . . , n,

We fix

consider rank-one Wishart measurement m = 50 and let n  {0.24, 0.26, . . . , 0.36,

matrices Xi = 0.4, . . . , 0.56} *

mz2i zain,d zri i.i.d.{N1,(02,,

I .

), ..

i= , 10}

vary. Each configuration of (n, r) is run with 50 replications. In each of these, we generate data

yi = tr(Xi) + i,  = 0.1, i = 1, . . . , n,

(15)

where  is generated randomly as rank r Wishart matrices and the {i}ni=1 are i.i.d. N (0, 1).

6

|Sigma - Sigma*|1

|Sigma - Sigma*|1

|Sigma - Sigma*|
1

0.09 r: 1
0.08 0.07 0.06 0.05

constrained LS regularized LS # regularized LS Chen et al. # Chen et al. oracle

0.16 0.14 0.12
0.1

r: 2

constrained LS regularized LS # regularized LS Chen et al. # Chen et al. oracle

0.35 0.3
0.25 0.2

r: 4

constrained LS regularized LS # regularized LS Chen et al. # Chen et al. oracle

0.04

0.08

0.15

0.03 600 700 800 900 1000 1100 1200 1300 1400 n

0.06 600 700 800 900 1000 1100 1200 1300 1400 n

600 700 800 900 1000 1100 1200 1300 1400 n

|Sigma - Sigma*|
1

1

r: 6 constrained LS
0.9 regularized LS #

0.8

regularized LS Chen et al. #

0.7 Chen et al. oracle

0.6

0.5

0.4

0.3

0.2 600 700 800 900 1000 1100 1200 1300 1400 n

|Sigma - Sigma*|1

1.2
1.1 r: 8
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3

constrained LS regularized LS # regularized LS Chen et al. # Chen et al. oracle

700 800 900 1000 1100 1200 1300 1400 n

|Sigma - Sigma*|
1

2 1.8 1.6 1.4 1.2
1 0.8 0.6 0.4
800

r: 10

constrained LS regularized LS # regularized LS Chen et al. # Chen et al. oracle

900 1000 1100 1200 1300 1400 n

Figure 1: Average estimation error (over 50 replications) in nuclear norm for fixed m = 50 and certain choices of n and r. In the legend, "LS" is used as a shortcut for "least squares". Chen et

al. refers to (16). "#"indicates an oracular choice of the tuning parameter. "oracle" refers to the ideal

error r m/n. Best seen in color.

Regularization-based approaches. We compare  to the corresponding nuclear norm regularized estimator in (11). Regarding the choice of the regularization parameter , we consider the grid
 * {0.01, 0.05, 0.1, 0.3, 0.5, 1, 2, 4, 8, 16}, where  =  m/n as recommended in [17] and pick  so that the prediction error on a separate validation data set of size n generated from (15) is minimized. Note that in general, neither  is known nor an extra validation data set is available. Our goal here is to ensure that the regularization parameter is properly tuned. In addition, we consider an oracular choice of  where  is picked from the above grid such that the performance measure
of interest (the distance to the target in the nuclear norm) is minimized. We also compare to the
constrained nuclear norm minimization approach of [8]:

min tr() subject to  0, and y - X () 1  .


(16)

For , we consider the grid n 2/ * {0.2, 0.3, . . . , 1, 1.25}. This specific choice is motivated by
the fact that E[ y - X () 1] = E[  1] = n 2/. Apart from that, tuning of  is performed as for the nuclear norm regularized estimator. In addition, we have assessed the performance of the approach in [3], which does not impose an spd constraint but adds another constraint to (16). That
additional constraint significantly complicates optimization and yields a second tuning parameter. Thus, instead of doing a 2D-grid search, we use fixed values given in [3] for known . The results are similar or worse than those of (16) (note in particular that positive semidefiniteness is not taken advantage of in [3]) and are hence not reported.

Discussion of the results. We conclude from Figure 1 that in most cases, the performance of the constrained least squares estimator does not differ much from that of the regularization-based methods with careful tuning. For larger values of r, the constrained least squares estimator seems to require slightly more measurements to achieve competitive performance.

Real Data Examples. We now present an application to recovery of spiked covariance matrices

which are of the form  =

r j=1

j ujuj

+

2I,

where

r



m

and

j



2

>

0,

j

=

1, . . . , r.

This model appears frequently in connection with principal components analysis (PCA).

Extension to the spiked case. So far, we have assumed that  is of low rank, but it is straightforward to extend the proposed approach to the case in which  is spiked as long as 2 is known or an estimate is available. A constrained least squares estimator of  takes the form  + 2I, where

  argmin 1 Sm+ 2n

y - X ( + 2I)

22.

(17)

The case of 2 unknown or general (unknown) diagonal perturbation is left for future research.

7

log10(|Sigma - Sigma*|F) log10(|Sigma - Sigma*|F)

0.6  = 1/N (1 sample)
2 0.4
 = 1/N (1 sample) 0.2 1.5  = 0.008

0 -0.2

 = 0.008

1

 = 0.08

-0.4  = 0.4

-0.6

 = 0.08

0.5

-0.8 -1
-1.2 -1.4
0

CBCL

 = 0.4  = 1 (all samples) oracle

2 4 6 8 10 12 n / (m * r)

0 NASDAQ

-0.5
oracle

 = 1 (all samples)

0123456 n / (m * r)

Figure 2: Average reconstruction errors log10  -  F in dependence of n/(mr) and the parameter . "oracle" refers to the best rank r-approximation r.

Data sets. (i) The CBCL facial image data set [1] consist of N = 2429 images of 19 x 19 pixels (i.e., m = 361). We take  as the sample covariance matrix of this data set. It turns out that  can be well approximated by r, r = 50, where r is the best rank r approximation to  obtained from computing its eigendecomposition and setting to zero all but the top r eigenvalues. (ii) We construct a second data set from the daily end prices of m = 252 stocks from the technology
sector in NASDAQ, starting from the beginning of the year 2000 to the end of the year 2014 (in total N = 3773 days, retrieved from finance.yahoo.com). We take  as the resulting sample correlation matrix and choose r = 100.

Experimental setup. As in preceding measurements, we consider n random Wishart measurements for the operator X , where n = C(mr), where C ranges from 0.25 to 12. Since
r -  F /  F  10-3 for both data sets, we work with 2 = 0 in (17) for simplicity. To make recovery of  more difficult, we make the problem noisy by using observations

yi = tr(XiSi), i = 1, . . . , n,

(18)

where Si is an approximation to  obtained from the sample covariance respectively sample correlation matrix of N data points randomly sampled with replacement from the entire data set,
i = 1, . . . , n, where  ranges from 0.4 to 1/N (Si is computed from a single data point). For each choice of n and , the reported results are averages over 20 replications.

Results. For the CBCL data set, as shown in Figure 2,  accurately approximates  once the number of measurements crosses 2mr. Performance degrades once additional noise is introduced to the problem by using measurements (18). Even under significant perturbations ( = 0.08), reasonable reconstruction of  remains possible, albeit the number of required measurements increases accordingly. In the extreme case  = 1/N , the error is still decreasing with n, but millions of
samples seems to be required to achieve reasonable reconstruction error.

The general picture is similar for the NASDAQ data set, but the difference between using measurements based on the full sample correlation matrix on the one hand and approximations based on random subsampling (18) on the other hand are more pronounced.

4 Conclusion
We have investigated trace regression in the situation that the underlying matrix is symmetric positive semidefinite. Under restrictions on the design, constrained least squares enjoys similar statistical properties as methods employing nuclear norm regularization. This may come as a surprise, as regularization is widely regarded as necessary in small sample settings.

Acknowledgments
The work of Martin Slawski and Ping Li is partially supported by NSF-DMS-1444124, NSF-III1360971, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137.

8

References
[1] CBCL face dataset. http://cbcl.mit.edu/software-datasets/FaceData2.html.
[2] D. Amelunxen, M. Lotz, M. McCoy, and J. Tropp. Living on the edge: phase transitions in convex programs with random data. Information and Inference, 3:224-294, 2014.
[3] T. Cai and A. Zhang. ROP: Matrix recovery via rank-one projections. The Annals of Statistics, 43:102- 138, 2015.
[4] E. Candes and X. Li. Solving quadratic equations via PhaseLift when there are about as many equations as unknowns. Foundation of Computational Mathematics, 14:1017-1026, 2014.
[5] E. Candes and Y. Plan. Tight oracle bounds for low-rank matrix recovery from a minimal number of noisy measurements. IEEE Transactions on Information Theory, 57:2342-2359, 2011.
[6] E. Candes and B. Recht. Exact matrix completion via convex optimization. Foundation of Computational Mathematics, 9:2053-2080, 2009.
[7] E. Candes, T. Strohmer, and V. Voroninski. PhaseLift: exact and stable signal recovery from magnitude measurements via convex programming. Communications on Pure and Applied Mathematics, 66:1241- 1274, 2012.
[8] Y. Chen, Y. Chi, and A. Goldsmith. Exact and Stable Covariance Estimation from Quadratic Sampling via Convex Programming. IEEE Transactions on Information Theory, 61:4034-4059, 2015.
[9] K. Davidson and S. Szarek. Handbook of the Geometry of Banach Spaces, volume 1, chapter Local operator theory, random matrices and Banach spaces, pages 317-366. 2001.
[10] L. Demanet and P. Hand. Stable optimizationless recovery from phaseless measurements. Journal of Fourier Analysis and its Applications, 20:199-221, 2014.
[11] D. Gross, Y.-K. Liu, S. Flammia, S. Becker, and J. Eisert. Quantum State Tomography via Compressed Sensing. Physical Review Letters, 105:150401-15404, 2010.
[12] R. Horn and C. Johnson. Matrix Analysis. Cambridge University Press, 1985.
[13] M. Kabanva, R. Kueng, and H. Rauhut und U. Terstiege. Stable low rank matrix recovery via null space properties. arXiv:1507.07184, 2015.
[14] M. Klibanov, P. Sacks, and A. Tikhonarov. The phase retrieval problem. Inverse Problems, 11:1-28, 1995.
[15] V. Koltchinskii, K. Lounici, and A. Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39:2302-2329, 2011.
[16] N. Meinshausen. Sign-constrained least squares estimation for high-dimensional regression. The Electronic Journal of Statistics, 7:1607-1631, 2013.
[17] S. Negahban and M. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. The Annals of Statistics, 39:1069-1097, 2011.
[18] B. Recht, M. Fazel, and P. Parillo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52:471-501, 2010.
[19] A. Rohde and A. Tsybakov. Estimation of high-dimensional low-rank matrices. The Annals of Statistics, 39:887-930, 2011.
[20] B. Scholkopf and A. Smola. Learning with kernels. MIT Press, Cambridge, Massachussets, 2002.
[21] M. Slawski and M. Hein. Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization. The Electronic Journal of Statistics, 7:3004-3056, 2013.
[22] M. Slawski, P. Li, and M. Hein. Regularization-free estimation in trace regression with positive semidefinite matrices. arXiv:1504.06305, 2015.
[23] N. Srebro, J. Rennie, and T. Jaakola. Maximum margin matrix factorization. In Advances in Neural Information Processing Systems 17, pages 1329-1336, 2005.
[24] R. Tibshirani. Regression shrinkage and variable selection via the lasso. Journal of the Royal Statistical Society Series B, 58:671-686, 1996.
[25] J. Tropp. User-friendly tools for random matrices: An introduction. 2014. http://users.cms. caltech.edu/jtropp/.
[26] R. Vershynin. How close is the sample covariance matrix to the actual covariance matrix ? Journal of Theoretical Probability, 153:405-419, 2012.
[27] M. Wang, W. Xu, and A. Tang. A unique 'nonnegative' solution to an underdetermined system: from vectors to matrices. IEEE Transactions on Signal Processing, 59:1007-1016, 2011.
[28] C. Williams and M. Seeger. Using the Nystrom method to speed up kernel machines. In Advances in Neural Information Processing Systems 14, pages 682-688, 2001.
9

