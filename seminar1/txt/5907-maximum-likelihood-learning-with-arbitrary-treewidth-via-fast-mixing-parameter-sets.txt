Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing Parameter Sets
Justin Domke NICTA, Australian National University justin.domke@nicta.com.au
Abstract
Inference is typically intractable in high-treewidth undirected graphical models, making maximum likelihood learning a challenge. One way to overcome this is to restrict parameters to a tractable set, most typically the set of tree-structured parameters. This paper explores an alternative notion of a tractable set, namely a set of "fast-mixing parameters" where Markov chain Monte Carlo (MCMC) inference can be guaranteed to quickly converge to the stationary distribution. While it is common in practice to approximate the likelihood gradient using samples obtained from MCMC, such procedures lack theoretical guarantees. This paper proves that for any exponential family with bounded sufficient statistics, (not just graphical models) when parameters are constrained to a fast-mixing set, gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability. When unregularized, to find a solution -accurate in log-likelihood requires a total amount of effort cubic in 1/, disregarding logarithmic factors. When ridge-regularized, strong convexity allows a solution -accurate in parameter distance with effort quadratic in 1/. Both of these provide of a fully-polynomial time randomized approximation scheme.
1 Introduction
In undirected graphical models, maximum likelihood learning is intractable in general. For example, Jerrum and Sinclair [1993] show that evaluation of the partition function (which can easily be computed from the likelihood) for an Ising model is #P-complete, and that even the existence of a fully-polynomial time randomized approximation scheme (FPRAS) for the partition function would imply that RP = NP.
If the model is well-specified (meaning that the target distribution falls in the assumed family) then there exist several methods that can efficiently recover correct parameters, among them the pseudolikelihood [3], score matching [16, 22], composite likelihoods [20, 30], Mizrahi et al.'s [2014] method based on parallel learning in local clusters of nodes and Abbeel et al.'s [2006] method based on matching local probabilities. While often useful, these methods have some drawbacks. First, these methods typically have inferior sample complexity to the likelihood. Second, these all assume a well-specified model. If the target distribution is not in the assumed class, the maximum-likelihood solution will converge to the M-projection (minimum of the KL-divergence), but these estimators do not have similar guarantees. Third, even when these methods succeed, they typically yield a distribution in which inference is still intractable, and so it may be infeasible to actually make use of the learned distribution.
Given these issues, a natural approach is to restrict the graphical model parameters to a tractable set , in which learning and inference can be performed efficiently. The gradient of the likelihood is determined by the marginal distributions, whose difficulty is typically determined by the treewidth of the graph. Thus, probably the most natural tractable family is the set of tree-structured distributions,
1

where  = { : tree T, (i, j)  T, ij = 0}. The Chow-Liu algorithm [1968] provides an efficient method for finding the maximum likelihood parameter vector  in this set, by computing the mutual information of all empirical pairwise marginals, and finding the maximum spanning tree. Similarly, Heinemann and Globerson [2014] give a method to efficiently learn high-girth models where correlation decay limits the error of approximate inference, though this will not converge to the M-projection when the model is mis-specified.
This paper considers a fundamentally different notion of tractability, namely a guarantee that Markov chain Monte Carlo (MCMC) sampling will quickly converge to the stationary distribution. Our fundamental result is that if  is such a set, and one can project onto , then there exists a FPRAS for the maximum likelihood solution inside . While inspired by graphical models, this result works entirely in the exponential family framework, and applies generally to any exponential family with bounded sufficient statistics.
The existence of a FPRAS is established by analyzing a common existing strategy for maximum likelihood learning of exponential families, namely gradient descent where MCMC is used to generate samples and approximate the gradient. It is natural to conjecture that, if the Markov chain is fast mixing, is run long enough, and enough gradient descent iterations are used, this will converge to nearly the optimum of the likelihood inside , with high probability. This paper shows that this is indeed the case. A separate analysis is used for the ridge-regularized case (using strong convexity) and the unregularized case (which is merely convex).

2 Setup

Though notation is introduced when first used, the most important symbols are given here for more reference.

*  - parameter vector to be learned

* M - Markov chain operator corresponding to 

* k - estimated parameter vector at k-th gradient descent iteration

*

qk

=

Mv
k-1

r

- approximate distribution sampled from at iteration k.

(v

iterations of the

Markov chain corresponding to k-1 from arbitrary starting distribution r.)

*  - constraint set for 

* f - negative log-likelihood on training data

* L - Lipschitz constant for the gradient of f .

*  = arg min f () - minimizer of likelihood inside of 

* K - total number of gradient descent steps

* M - total number of samples drawn via MCMC

* N - length of vector x.

* v - number of Markov chain transitions applied for each sample

* C,  - parameters determining the mixing rate of the Markov chain. (Equation 3)

* Ra - sufficient statistics norm bound.

* f - desired optimization accuracy for f

*  - desired optimization accuracy for 

*  - permitted probability of failure to achieve a given approximation accuracy

This paper is concerned with an exponential family of the form
p(x) = exp( * t(x) - A()),
where t(x) is a vector of sufficient statistics, and the log-partition function A() ensures normalization. An undirected model can be seen as an exponential family where t consists of indicator functions for each possible configuration of each clique [32]. While such graphical models motivate this work, the results are most naturally stated in terms of an exponential family and apply more generally.

2

* Initialize 0 = 0. * For k = 1, 2, ..., K

- Draw samples. For i = 1, ..., M , sample

xik-1



qk-1

:=

Mv
k-1

r.

- Estimate the gradient as

f (k-1) +

ek



1 M

M

t(xik-1) - t+ .

i=1

- Update the parameter vector as

k  

k-1

-

1 L

(f (k-1)

+

ek ))

.

*

Output K

or

1 K

K k=1

k

.

f () 
0 

Figure 1: Left: Algorithm 1, approximate gradient descent with gradients approximated via MCMC, analyzed in this paper. Right: A cartoon of the desired performance, stochastically finding a solution near , the minimum of the regularized negative log-likelihood f () in the set .

We are interested in performing maximum-likelihood learning, i.e. minimizing, for a dataset

z1, ..., zD,

f ()

=

-

1 D

D

log

p (zi )

+

 2

22

=

A()

-



*

t+

 2

22,

i=1

(1)

where

we

define

t =

1 D

D i=1

t(zi).

It

is

easy

to

see

that

the

gradient

of

f

takes

the

form

f () = Ep [t(X)] - t+ .

If one would like to optimize f using a gradient-based method, computing the expectation of t(X) with respect to p can present a computational challenge. With discrete graphical models, the expected value of t is determined by the marginal distributions of each factor in the graph. Typically, the computational difficulty of computing these marginal distributions is determined by the
treewidth of the graph- if the graph is a tree, (or close to a tree) the marginals can be computed by the
junction-tree algorithm [18]. One option, with high treewidth, is to approximate the marginals with
a variational method. This can be seen as exactly optimizing a "surrogate likelihood" approximation
of Eq. 1 [31].

Another common approach is to use Markov chain Monte Carlo (MCMC) to compute a sample

{xi}Mi=1 from a distribution close to p, and then approximate Ep [t(X)] by (1/M )

M i=1

t(xi).

This strategy is widely used, varying in the model type, the sampling algorithm, how samples are

initialized, the details of optimization, and so on [10, 25, 27, 24, 7, 33, 11, 2, 29, 5]. Recently,

Steinhardt and Liang [28] proposed learning in terms of the stationary distribution obtained from a

chain with a nonzero restart probability, which is fast-mixing by design.

While popular, such strategies generally lack theoretical guarantees. If one were able to exactly sample from p, this could be understood simply as stochastic gradient descent. But, with MCMC, one can only sample from a distribution approximating p, meaning the gradient estimate is not only noisy, but also biased. In general, one can ask how should the step size, number of iterations,
number of samples, and number of Markov chain transitions be set to achieve a convergence level.

The gradient descent strategy analyzed in this paper, in which one updates a parameter vector k

using approximate gradients is outlined and shown as a cartoon in Figure 1. Here, and in the rest

of the paper, we use pk as a shorthand estimated gradient and the true gradient

ffor(pk-k1,

and we let ek denote the ). The projection operator

difference is defined

between the by [] =

arg min || - ||2.

We assume that the parameter set  is constrained to a set  such that MCMC is guaranteed to mix at a certain rate (Section 3.1). With convexity, this assumption can bound the mean and variance

3

of the errors at each iteration, leading to a bound on the sum of errors. With strong convexity, the

error of the gradient at each iteration is bounded with high probability. Then, using results due to

[26] for projected gradient descent with errors in the gradient, we show a schedule the number of iterations K, the number of samples M , and the number of Markov transitions v such that with high

probability,

f

1 K

K

k

- f ()  f or K - 2  ,

k=1

for the convex or strongly convex cases, respectively, where   arg min f (). The total number of Markov transitions applied through the entire algorithm, KM v grows as (1/f )3 log(1/f ) for the convex case, (1/2) log(1/2) for the strongly convex case, and polynomially in all other parameters of the problem.

3 Background

3.1 Mixing times and Fast-Mixing Parameter Sets

This Section discusses some background on mixing times for MCMC. Typically, mixing times are

defined in terms of the total-variation distance p - qT V = maxA |p(A) - q(A)|, where the max-

imum ranges over the sample space. For discrete distributions, this can be shown to be equivalent to

p - qT V

=

1 2

x |p(x) - q(x)|.

We assume that a sampling algorithm is known, a single iteration of which can be thought of an
operator M that transforms some starting distribution into another. The stationary distribution is p, i.e. limv Mvq = p for all q. Informally, a Markov chain will be fast mixing if the total variation distance between the starting distribution and the stationary distribution decays rapidly in the length of the chain. This paper assumes that a convex set  and constants C and  are known such that for all    and all distributions q,

Mv q - pT V  Cv.

(2)

This means that the distance between an arbitrary starting distribution q and the stationary distribution p decays geometrically in terms of the number of Markov iterations v. This assumption is justified by the Convergence Theorem [19, Theorem 4.9], which states that if M is irreducible and aperiodic with stationary distribution p, then there exists constants   (0, 1) and C > 0 such that

d(v) := sup Mvq - pT V  Cv.
q

(3)

Many results on mixing times in the literature, however, are stated in a less direct form. Given a

constant , the mixing time is defined by  () = min{v : d(v)  }. It often happens that bounds

on mixing times are stated as something like  ()  follows from this that Mvq - pT V  Cv with C =

a

+

b

ln

1 

for some constants

exp(a/b) and  = exp(-1/b).

a

and b.

It

A simple example of a fast-mixing exponential family is the Ising model, defined for x  {-1, +1}N as


p(x|) = exp 

ij xixj + ixi - A() .

(i,j)Pairs

i

A simple result for this model is that, if the maximum degree of any node is  and |ij |   for

all

(i,

j),

then

for

univariate

Gibbs

sampling

with

random

updates,

 ()





N log(N/) 1- tanh()



[19].

The

algorithm discussed in this paper needs the ability to project some parameter vector  onto  to find

arg min ||-||2. Projecting a set of arbitrary parameters onto this set of fast-mixing parameters

is trivial- simply set ij =  for ij >  and ij  - for ij < -.

For more dense graphs, it is known [12, 9] that, for a matrix norm  *  that is the spectral norm  * 2, or induced 1 or infinity norms,

 () 

N log(N/) 1 - R()

(4)

4

where Rij () = |ij |. Domke and Liu [2013] show how to perform this projection for the Ising model when  *  is the spectral norm  * 2 with a convex optimization utilizing the singular value
decomposition in each iteration.

Loosely speaking, the above result shows that univariate Gibbs sampling on the Ising model is fastmixing, as long as the interaction strengths are not too strong. Conversely, Jerrum and Sinclair [1993] exhibited an alternative Markov chain for the Ising model that is rapidly mixing for arbitrary interaction strengths, provided the model is ferromagnetic, i.e. that all interaction strengths are positive with ij  0 and that the field is unidirectional. This Markov chain is based on sampling in different "subgraphs world" state-space. Nevertheless, it can be used to estimate derivatives of the Ising model log-partition function with respect to parameters, which allows estimation of the gradient of the log-likelihood. Huber [2012] provided a simulation reduction to obtain an Ising model sample from a subgraphs world sample.

More generally, Liu and Domke [2014] consider a pairwise Markov random field, defined as



p(x|) = exp  ij (xi, xj ) + i(xi) - A() ,
i,j i

and

show

that,

if

one

defines

Rij ()

=

maxa,b,c

1 2

|ij

(a,

b)-ij(a, c)|,

then

again

Equation

4

holds.

An algorithm for projecting onto the set  = { : R()  c} exists.

There are many other mixing-time bounds for different algorithms, and different types of models [19]. The most common algorithms are univariate Gibbs sampling (often called Glauber dynamics in the mixing time literature) and Swendsen-Wang sampling. The Ising model and Potts models are the most common distributions studied, either with a grid or fully-connected graph structure. Often, the motivation for studying these systems is to understand physical systems, or to mathematically characterize phase-transitions in mixing time that occur as interactions strengths vary. As such, many existing bounds assume uniform interaction strengths. For all these reasons, these bounds typically require some adaptation for a learning setting.

4 Main Results

4.1 Lipschitz Gradient

For lack of space, detailed proofs are postponed to the appendix. However, informal proof sketches are provided to give some intuition for results that have longer proofs. Our first main result is that the regularized log-likelihood has a Lipschitz gradient. Theorem 1. The regularized log-likelihood gradient is L-Lipschitz with L = 4R22 + , i.e.
f () - f ()2  (4R22 + ) - 2.

Proof sketch.

It is easy, by the triangle inequality, that f ()-f ()2





dA d

-

dA d

2

+



-

2

.

Next,

using

the

assumption

that

t(x)2



R2,

one

can

bound

that



dA d

-

dA d

2



2R2p -pT V

.

Finally, some effort can bound that p - pT V  2R2 - 2.

4.2 Convex convergence

Now, our first major result is a guarantee on the convergence that is true both in the regularized case where  > 0 and the unregularized case where  = 0.

Theorem 2.

With probability at least 1 - , at long as M



3K/

log(

1 

),

Algorithm

1

will

satisfy

f

1 K

K

k

k=1

- f ()



8R22 KL

L0 - 2 4R2

+ log

1 

+

K M

+ KCv

2
.

Proof sketch. First, note that f is convex, since the Hessian of f is the covariance of t(X) when



=

0 and 

>

0 only adds a quadratic.

Now, define the quantity dk

=

1 M

M m=1

t(Xmk )

-

5

Eqk [t(X)] to be the difference between the estimated expected value of t(X) under qk and the true value. An elementary argument can bound the expected value of dk, while the Efron-Stein

inequality can bounds its variance. show that, with probability 1 - ,

Using both

K k=1

dk

of 

these bounds 2R2(K/ M

in +

Bernstein's inequality can then

log

1 

).

Finally,

we

can

observe

that

K k=1

ek



K k=1

dk

+

K k=1

Eqk [t(X)]

-

Epk

[t(X )]2 .

By

the

assumption

on

mixing

2spRe2e(dK, t/he Mlas+t tleorgm1i)s+bo2uKndRe2dCbyv

2KR2Cv. And so, with probability 1 - , . Finally, a result due to Schmidt et al. [26] on the

K k=1

ek





convergence

of gradient descent with errors in estimated gradients gives the result.

Intuitively, this result has the right character. If M grows on the order of K2 and v grows on the

order of log K/(- log ), then all terms inside the quadratic will be held constant, and so if we set

K of the order 1/, the sub-optimality will on the order of  with a total computational effort roughly on the order of (1/)3 log(1/). The following results pursue this more carefully. Firstly, one can

observe that a minimum amount of work must be performed.

Theorem 3.

For a, b, c,  > 0, if K, M, v

> 0 are set so that

1 K

(a

+

b

K M

+ Kcv)2

 , then

KMv



a4b2 3

log

ac 

(- log )

.

 Since it must be true that a/ K + b

K/M

+

 K cv



,

each

of

these

three

terms

must

also

be at most , giving lower-bounds on K, M , and v. Multiplying these gives the result.

Next, an explicit schedule for K, M , and v is possible, in terms of a convex set of parameters 1, 2, 3. Comparing this to the lower-bound above shows that this is not too far from optimal.

Theorem 4. Suppose that a, b, c,  > 0. If 1 + 2 + 3 = 1, 1, 2, 3 > 0, then setting

K

=

,a2
12 

M

=

(

ab 12



)2

,

v

=

log

ac 13

/(-

log

)

is

sufficient to guarantee that

1 K

(a

+

b

K M

+

Kcv)2   with a total work of

KMv

=

1 14 22

a4b2 3

log

ac 13

.

(- log )

Simply verify that the  bound holds, and multiply the terms together.

For

example,

setting

1

=

0.66,

2

=

0.33

and

3

=

0.01

gives

that

KMv



48.4

a4b2 3

log

ac 

+5.03

(- log )

.

Finally, we can give an explicit schedule for K, M , and v, and bound the total amount of work that

needs to be performed.

Theorem 5.

If D  max

0

-

2,

4R2 L

log

1 

, then for all  there is a setting of K, M, v such

that

f

(

1 K

K k=1

k )

-

f ()



f

with

probability

1

-



and

KMv



32LR22D4 14223f (1 - )

log

4DR2C 13f

.

[Proof sketch] This follows from setting K, M , and v as in Theorem 4 with a = L0 -

2/(4R2)

+

log

1 

,

b

=

1,

c

=

C,

and



=

f

L/(8R22).

4.3 Strongly Convex Convergence

This section gives the main result for convergence that is true only in the regularized case where  > 0. Again, the main difficulty in this proof is showing that the sum of the errors of estimated gradients at each iteration is small. This is done by using a concentration inequality to show that the error of each estimated gradient is small, and then applying a union bound to show that the sum is small. The main result is as follows.
Theorem 6. When the regularization constant obeys  > 0, with probability at least 1- Algorithm 1 will satisfy

K

-

2



(1

-

 L

)K

0

-

2

+

L 

R2 2M

1+

2

log

K 

+ 2R2Cv

.

6

Proof sketch. When  = 0, f is convex (as in Theorem 2) and so is strongly convex when

 > 0. The basic proof technique here is to decompose the error in a particular step as ek+12 



1 M

M i=1

t(xki )

-

Eqk [t(X)]2

+

effding's inequality can bound the

Eqk first

[t(X)] - Epk [t(X)]2. term, with probability 1

A multidimensional variant of Ho-

-  by R2(1 +

2

log

1 

 )/ M

,

while our assumption on mixing speed can bound the second term by 2R2Cv. Applying this to

all iterations using  = /K gives that all errors are simultaneously bounded as before. This can

then be used in another result due to Schmidt et al. [26] on the convergence of gradient descent with

errors in estimated gradients in the strongly convex case.

A similar proof strategy could be used for the convex case where, rather than directly bounding the sum of the norm of errors of all steps using the Efron-Stein inequality and Bernstein's bound, one could simply bound the error of each step using a multidimensional Hoeffding-type inequality, and then apply this with probability /K to each step. This yields a slightly weaker result than that shown in Theorem 2. The reason for applying a uniform bound on the errors in gradients here is that Schmidt et al.'s bound [26] on the convergence of proximal gradient descent on strongly convex functions depends not just on the sum of the norms of gradient errors, but a non-uniform weighted variant of these.

Again, we consider how to set parameters to guarantee that K is not too far from  with a minimum amount of work. Firstly, we show a lower-bound.

Theorem 7.

Suppose a, b, c

>

0.

Then for any K, M, v

such

that

K

a

+

b M

it

must

be

the

case

that

KMv



b2 2

log

a 

log

c 

(- log )(- log )

log

log

a 

(- log )

.

log(K/)+cv  .

[Proof sketch] This is established by noticing that Ka, b
M

log

K 

,

and

cv

must

each

be

less

than , giving lower bounds on K, M , and v.

Next, we can give an explicit schedule that is not too far off from this lower-bound.

Theorem 8. Suppose that a, b, c,  > 0. If 1 + 2 + 3 = 1, i > 0, then setting K =

log(

a 1



)/(-

log

),

M

=

b2 2 22

1+

2

2 log(K/)

and v = log

c 3

/(- log ) is sufficient

to guarantee that K a + b (1 + 2 log(K/)) + cv   with a total work of at most
M

KMV



b2 222

log

a 1

log

c 3

(- log )(- log ) 

 1 +

2

2

log

log(

a 1

)

(- log )



.



For example, if you choose 2 = 1/ 2 and 1 = 3 = (1 - 1/ 2)/2  0.1464, then this varies

from the lower-bound in Theorem 7 by a factor of two, and a multiplicative factor of 1/3  6.84

inside the logarithmic terms.

Corollary 9.

If we choose K



L 

log

0 - 2 1

,M



L2 R2 22 22 2

1+

2
2 log(K/) , and v 

1 1-

log

(2LR2C/(3)),

then

K

- 2





with

probability at

least

1 - ,

and the

total

amount of work is bounded by

2

KMv



L3R2 22223(1 -

)

log

0 - 2 1

1+

2 log

L 

log

0 - 2 1

.

5 Discussion

An important detail in the previous results is that the convex analysis gives convergence in terms of
the regularized log-likelihood, while the strongly-convex analysis gives convergence in terms of the parameter distance. If we drop logarithmic factors, the amount of work necessary for f - optimality in the log-likelihood using the convex algorithm is of the order 1/3f , while the amount of work necessary for  - optimality using the strongly convex analysis is of the order 1/2. Though these quantities are not directly comparable, the standard bounds on sub-optimality for -strongly convex functions with L-Lipschitz gradients are that 2/2  f  L2/2. Thus, roughly speaking, when regularized for the strongly-convex analysis shows that f optimality in the log-likelihood can be achieved with an amount of work only linear in 1/f .

7

100 f(k) - f(*)

100 10-1

|| k - * ||2

0.4 0.2
0

-0.2

10-50

20 40 iterations k

60 10-20

20 40 iterations k

60 -0.1400

101 iterations k

102

Figure 2: Ising Model Example. Left: The difference of the current test log-likelihood from the optimal log-likelihood on 5 random runs. Center: The distance of the current estimated parameters from the optimal parameters on 5 random runs. Right: The current estimated parameters on one run, as compared to the optimal parameters (far right).

6 Example

While this paper claims no significant practical contribution, it is useful to visualize an example.

Take an Ising model p(x)  exp( (i,j)Pairs ij xixj) for xi  {-1, 1} on a 4 x 4 grid with 5

random vectors as training data. The sufficient statistics are t(x) = {xixj|(i, j)  Pairs}, and with 24 pairs, t(x)2  R2 = 24. For a fast-mixing set, constrain |ij |  .2 for all pairs. Since

the maximum degree is 4,

 ()





N log(N/) 1-4 tanh(.2)



.

Fix



=

1, 

=

2 and 

=

0.1.

Though the

theory above suggests the Lipschitz constant L = 4R22 +  = 97, a lower value of L = 10 is used, which converged faster in practice (with exact or approximate gradients). Now, one can derive that

0 - 2  D = 24 x (2 x .2)2, C = log(16) and  = exp(-(1 - 4 tanh .2)/16). Applying

Corollary 9 with 1 = .01, 2 = .9 and 3 = .1 gives K = 46, M = 1533 and v = 561. Fig. 2

shows the results. In practice, the algorithm finds a solution tighter than the specified , indicating

a degree of conservatism in the theoretical bound.

7 Conclusions
This section discusses some weaknesses of the above analysis, and possible directions for future work. Analyzing complexity in terms of the total sampling effort ignores the complexity of projection itself. Since projection only needs to be done K times, this time will often be very small in comparison to sampling time. (This is certainly true in the above example.) However, this might not be the case if the projection algorithm scales super-linearly in the size of the model.
Another issue to consider is how the samples are initialized. As far as the proof of correctness goes, the initial distribution r is arbitrary. In the above example, a simple uniform distribution was used. However, one might use the empirical distribution of the training data, which is equivalent to contrastive divergence [5]. It is reasonable to think that this will tend to reduce the mixing time when the p is close to the model generating the data. However, the number of Markov chain transitions v prescribed above is larger than typically used with contrastive divergence, and Algorithm 1 does not reduce the step size over time. While it is common to regularize to encourage fast mixing with contrastive divergence [14, Section 10], this is typically done with simple heuristic penalties. Further, contrastive divergence is often used with hidden variables. Still, this provides a bound for how closely a variant of contrastive divergence could approximate the maximum likelihood solution.

The above analysis does not encompass the common strategy for maximum likelihood learning where one maintains a "pool" of samples between iterations, and initializes one Markov chain at each iteration from each element of the pool. The idea is that if the samples at the previous iteration were close to pk-1 and pk-1 is close to pk, then this provides an initialization close to the current solution. However, the proof technique used here is based on the assumption that the samples xki at each iteration are independent, and so cannot be applied to this strategy.

Acknowledgements
Thanks to Ivona Bezakova, Aaron Defazio, Nishant Mehta, Aditya Menon, Cheng Soon Ong and Christfried Webers. NICTA is funded by the Australian Government through the Dept. of Communications and the Australian Research Council through the ICT Centre of Excellence Program.

8

References
[1] Abbeel, P., Koller, D., and Ng, A. Learning factor graphs in polynomial time and sample complexity. Journal of Machine Learning Research, 7:1743-1788, 2006.
[2] Asuncion, A., Liu, Q., Ihler, A., and Smyth, P. Learning with blocks composite likelihood and contrastive divergence. In AISTATS, 2010.
[3] Besag, J. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society. Series D (The Statistician), 24(3):179-195, 1975.
[4] Boucheron, S., Lugosi, G., and Massart, P. Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press, 2013.
[5] Carreira-Peripinan, M. A. and Hinton, G. On contrastive divergence learning. In AISTATS, 2005. [6] Chow, C. I. and Liu, C. N. Approximating discrete probability distributions with dependence trees. IEEE
Transactions on Information Theory, 14:462-467, 1968. [7] Descombes, X., Robin Morris, J. Z., and Berthod, M. Estimation of markov Random field prior parame-
ters using Markov chain Monte Carlo maximum likelihood. IEEE Transactions on Image Processing, 8 (7):954-963, 1996. [8] Domke, J. and Liu, X. Projecting Ising model parameters for fast mixing. In NIPS, 2013. [9] Dyer, M. E., Goldberg, L. A., and Jerrum, M. Matrix norms and rapid mixing for spin systems. Ann. Appl. Probab., 19:71-107, 2009. [10] Geyer, C. Markov chain Monte Carlo maximum likelihood. In Symposium on the Interface, 1991. [11] Gu, M. G. and Zhu, H.-T. Maximum likelihood estimation for spatial models by Markov chain Monte Carlo stochastic approximation. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2):339-355, 2001. [12] Hayes, T. A simple condition implying rapid mixing of single-site dynamics on spin systems. In FOCS, 2006. [13] Heinemann, U. and Globerson, A. Inferning with high girth graphical models. In ICML, 2014. [14] Hinton, G. A practical guide to training restricted boltzmann machines. Technical report, University of Toronto, 2010. [15] Huber, M. Simulation reductions for the ising model. Journal of Statistical Theory and Practice, 5(3): 413-424, 2012. [16] Hyvarinen, A. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6:695-709, 2005. [17] Jerrum, M. and Sinclair, A. Polynomial-time approximation algorithms for the ising model. SIAM Journal on Computing, 22:1087-1116, 1993. [18] Koller, D. and Friedman, N. Probabilistic Graphical Models: Principles and Techniques. MIT Press, 2009. [19] Levin, D. A., Peres, Y., and Wilmer, E. L. Markov chains and mixing times. American Mathematical Society, 2006. [20] Lindsay, B. Composite likelihood methods. Contemporary Mathematics, 80(1):221-239, 1988. [21] Liu, X. and Domke, J. Projecting Markov random field parameters for fast mixing. In NIPS, 2014. [22] Marlin, B. and de Freitas, N. Asymptotic efficiency of deterministic estimators for discrete energy-based models: Ratio matching and pseudolikelihood. In UAI, 2011. [23] Mizrahi, Y., Denil, M., and de Freitas, N. Linear and parallel learning of markov random fields. In ICML, 2014. [24] Papandreou, G. and Yuille, A. L. Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models. In ICCV, 2011. [25] Salakhutdinov, R. Learning in Markov random fields using tempered transitions. In NIPS, 2009. [26] Schmidt, M., Roux, N. L., and Bach, F. Convergence rates of inexact proximal-gradient methods for convex optimization. In NIPS, 2011. [27] Schmidt, U., Gao, Q., and Roth, S. A generative perspective on MRFs in low-level vision. In CVPR, 2010. [28] Steinhardt, J. and Liang, P. Learning fast-mixing models for structured prediction. In ICML, 2015. [29] Tieleman, T. Training restricted Boltzmann machines using approximations to the likelihood gradient. In ICML, 2008. [30] Varin, C., Reid, N., and Firth, D. An overview of composite likelihood methods. Statistica Sinica, 21: 5-24, 2011. [31] Wainwright, M. Estimating the "wrong" graphical model: Benefits in the computation-limited setting. Journal of Machine Learning Research, 7:1829-1859, 2006. [32] Wainwright, M. and Jordan, M. Graphical models, exponential families, and variational inference. Found. Trends Mach. Learn., 1(1-2):1-305, 2008. [33] Zhu, S. C., Wu, Y., and Mumford, D. Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2):107-126, 1998.
9

