Unified View of Matrix Completion under General Structural Constraints

Suriya Gunasekar UT at Austin, USA suriya@utexas.edu

Arindam Banerjee UMN Twin Cities, USA banerjee@cs.umn.edu

Joydeep Ghosh UT at Austin, USA ghosh@ece.utexas.edu

Abstract
Matrix completion problems have been widely studied under special low dimensional structures such as low rank or structure induced by decomposable norms. In this paper, we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by any norm regularization. We consider two estimators for the general problem of structured matrix completion, and provide unified upper bounds on the sample complexity and the estimation error. Our analysis relies on generic chaining, and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain partial complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of Gaussian widths, and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization. Further, we provide several non-trivial examples of structures included in our framework, notably including the recently proposed spectral k-support norm.
1 Introduction
The task of completing the missing entries of a matrix from an incomplete subset of (potentially noisy) entries is encountered in many applications including recommendation systems, data imputation, covariance matrix estimation, and sensor localization among others. Traditionally ill-posed high dimensional estimation problems, where the number of parameters to be estimated is much higher than the number of observations, has been extensively studied in the recent literature. However, matrix completion problems are particularly ill-posed as the observations are both limited (high dimensional), and the measurements are extremely localized, i.e., the observations consist of individual matrix entries. The localized measurement model, in contrast to random Gaussian or sub- Gaussian measurements, poses additional complications in general high dimensional estimation.
For well-posed estimation in high dimensional problems including matrix completion, it is imperative that low dimensional structural constraints are imposed on the target. For matrix completion, the special case of low-rank constraint has been widely studied. Several existing work propose tractable estimators with near-optimal recovery guarantees for (approximate) low-rank matrix completion [8, 7, 28, 26, 18, 19, 22, 11, 20, 21]. A recent work [16] addresses the extension to structures with decomposable norm regularization. However, the scope of matrix completion extends for low dimensional structures far beyond simple low-rankness or decomposable norm structures.
In this paper, we consider a unified statistical analysis of matrix completion under a general set of low dimensional structures that are induced by any suitable norm regularization. We provide statistical analysis of two generalized matrix completion estimators, the constrained norm minimizer, and the generalized matrix Dantzig selector (Section 2.2). The main results in the paper (Theorem 1a- 1b) provide unified upper bounds on the sample complexity and estimation error of these estimators
1

for matrix completion under any norm regularization. Existing results on matrix completion with low rank or other decomposable structures can be obtained as special cases of our general results.

Our unified analysis of sample complexity is motivated by recent work on high dimensional estimation using global (sub) Gaussian measurements [10, 1, 35, 3, 37, 5]. A key ingredient in the recovery analysis of high dimensional estimation involves establishing a certain variation of Restricted Isometry Property (RIP) [9] of the measurement operator. It has been shown that such properties are satisfied by Gaussian and sub-Gaussian measurement operators with high probability. Unfortunately, as has been noted before by Candes et al. [8], owing to highly localized measurements, such conditions are not satisfied in the matrix completion problem, and the existing results based on global (sub) Gaussian measurements are not directly applicable. In fact, a key question we consider is: given the radically limited measurement model in matrix completion, by how much would the sample complexity of estimation increase beyond the known sample complexity bounds for global (sub) Gaussian measurements. Our results upper bounds the sample complexity for matrix completion to within only a log d factor larger over that of global (sub) Gaussian measurements [10, 3, 5]. While the result is known for low rank matrix completion using nuclear norm minimization [26, 20], with a careful use of generic chaining, we show that the log d factor suffices for structures induced by any norm! As a key intermediate result, we show that a useful form restricted strong convexity (RSC) [27] holds for the localized measurements encountered in matrix completion under general norm regularized structures. The result substantially generalizes existing RSC results for matrix completion under the special cases of nuclear norm and decomposable norm regularization [26, 16].

For our analysis, we use tools from generic chaining [33] to characterize the main results (Theorem 1a-1b) in terms of the Gaussian width (Definition 1) of certain error sets. Gaussian widths provide a powerful geometric characterization for quantifying the complexity of a structured low dimensional subset in a high dimensional ambient space. Such a unified characterization in terms of Gaussian width has the advantage that numerous tools have been developed in the literature for bounding the Gaussian width for structured sets, and this literature can be readily leveraged to derive new recovery guarantees for matrix completion under suitable structural constraints (Appendix D.2).

In addition to the theoretical elegance of such a unified framework, identifying useful but potentially non-decomposable low dimensional structures is of significant practical interest. The broad class of structures enforced through symmetric convex bodies and symmetric atomic sets [10] can be analyzed under this paradigm (Section 2.1). Such specialized structures can potentially capture the constraints in certain applications better than simple low-rankness. In particular, we discuss in detail, a non-trivial example of the spectral k-support norm introduced by McDonald et al. [25].

To summarize the key contributions of the paper: * Theorem 1a-1b provide unified upper bounds on sample complexity and estimation error for matrix completion estimators using general norm regularization: a substantial generalization of the existing results on matrix completion under structural constraints. * Theorem 1a is applied to derive statistical results for the special case of matrix completion under spectral k-support norm regularization. * An intermediate result, Theorem 5 shows that under any norm regularization, a form of Restricted Strong Convexity (RSC) holds in the matrix completion setting with extremely localized measurements. Further, a certain partial measure of complexity of a set is encountered in matrix completion analysis (12). Another intermediate result, Theorem 2 provides bounds on the partial complexity measures in terms of a better understood complexity measure of Gaussian width. These intermediate results are of independent interest beyond the scope of the paper.

Notations and Preliminaries

Indexes i, j are typically used to index rows and columns respectively of matrices, and index k is used to index the observations. ei, ej, ek, etc. denote the standard basis in appropriate dimensions1.
Notation G ans g are used to denote a matrix and vector respectively, with independent standard

Gaussian random variables. P(.) and E(.) denote the probability of an event and the expectation of a random variable, respectively. Given an integer N , let [N ] = {1, 2, . . . , N }. Euclidean norm in a

vector space is denoted as x 2 = x, x . For a matrix X with singular values 1  2  . . .,

common norms include the Frobenius norm X F =

i i2, the nuclear norm X  = i i,

1for brevity we omit the explicit dependence of dimension unless necessary

2

the spectral norm X op = 1, and the maximum norm X  = maxij |Xij|. Also let, Sd1d2-1 = {X  Rd1xd2 : X F = 1} and Bd1d2 = {X  Rd1xd2 : X F  1}. Finally, given a norm . defined on a vectorspace V, its dual norm is given by X  = sup Y 1 X, Y .
Definition 1 (Gaussian Width). Gaussian width of a set S  Rd1xd2 is a widely studied measure of complexity of a subset in high dimensional ambient space and is given by:

wG(S) = EG sup X, G ,
X S

(1)

where recall that G is a matrix of independent standard Gaussian random variables. Some key results

on Gaussian width are discussed in Appendix D.2.

Definition 2 (Sub-Gaussian Random Variable [36]). The sub-Gaussian norm of a random variable X is given by: X 2 = supp1 p-1/2(E|X|p)1/p. X is said be b-sub-Gaussian if X 2  b.

Equivalently, X is sub-Gaussian if one of the following conditions are satisfied for some constants

k1, (1)

k2,pand1k,3(E[L|Xem|pm)1a/5p.5 obf[3p6, ]].

(2) t > 0, P(|X| > t)  e1-t2/k12b2 ,

(3) E[ek2X2/b2 ]  e, or

(4) if EX = 0, then s > 0, E[esX ]  ek3s2b2/2.

Definition 3 (Restricted Strong Convexity (RSC)). A function L is said to satisfy Restricted Strong

Convexity

(RSC)

at

 with  

respect to a subset S, if for some RSC S, L( + ) - L() - L(), 

paraLmeter2F.L

>

0,

(2)

Definition 4 (Spikiness Ratio [26]). For X  Rd1xd2 , a measure of the "spikiness" is given by:

sp(X) =

d1d2 X  . XF

(3)

Definition 5 (Norm Compatibility Constant [27]). The compatibility constant of a norm R : V  R

under a closed convex cone C  V is defined as follows:

R(C)

=

sup
X C \{0}

R(X ) XF

.

(4)

2 Structured Matrix Completion

Denote the ground truth target matrix as   Rd1xd2 ; let d = d1 + d2. In the noisy matrix completion, observations consists of individual entries of  observed through an additive noise channel.

Sub-Gaussian Noise: Given, a list of independently sampled standard basis  = {Ek = eik ejk :

ik  [d1], jk  [d2]} with potential duplicates, observations (yk)  R|| are given by:

yk = , Ek + k, for k = 1, 2, . . . , ||,

(5)

where   R|| is the noise vector of independent sub-Gaussian random variables with E[k] = 0,

and (note

k 2 = 1 (recall . Var(k)  constant).

2 from Definition 2), and 2 is scaled variance of noise Also, without loss of generality, assume normalization

per 

observation, F = 1.

Uniform Sampling: Assume that the entries in  are drawn independently and uniformly:

Ek  uniform{eiej : i  [d1], j  [d2]}, for Ek  .

(6)

Given , define the linear operator P : Rd1xd2  R as follows (ek  R||):

P(X) =

|| k=1

X, Ek

ek

(7)

Structural Constraints For matrix completion with || < d1d2, low dimensional structural constraints on  are necessary for well-posedness. We consider a generalized constraint setting wherein for some low-dimensional model space M,   M is enforced through a surrogate norm regularizer R(.). We make no further assumptions on R other than it being a norm in Rd1xd2 .

Low Spikiness In matrix completion under uniform sampling model, further restrictions on  (be-

yond low dimensional structure) are required to ensure that the most informative entries of the matrix

are observed with high probability [8]. Early work assumed stringent matrix incoherence conditions

for low-rank completion to preclude such matrices [7, 18, 19], while more recent work [11, 26], re-

lax these assumptions to a more intuitive restriction of the spikiness ratio, defined in (3). However,

under this relaxation only an approximate recovery is typically guaranteed in low-noise regime, as

opposed to near exact recovery under incoherence assumptions.

Assumption 1 (Spikiness Ratio). There exists  > 0, such that





=

sp()

 F d1 d2

  .
d1 d2

3

2.1 Special Cases and Applications

We briefly introduce some interesting examples of structural constraints with practical applications.
Example 1 (Low Rank and Decomposable Norms). Low-rankness is the most common structure used in many matrix estimation problems including collaborative filtering, PCA, spectral clustering, etc. Convex estimators using nuclear norm   regularization has been widely studied statistically [8, 7, 28, 26, 18, 19, 22, 11, 20, 21]. A recent work [16] extends the analysis of low rank matrix completion to general decomposable norms, i.e. R : X, Y  (M, M), R(X+Y ) = R(X)+R(Y ).
Example 2 (Spectral k-support Norm). A non-trivial and significant example of norm regularization that is not decomposable is the spectral k-support norm recently introduced by McDonald et al. [25]. Spectral k-support norm is essentially the vector k-support norm [2] applied on the singular values () of a matrix   Rd1xd2 . Without loss of generality, let d = d1 = d2. Let Gk = {g  [d] : |g|  k} be the set of all subsets [d] of cardinality at most k, and denote the set V(Gk) = {(vg)gGk : vg  Rd, supp(vg)  g}. The spectral k-support norm is given by:

 k-sp = inf

vg 2 :

vg = () ,

vV(Gk) gGk

gGk

(8)

McDonald et al. [25] showed that spectral k-support norm is a special case of cluster norm [17]. It was further shown that in multi-task learning, wherein the tasks (columns of ) are assumed to be clustered into dense groups, the cluster norm provides a trade-off between intra-cluster variance, (inverse) inter-cluster variance, and the norm of the task vectors. Both [17] and [25] demonstrate superior empirical performance of cluster norms (and k-support norm) over traditional trace norm and spectral elastic net minimization on bench marked matrix completion and multi-task learning datasets. However, there are no known work on the statistical analysis of matrix completion with spectral k-support norm regularization. In Section 3.2, we discuss the consequence of our main theorem for this non-trivial special case.

Example 3 (Additive Decomposition). Elementwise sparsity is a common structure often assumed
in high-dimensional estimation problems. However, in matrix completion, elementwise sparsity
conflicts with Assumption 1 (and more traditional incoherence assumptions). Indeed, it is easy to see that with high probability most of the || d1d2 uniformly sampled observations will be zero, and an informed prediction is infeasible. However, elementwise sparse structures are often used within an additive decomposition framework, wherein  = k (k), such that each component matrix (k) is in turn structured (e.g. low rank+sparse used for robust PCA [6]). In such structures,
there is no scope for recovering sparse components outside the observed indices, and it is assumed that: (k) is sparse  supp((k))  . Further, the sparsity assumption might still conflict with
the spikiness assumption. In such cases, consistent matrix completion is feasible under additional
regularity assumptions if the superposed matrix is non-spiky. A candidate norm regularizer for such
structures is the weighted infimum convolution of individual structure inducing norms [6, 39],

Rw() = inf

wkRk((k)) : (k) =  .

kk

Example 4 (Other Applications). Other potential applications including cut matrices [30, 10], structures induced by compact convex sets, norms inducing structured sparsity assumptions on the spectrum of , etc. can also be handled under the paradigm of this paper.

2.2 Structured Matrix Estimator

Let R be the norm surrogate for the structural constraints on , and R denote its dual norm. We propose and analyze two convex estimators for the task of structured matrix completion: Constrained Norm Minimizer

cn = argmin R()








d1 d2

s.t. P() - y 2  cn.

(9)

Generalized Matrix Dantzig Selector

ds = argmin R()








d1 d2



s.t.

d1d2 ||

R P (P ()

-

y)



ds,

(10)

4

where P : R  Rd1xd2 is the linear adjoint of P, i.e. P(X), y = X, P(y) . Note: Theorem 1a-1b gives consistency results for (9) and (10), respectively, under certain conditions on the parameters cn > 0, ds > 0, and  > 1. In particular, these conditions assume knowledge of the noise variance 2 and spikiness ratio sp(). In practice, both  and sp() are typically unknown and the parameters are tuned by validating on held out data.

3 Main Results

We define the following "restricted" error cone and its subset: TR = TR() = cone{ : R( + )  R()}, and ER = TR  Sd1d2-1.

(11)

Let cn and ds be the estimates from (9) and (10), respectively. If cn and ds are chosen such that  belongs to the feasible sets in (9) and (10), respectively, then the error matrices cn = cn -  and ds = ds -  are contained in TR.

Theorem 1a (Constrained Norm Minimizer). Under the problem setup in Section 2, let cn =  + cn be the estimate from (9) with cn = 2. For large enough c0, if || > c20wG2 (ER) log d, then there exists an RSC parameter c0 > 0 and constants c1, c2, c3 such that with probability greater than 1-exp(-c1wG2 (ER))-2 exp(-c2wG2 (ER) log d),

1 d1d2

cn

2 F



4

max

c32 , 2 c20wG2 (ER) log d

c0 d1d2

||

.

Theorem 1b (Matrix Dantzig Selector). Under the problem setup in Section 2, let ds =

 + ds be the estimate from (10) with ds  2

d1 d2 ||

R

P

(w).

For large enough c0, if

|| > c20wG2 (ER) log d, then there exists an RSC parameter c0 > 1, and constant c1 such that with probability greater than 1-exp(-c1wG2 (ER)),

ds

2 F



4

max

2ds

2R(TR 2c0

)

,

2

c20

wG2

(ER ||

)

log

d

.

Recall Gaussian width wG and subspace compatibility constant R from (1) and (4), respectively.

Remarks:

1. If R() =


  and rank() = r, then wG2 (ER)  3dr, R(TR)  2r and

d1 d2 ||

P() 2  2

d log d ||

w.h.p [10,

14,

26].

Using these bounds in

Theorem 1b recovers

near-optimal results for low rank matrix completion under spikiness assumption [26].

2. For both estimators, upper bound on sample complexity is dominated by the square of Gaussian

width which is often considered the effective dimension of a subset in high dimensional space

and plays a key role in high dimensional estimation under Gaussian measurement ensembles.

The results show that, independent of R(.), the upper bound on sample complexity for consistent

matrix completion with highly localized measurements is within a log d factor of the known

sample complexity of  wG2 (ER) for estimation from Gaussian measurements [3, 10, 37, 5]. 3. First term in estimation error bounds in Theorem 1a-1b scales with 2 which is the per observa-

tion noise variance (upto constant). The second term is an upper bound on error that arises due to unidentifiability of  within a certain radius under the spikiness constraints [26]; in contrast

[7] show exact recovery when  = 0 using more stringent matrix incoherence conditions.

4. Bound on cn from Theorem 1a is comparable to the result by Candes et al. [7] for low rank matrix completion under non-low-noise regime, where the first term dominates, and those of [10, 35] for high dimensional estimation under Gaussian measurements. With a bound on wG2 (ER), it is easy to specialize this result for new structural constraints. However, this bound is potentially loose and asymptotically converges to a constant error proportional to the noise variance 2.
5. The estimation error bound in Theorem 1b is typically sharper than that in Theorem 1a. How-
ever, for specific structures, using application of Theorem 1b requires additional bounds on ER(P(W )) and R(TR) besides wG2 (ER).

3.1 Partial Complexity Measures Recall that for wG(S) = E supXS X, G and R|| g  N (0, I||) is a standard normal vector.

5

Definition 6 (Partial Complexity Measures). Given a randomly sampled collection  = {Ek  Rd1xd2 }, and a random vector   R||, the partial -complexity measure of S is given by:

w,(S) = E, sup X, P() .
X S -S

(12)

The special cases where  is a vector of standard Gaussian g, or standard Rademacher (i.e. k 
{-1, 1} w.p. 1/2) variables, are of particular interest. In the case of symmetric , like g and , w,(S) = 2E, supXS X, P() , and the later expression will be used interchangeably ignoring the constant term.

Theorem 2 (Partial Gaussian Complexity). Let S  Bd1d2 , and let  be sampled according to (6).  universal constants K1, K2, K3, and K4 such that:

w,g(S)  K1

|| d1d2 wG(S) + min K2

E sup
X,Y S

P(X - Y )

22,

K3

sup
X S

sp (X ) d1d2

(13)

Further, for a centered i.i.d. 1-sub-Gaussian vector   R||, w,(S)  K4w,g(S). Note: For  [d1] x [d2], the second term in (13) is a consequence of the localized measurements.

3.2 Spectral k-Support Norm

We introduced spectral k-support norm in Section 2.1. The estimators from (9) and (10) for spectral

k-support norm can be efficiently solved through proximal methods using the proximal operators

derived in [25]. We are interested in the statistical guarantees for matrix completion using spectral

k-support norm regularization. We extend the analysis for upper bounding the Gaussian width of the

descent cone for the vector k-support norm by [29] to the case of spectral k-support norm. WLOG

let d1 = d2 = d. Let   Rd be the vector of singular values of  sorted in non-ascending order.

Let

r



{0, 1, 2, . . . , k - 1}

be

the

unique

integer

satisfying:

k-r-1

>

1 r+1

p i=k-r

i



k-r .

Denote I2 = {1, 2, . . . , k - r - 1}, I1 = {k - r, k - r + 1, . . . , s}, and I0 = {s + 1, s + 2, . . . , d}.

Finally, for I  [d], (I)i = 0 i  Ic, and (I)i = i i  I.

Lemma 3. If rank of  is s and ER is the error set from R() =  k-sp, then

wG2 (ER)  s(2d- s) +

(r + 1)2 I1

I2
2 1

2
2 + |I1|

(2d- s).

Proof of the above lemma is provided in the appendix. Lemma 3 can be combined with Theorem 1a

to obtain recovery guarantees for matrix completion under spectral k-support norm.

4 Discussions and Related Work

Sample Complexity: For consistent recovery in high dimensional convex estimation, it is desirable that the descent cone at the target parameter  is "small" relative to the feasible set (enforced by the observations) of the estimator. Thus, a measure of complexity/size of the error cone at  is crucial in establishing sample complexity and estimation error bounds. Results in this paper are largely characterized in terms of a widely used complexity measure of Gaussian width wG(.), and can be compared with the literature on estimation from Gaussian measurements.
Error Bounds: Theorem 1a provides estimation error bounds that depends only on the Gaussian width of the descent cone. In non-low-noise regime, this result is comparable to analogous results of constrained norm minimization [6, 10, 35]. However, this bound is potentially loose owing to mismatched data-fit term using squared loss, and asymptotically converges to a constant error proportional to the noise variance 2. A tighter analysis on the estimation error can be obtained for the matrix Dantzig selector (10) from Theorem 1b. However, application of Theorem 1b requires computing high probability upper bound on R(P(W )). The literature on norms of random matrices [13, 24, 36, 34] can be exploited in deriving such bounds. Beside, in special cases: if R(.)  K . , then KR(.)  . op can be used to obtain asymptotically consistent results.
Finally, under near zero-noise, the second term in the results of Theorem 1 dominates, and bounds are weaker than that of [6, 19] owing to the relaxation of stronger incoherence assumption.

6

Related Work and Future Directions: The closest related work is the result on consistency of matrix completion under decomposable norm regularization by [16]. Results in this paper are a strict generalization to general norm regularized (not necessarily decomposable) matrix completion. We provide non-trivial examples of application where structures enforced by such non-decomposable norms are of interest. Further, in contrast to our results that are based on Gaussian width, the RSC parameter in [16] depends on a modified complexity measure R(d, ||) (see definition in [16]). An advantage of results based on Gaussian width is that, application of Theorem 1 for special cases can greatly benefit from the numerous tools in the literature for the computation of wG(.). Another closely related line of work is the non-asymptotic analysis of high dimensional estimation under random Gaussian or sub-Gaussian measurements [10, 1, 35, 3, 37, 5]. However, the analysis from this literature rely on variants of RIP of the measurement ensemble [9], which is not satisfied by the the extremely localized measurements encountered in matrix completion[8]. In an intermediate result, we establish a form of RSC for matrix completion under general norm regularization: a result that was previously known only for nuclear norm and decomposable norm regularization.
In future work, it is of interest to derive matching lower bounds on estimation error for matrix completion under general low dimensional structures, along the lines of [22, 5] and explore special case applications of the results in the paper. We also plan to derive explicit characterization of ds in terms of Gaussian width of unit balls by exploiting generic chaining results for general Banach spaces [33].

5 Proof Sketch

Proofs of the lemmas are provided in the Appendix.

5.1 Proof of Theorem 1

Define the following set of -non-spiky matrices in Rd1xd2 for constant c0 from Theorem 1:



A() = X : sp(X) =

d1d2 X    . XF

(14)

Define,

1 || c0 = c0 wG2 (ER) log d

(15)

Case 1: Spiky Error Matrix When following bound on error is immediate

the error matrix from (9) or using      + 

(10)2has/ladrg1ed2spinik(i3n)e.ss

ratio,

Proposition 4 (Spiky Error Matrix). For the constant c0 in Theorem 1a, if sp(cn) / A(c0 ), then

cn F  2c0

wG2 (ER) log ||

d.

An

analogous

result

also

holds

for

ds.

Case 2: Non-Spiky Error Matrix Let ds, cn  A().

5.1.1 Restricted Strong Convexity (RSC)

Recall TR and ER from (11). The most significant step in the proof of Theorem 1 involves showing that over a useful subset of TR, a form of RSC (2) is satisfied by a squared loss penalty.

Theorem 5 (Restricted Strong Convexity). Let || > c20wG2 (ER) log d, for large enough constant c0; further, sub-sampling excess samples such that ||  (wG2 (ER) log2 d). There exists a RSC parameter c0 = 1 - c0 > 0, such that the following holds w.p. greater that 1 - exp(-c1wG2 (ER)),

X  TR  A(),

d1d2 ||

P (X )

2 2



c0

X

2 F

.

Proof in Appendix A combines empirical process tools along with Theorem 2.

Recall from (5), that y - P() = w, where w  R|| consists of independent sub-Gaussian random variables with E[wk] = 0 and wk 2 = 1 (recall . 2 from Definition 2).

7

5.1.2 Constrained Norm Minimizer
Lemma 6. Under the conditions of Theorem 1, let c1 be a constant such that k, Var(wk)  c1.  a universal constant c2 such that, if cn  2c1 ||, then with probability greater than 1 - 2 exp (-c2||), (a) ds  TR, and (b) P(cn) 2  2cn.

Using cn = 2c1 || in (9), if cn  A(), then using Theorem 5 and Lemma 6, w.h.p.

cn

2
F

1

P(cn)

2 2



4c212

.

d1d2 c0 ||

c0

(16)

5.1.3 Matrix Dantzig Selector



Proposition 7. ds  

d1 d2 ||

R P (w)



w.h.p.

(a)

ds

 TR;

(b)

d1 d2 ||

R

P

(P

(ds

))



2ds

.

Above result follows from optimality of ds and triangle inequality. Also,



d1d2 ||

P(ds)

2 2



d1d2 ||

R P (P (ds ))R(ds )



2dsR(TR)

ds

F,

where recall norm compatibility constant R(TR) from (4). Finally, using Theorem 5, w.h.p.

ds

2 F



1

P(ds)

2 2



4ds

R(TR

)

ds

F.

d1d2

|| c0

c0 d1d2

(17)

5.2 Proof of Theorem 2

Let the entries of  = {Ek = eik ejk : k = 1, 2, . . . , ||} be sampled as in (6). Recall that g  R|| is a standard normal vector. For a compact S  Rd1xd2 , it suffices to prove Theorem 2 for a dense countable subset of S. Overloading S to such a countable subset, define following random process:

(X,g(X))XS, where X,g(X) = X, P(g) = k X, Ek gk.

(18)

We start with a key lemma in the proof of Theorem 2. Proof of this lemma, provided in Appendix B, uses tools from the broad topic of generic chaining developed in recent works [31, 33].
Lemma 8.  constants k1, k2 such that for S  Sd1d2-1, then

w,g(S) = E sup X,g(X)  k1
X S

|| d1d2 wG(S) + k2

E sup P(X - Y ) 22.
X,Y S

Lemma 9. There exists constants k3, k4, such that for S  Sd1d2-1

E sup
X,Y S

P(X - Y )

2 2

 k3 sup
X S

sp (X ) d1d2

w,g

(S

)

+

k4

|| d1d2

wG2 (S

)

 Theorem 2 follows by combining Lemma 8 and Lemma 9, and simplifying using ab  a/2 + b/2 and triangle inequality (See Appendix B). The statement in Theorem 2 about partial sub-Gaussian complexity follows from a standard result in empirical process given in Lemma 12.

Acknowledgments We thank the anonymous reviewers for helpful comments and suggestions. S. Gunasekar and J. Ghosh acknowledge funding funding from NSF grants IIS-1421729, IIS-1417697, and IIS1116656. A. Banerjee acknowledges NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and NASA grant NNX12AQ39A.

8

References
[1] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: A geometric theory of phase transitions in convex optimization. Inform. Inference, 2014.
[2] A. Argyriou, R. Foygel, and N. Srebro. Sparse prediction with the k-support norm. In NIPS, 2012. [3] A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. In NIPS, 2014. [4] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh. Clustering with bregman divergences. JMLR, 2005. [5] T. Cai, T. Liang, and A. Rakhlin. Geometrizing local rates of convergence for linear inverse problems.
arXiv preprint, 2014. [6] E. J. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? ACM, 2011. [7] E. J. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 2010. [8] E. J. Candes and B. Recht. Exact matrix completion via convex optimization. FoCM, 2009. [9] Emmanuel J Candes and Terence Tao. Decoding by linear programming. Information Theory, IEEE
Transactions on, 2005. [10] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse
problems. Foundations of Computational Mathematics, 2012. [11] M. A. Davenport, Y. Plan, E. Berg, and M. Wootters. 1-bit matrix completion. Inform. Inference, 2014. [12] R. M. Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian processes. Journal
of Functional Analysis, 1967. [13] A. Edelman. Eigenvalues and condition numbers of random matrices. Journal on Matrix Analysis and
Applications, 1988. [14] M. Fazel, H Hindi, and S. P. Boyd. A rank minimization heuristic with application to minimum order
system approximation. In American Control Conference, 2001. [15] J. Forster and M. Warmuth. Relative expected instantaneous loss bounds. Journal of Computer and
System Sciences, 2002. [16] S. Gunasekar, P. Ravikumar, and J. Ghosh. Exponential family matrix completion under structural con-
straints. In ICML, 2014. [17] L. Jacob, J. P. Vert, and F. R. Bach. Clustered multi-task learning: A convex formulation. In NIPS, 2009. [18] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans. IT, 2010. [19] R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. JMLR, 2010. [20] O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli, 2014. [21] O. Klopp. Matrix completion by singular value thresholding: sharp bounds. arXiv preprint arXiv, 2015. [22] Vladimir Koltchinskii, Karim Lounici, Alexandre B Tsybakov, et al. Nuclear-norm penalization and
optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 2011. [23] M. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer, 1991. [24] A. E. Litvak, A. Pajor, M. Rudelson, and N. Tomczak-Jaegermann. Smallest singular value of random
matrices and geometry of random polytopes. Advances in Mathematics, 2005. [25] A. M. McDonald, M. Pontil, and D. Stamos. New perspectives on k-support and cluster norms. arXiv
preprint, 2014. [26] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal
bounds with noise. JMLR, 2012. [27] S. Negahban, B. Yu, M. J. Wainwright, and P. Ravikumar. A unified framework for high-dimensional
analysis of m-estimators with decomposable regularizers. In NIPS, 2009. [28] B. Recht. A simpler approach to matrix completion. JMLR, 2011. [29] E. Richard, G. Obozinski, and J.-P. Vert. Tight convex relaxations for sparse matrix factorization. In
ArXiv e-prints, 2014. [30] N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Learning Theory. Springer, 2005. [31] M. Talagrand. Majorizing measures: the generic chaining. The Annals of Probability, 1996. [32] M. Talagrand. Majorizing measures without measures. Annals of probability, 2001. [33] M. Talagrand. Upper and Lower Bounds for Stochastic Processes. Springer, 2014. [34] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational
Mathematics, 2012. [35] J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements. arXiv
preprint, 2014. [36] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed sensing,
pages 210-268, 2012. [37] R. Vershynin. Estimation in high dimensions: a geometric perspective. ArXiv e-prints, 2014. [38] A. G. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra and its
Applications, 1992. [39] E. Yang and P. Ravikumar. Dirty statistical models. In NIPS, 2013.
9

