Bandit Smooth Convex Optimization: Improving the Bias-Variance Tradeoff

Ofer Dekel Microsoft Research
Redmond, WA oferd@microsoft.com

Ronen Eldan Weizmann Institute
Rehovot, Israel roneneldan@gmail.com

Tomer Koren Technion
Haifa, Israel tomerk@technion.ac.il

Abstract
Bandit convex optimization is one of the fundamental problems in the field of online learning. The best algorithm for the general bandit convex optimization problem guarantees a regret of Oe(T 5/6), while the best known lower bound is (T 1/2). Many attempts have been made to bridge the huge gap between these bounds. A particularly interesting special case of this problem assumes that the loss functions are smooth. In this case, the best known algorithm guarantees a regret of Oe(T 2/3). We present an efficient algorithm for the bandit smooth convex optimization problem that guarantees a regret of Oe(T 5/8). Our result rules out an (T 2/3) lower bound and takes a significant step towards the resolution of this open problem.

1 Introduction
Bandit convex optimization [11, 5] is the following online learning problem. First, an adversary privately chooses a sequence of bounded and convex loss functions f1, . . . , fT defined over a convex domain K in d-dimensional Euclidean space. Then, a randomized decision maker iteratively chooses a sequence of points x1, . . . , xT , where each xt 2 K. On iteration t, after choosing the point xt, the decision maker incurs a loss of ft(xt) and receives bandit feedback: he observes the value of his loss but he does not receive any other information about the function ft. The decision maker uses the feedback to make better choices on subsequent rounds. His goal is to minimize regret, which is the difference between his loss and the loss incurred by the best fixed point in K. If the regret grows sublinearly with T , it indicates that the decision maker's performance improves as the length of the sequence increases, and therefore we say that he is learning. Finding an optimal algorithm for bandit convex optimization is an elusive open problem. The first algorithm for this problem was presented in Flaxman et al. [11] and guarantees a regret of R(T ) = Oe(T 5/6) for any sequence of loss functions (here and throughout, the asymptotic Oe notation hides a polynomial dependence on the dimension d as well as logarithmic factors). Despite the ongoing effort to improve on this rate, it remains the state of the art. On the other hand, Dani et al. [9] proves that for any algorithm there exists a worst-case sequence of loss functions for which R(T ) = (T 1/2), and the gap between the upper and lower bounds is huge. While no progress has been made on the general form of the problem, some progress has been made in interesting special cases. Specifically, if the bounded convex loss functions are also assumed to be Lipschitz, Flaxman et al. [11] improves their regret guarantee to R(T ) = Oe(T 3/4). If the loss functions are smooth (namely, their gradients are Lipschitz), Saha and Tewari [15] present an algorithm with a guaranteed regret of Oe(T 2/3). Similarly, if the loss functions are bounded, Lipschitz, and strongly convex, the guaranteed regret is Oe(T 2/3) [3]. If even stronger assumptions are made, an optimal regret rate of e (T 1/2) can be guaranteed; namely, when the loss functions are both smooth
1

and strongly-convex [12], when they are Lipschitz and linear [2], and when Lipschitz loss functions are not generated adversarially but drawn i.i.d. from a fixed and unknown distribution [4]. Recently, Bubeck et al. [8] made progress that did not rely on additional assumptions, such as Lipschitz, smoothness, or strong convexity, but instead considered the general problem in the onedimensional case. That result proves that there exists an algorithm with optimal e (T 1/2) regret for arbitrary univariate convex functions ft : [0, 1] 7! [0, 1]. Subsequently, and after the current paper was written, Bubeck and Eldan [7] generalized this result to bandit convex optimization in general Euclidean spaces (albeit requiring a Lipschitz assumption). However, the proofs in both papers are non-constructive and do not give any hint on how to construct a concrete algorithm, nor any indication that an efficient algorithm exists. The current state of the bandit convex optimization problem has given rise to two competing conjectures. Some believe that there exists an efficient algorithm that matches the current lower bound. Meanwhile, others are trying to prove larger lower bounds, in the spirit of [10], even under the assumption that the loss functions are smooth; if the (T 1/2) lower bound is loose, a natural guess of the true regret rate would be e (T 2/3).1 In this paper, we take an important step towards the resolution of this problem by presenting an algorithm that guarantees a regret of e (T 5/8) against any sequence of bounded, convex, smooth loss functions. Compare this result to the previous stateof-the-art result of e (T 2/3) (noting that 2/3 = 0.666... and 5/8 = 0.625). This result rules out the possibility of proving a lower bound of (T 2/3) with smooth functions. While there remains a sizable gap with the T 1/2 lower bound, our result brings us closer to finding the elusive optimal algorithm for bandit convex optimization, at least in the case of smooth functions. Our algorithm is a variation on the algorithms presented in [11, 1, 15], with one new idea. These algorithms all follow the same template: on each round, the algorithm computes an estimate of rft(xt), the gradient of the current loss function at the current point, by applying a random perturbation to xt. The sequence of gradient estimates is then plugged into a first-order online optimization technique. The technical challenge in the analysis of these algorithms is to bound the bias and the variance of these gradient estimates. Our idea is take a window of consecutive gradient estimates and average them, producing a new gradient estimate with lower variance and higher bias. Overall, the new bias-variance tradeoff works in our favor and allows us to improve the regret upper-bound. Averaging uncorrelated random vectors to reduce variance is a well-known technique, but applying it in the context of bandit convex optimization algorithm is easier said than done and requires us to overcome a number of technical difficulties. For example, the gradient estimates in our window are taken at different points, which introduces a new type of bias. Another example is the difficulty that arrises when the sequence xs, . . . , xt travels adjacent to the boundary of the convex set K (imagine transitioning from one face of a hypercube to another); the random perturbation applied to xs and xt could be supported on orthogonal directions, yet we average the resulting gradient estimates and expect to get a meaningful low-variance gradient estimate. While the basic idea is simple, our non-trivial technical analysis is not, and may be of independent interest.

2 Preliminaries

We begin by defining smooth bandit convex optimization more formally, and recalling several basic results from previous work on the problem (Flaxman et al. [11], Abernethy et al. [2], Saha and Tewari [15]) that we use in our analysis. We also review the necessary background on self-concordant barrier functions.

2.1 Smooth Bandit Convex Optimization

In the bandit convex optimization problem, an adversary first chooses a sequence of convex functions

f1, . . . , fT : K 7! [0, 1], where K is a closed and convex domain in Rd. Then, on each round

t = 1, . . . , T , a randomized decision maker has to choose a point xt 2 K, and after committing

to his decision he incurs a loss of ft(xt), expected loss (where expectation is taken

and with

observes this loss as feedback. respect to his random choices)

iTs hEe[PdeTtc=is1ifotn(xmta)]kearn'ds

1In fact, we are aware of at least two separate research groups that invested time trying to prove such an (T 2/3) lower bound.

2

his regret is

"XT

#

XT

R(T ) = E ft(xt)
t=1

min
x2K

ft(x) .

t=1

Throughout, we use the notation Et[*] to indicate expectations conditioned on all randomness up to

and including round t 1.

We make the following assumptions. First, we assume that each of the functions f1, . . . , fT is LLipschitz with respect to the Euclidean norm k*k2, namely that |ft(x) ft(y)|  Lkx yk2 for all x, y 2 K. We further assume that ft is H-smooth with respect to k*k2, which is to say that
8 x, y 2 K , krft(x) rft(y)k2  H kx yk2 . In particular, this implies that ft is continuously differentiable over K. Finally, we assume that the Euclidean diameter of the decision domain K is bounded by D > 0.

2.2 First Order Algorithms with Estimated Gradients

The online convex optimization problem becomes much easier in the full information setting, where

the decision maker's feedback includes the vector gt = rft(xt), the gradient (or subgradient) of ft at the point xt. In this setting, the decision maker can use a first-order online algorithm, such as the projected online gradient descent algorithm [17] or dual averaging [13] (sometimes known as

follow the regularized leader [16]), and guarantee a regret of O(T 1/2). The dual averaging approach

sets

xt

to

be

the

solution

to

the

following

optimization ( Xt 1

problem,

)

xt = arg min x * s,tgs + R(x) ,

(1)

x2K

s=1

where R is a suitably chosen regularizer, and for all t = 1, . . . , T and s = 1, . . . , t we define a

non-negative weight s,t. Typically, all of the weights (s,t) are set to a constant value , called the learning rate parameter.

However, since we are not in the full information setting and the decision maker does not observe gt, the algorithms mentioned above cannot be used directly. The key observation of Flaxman et al. [11],

which is later reused in all of the follow-up work, is that gt can be estimated by randomly perturbing the point xt. Specifically, on round t, the algorithm chooses the point

yt = xt + Atut ,

(2)

instead of the original point xt, where > 0 is a parameter that controls the magnitude of the perturbation, At is a positive definite d  d matrix, and ut is drawn from the uniform distribution on the unit sphere. In Flaxman et al. [11], At is simply set to the identity matrix whereas in Saha and Tewari [15], At is more carefully tailored to the point xt (see details below). In any case, care should be taken to ensure that the perturbed point yt remains in the convex set K. The observed value ft(yt) is then used to compute the gradient estimate

gt = d ft(yt)At 1ut ,

(3)

and this estimate is timator of rft(xt),

fed to the first-order optimization it is an unbiased estimator for the

algorithm. gradient of

Wa dhiiflfeergetntisfunnocttiaonnu, nfbti,adseefidneesd-

by

ft(x) = Et[ft(x + Atv)] ,

(4)

where v 2 Rd is uniformly drawn from the unit ball. The function ft(x) is a smoothed version of

fptro, wpehritcyhopflfaytsisa

key role in our summarized in

analysis and in many of the following lemma.

the

previous

results

on

this

topic.

The

main

Lemma 1 (Flaxman et al. [11], Saha and Tewari [15, Lemma 5]). For any differentiable function

f : Rd 7! R, positive definite matrix A, x 2 Rd, and 2 (0, 1], define g = (d/ )f (x+ Au)*A 1u, where u is uniform on the unit sphere. Also, let f(x) = E[f (x + Av)] where v is uniform on the

unit ball. Then E[g] = rf(x).

The difference between rft(xt) and rft(xt) is the bias of the gradient estimator gt. The analysis in Flaxman et al. [11], Abernethy et al. [2], Saha and Tewari [15] focuses on bounding the bias and the variance of gt and their effect on the first-order optimization algorithm.

3

2.3 Self-Concordant Barriers

Following [2, 1, 15], our algorithm and analysis rely on the properties of self-concordant barrier functions. Intuitively, a barrier is a function defined on the interior of the convex body K, which is rather flat in most of the interior of K and explodes to 1 as we approach its boundary. Additionally, a self-concordant barrier has some technical properties that are useful in our setting. Before giving the formal definition of a self-concordant barrier, we define the local norm defined by a self-concordant barrier. Definition 2 (Local Norm Induced by a Self-Concordant Barrier [14]). Let R : int(K) 7! R be a self-concordant barrier.pThe local norm induced by R at the poinpt x 2 int(K) is denoted by kzkx and defined as kzkx = zTr2R(x)z. Its dual norm is kzkx, = zT(r2R(x)) 1z.
In words, the local norm at x is the Mahalanobis norm defined by the Hessian of R at the point x, namely, r2R(x). We now give a formal definition of a self-concordant barrier. Definition 3 (Self-Concordant Barrier [14]). Let K  Rd be a convex body. A function R : int(K) 7! R is a #-self-concordant barrier for K if (i) R is three times continuously differentiable, (ii) R(x) ! 1 as x ! @K, and (iii) for all x 2 int(K) and y 2 Rd, R satisfies
p |r3R(x)[y, y, y]|  2kyk3x and |rR(x) * y|  #kykx .
This definition is given for completeness, and is not directly used in our analysis. Instead, we rely on some useful properties of self-concordant barriers. First and foremost, there exists a O(d)-selfconcordant barrier for any convex body [14, 6]. Efficiently-computable self-concordant barriers are only known for specific classes of convex bodies, such as polytopes, yet we make the standard assumption that we have an efficiently computable #-self-concordant barrier for the set K.
Another key feature of a self-concordant barrier is the set of Dikin ellipsoids that it defines. The Dikin ellipsoid at x 2 int(K) is simply the unit ball with respect to the local norm at x. A key feature of the Dikin ellipsoid is that it is entirely contained in the convex body K, for any x [see 14, Theorem 2.1.1]. Another technical property of a self-concordant barriers is that its Hessian changes slowly with respect to its local norm. Theorem 4 (Nesterov and Nemirovskii [14, Theorem 2.1.1]). Let K be a convex body with selfconcordant barrier R. For any x 2 int(K) and z 2 Rd such that kzkx < 1, it holds that
(1 kzkx)2 r2R(x) r2R(x + z) (1 kzkx) 2 r2R(x) .

While the self-concordant barrier explodes to infinity at the boundary of K, it is quite flat at points

that are far from the boundary. To make this statement formal, we define an operation that mul-

tiplicatively shrinks the set K toward the minimizer of R (called the analytic center of K). Let

y = arg min R(x) and assume without loss of generality that R(y) = 0. For any  2 (0, 1) let Ky, denote the set {y + (1 )(x y) : x 2 K}. The next theorem states that the barrier is flat in Ky, and explodes to 1 in the thin shell between Ky, and K.

Theorem 5 (Nesterov and Nemirovskii [14, Propositions 2.3.2-3]). Let K be a convex body with

#-self-concordant barrier R, let y = arg min R(x), and assume that R(y) = 0. For any  2 (0, 1],

it holds that

8 x 2 Ky,

R(x)



#

log

1 

.

Our assumptions on the loss functions, as the Lipschitz assumption or the smoothness assumption, are stated in terms of the standard Euclidean norm (which we denote by k * k2). Therefore, we will need to relate the Euclidean norm to the local norms defined by the self-concordant barrier. This is accomplished by the following lemma (whose proof appears in the supplementary material). Lemma 6. Let K be a convex body with self-concordant barrier R and let D be the (Euclidean) diameter of K. For any x 2 K, it holds that D 1kzkx,  kzk2  Dkzkx for all z 2 Rd.

2.4 Self-Concordant Barrier as a Regularizer Looking back at the dual averaging strategy defined in Eq. (1), we can now fill in some of the details that were left unspecified: [1, 15] set the regularization R in Eq. (1) to be a #-self-concordant barrier for the set K. We use the following useful lemma from Abernethy and Rakhlin [1] in our analysis.

4

Algorithm 1: Bandit Smooth Convex Optimization

Parameters: perturbation parameter barrier R : int(K) 7! R Initialize: y1 2 K arbitrarily for t = 1, . . . , T

2 (0, 1], dual averaging weights (s,t), self-concordant

At (r2R(xt)) 1/2

draw ut uniformly from the unit sphere

yt xt + Atut

choose yt, receive feedback ft(yt)

gt xt+1

(d/ar)gfmt(iyntx)2*KA{tx1u* tPts=1 s,tgs + R(x)}

Lemma 7 (Abernethy and Rakhlin [1]). Let K be a convex body with #-self-concordant barrier

R, xt

let g1, . . . , gT = arg minx2K

be {x

*vPecttso=r11s

in Rd, and let  > gs + R(x)}. Then,

0 be such that kgtkxt,



1 4

for all t.

Define

(i) (ii)

for for

all t it holds any x? 2 K

itthahtolkdxstthaxt Pt+1Tt=kx1tgt *

2kgtkxt,; (xt x?) 

1 

R(x?)

+

2

PT
t=1

kgtk2xt,.

Algorithms for bandit convex optimization that use a self-concordant regularizer also use the same self-concordant barrier to obtain gradient estimates. Namely, these algorithms perturb the dual averaging solution xt as in Eq. (3), with the perturbation matrix At set to (r2R(xt)) 1/2, the root of the inverse Hessian of R at the point xt. In other words, the distribution of yt is supported on the Dikin ellipsoid centered at xt, scaled by . Since 2 (0, 1], this form of perturbation guarantees that yt 2 K. Moreover, if yt is generated in this way and used to construct the gradient estimator gt, then the local norm of gt is bounded, as specified in the following lemma.
Lemma 8 (Saha and Tewari [15, Lemma 5]). Let K  Rd be a convex body with self-concordant barrier R. For any differentiable function f : K 7! [0, 1], 2 (0, 1], and x 2 int(K), define g = (d/ )f (y) * A 1u, where A = (r2R(x)) 1/2, y = x + Au, and u is drawn uniformly from the unit sphere. Then kgkx  d/ .

3 Main Result

Our algorithm for the bandit smooth convex optimization problem is a variant of the algorithm in Saha and Tewari [15], and appears in Algorithm 1. Following Abernethy and Rakhlin [1], Saha and Tewari [15], we use a self-concordant function as the dual averaging regularizer and we use its Dikin ellipsoids to perturb the points xt. The difference between our algorithm and previous ones is the introduction of dual averaging weights (s,t), for t = 1, . . . , T and s = 1, . . . , t, which allow us to vary the weight of each gradient in the dual averaging objective function.

In addition to the parameters , , and , we introduce a new buffering parameter k, which takes

non-negative integer values. We set the dual averaging weights in Algorithm 1 to be

(

s,t =

 if s  t k

t

s+1 k+1



if s > t

k,

(5)

where  > 0 is a global learning rate parameter. This choice of (s,t) effectively decreases the influence of the feedback received on the most recent k rounds. If k = 0, all of the (s,t) become equal to  and Algorithm 1 reduces to the algorithm in Saha and Tewari [15]. The surprising result is that there exists a different setting of k > 0 that gives a better regret bound.

We introduce a slight abuse of notation, which helps us simplify the presentation of our regret bound. We will eventually achieve the desired regret bound by setting the parameters , , and k to be some functions of T . Therefore, from now on, we treat the notation , , and k as an abbreviation for the functional forms (T ), (T ), and k(T ) respectively. The benefit is that we can now use asymptotic notation (e.g., O(k)) to sweep meaningless low-order terms under the rug.

5

We prove the following regret bound for this algorithm.

Theorem 9. Let f1, . . . , fT be a sequence of loss functions where each ft : K 7! [0, 1] is dif-

ferentiable, convex, H-smooth and L-Lipschitz, and where K  Rd is a convex body of diameter

D > 0 with #-self-concordant barrier R. For any ,  2 (0, 1] and k 2 {0, 1, . . . , T } assume that

Algorithm 1 is run with these parameters and with the weights defined in Eq. (5) (using k and ) to

generate the sequences x1, . . . , xT and y1, . . . , yT . If 12kd  and for any  2 (0, 1) it holds that

R(T )



HD2

2T

+

#

log

1 



+

64d2T 2(k + 1)

+

p 12(HD2 + DL)d kT

 +O T

 + T k

.

SthpaetcRifi(cTal)ly=, ifOw(epsdeTt

= d1/4T 5/8 log T ).

3/16,  = d

1/2T

5/8, k = d1/2T 1/8, and  = T

100, we get

Note that if we set k = 0 in our theorem, we recover the Oe(T 2/3) bound in Saha and Tewari [15] up to a small numerical constant (namely, the dependence on L, H, D, #, d, and T is the same).

4 Analysis

UEresgiPrnegtTta=ths1efnt(oytat)tion

fxt?(x=?)a.rgFmollionwx2inKgPFlTta=x1mfatn(xe)t,

the decision maker's regret becomes R(T ) = al. [11], Saha and Tewari [15], we rewrite the

" #"

#"

#

XT

R(T ) = E

ft(yt)

ft(xt) + E XT ft(xt)

ft(x?) + E XT ft(x?)

ft(x?) . (6)

| t=1

{z
a

} | t=1

{z
b

} | t=1

{z
c

}

This decomposition essentially adds a layer of hallucination to the analysis: we pretend that the loss functions are f1, . . . , fT instead of f1, . . . , fT and we also pretend that we chose the points x1, . . . , xT rather than y1, . . . , yT . We then analyze the regret in this pretend world (this regret is the expression in Eq. (6b)). Finally, we tie our analysis back to the real world by bounding the difference between that which we analyzed and the regret of the actual problem (this difference is the sum of Eq. (6a) and Eq. (6c)). The advantage of our pretend world over the real world is that we have unbiased gradient estimates g1, . . . , gT that can plug into the dual averaging algorithm.

The algorithm in Saha and Tewari [15] sets all of the dual averaging weights (s,t) equal to the constant learning rate  > 0. It decomposes the regret as in Eq. (6) and their main technical result is the following bound for the individual terms:

Theorem 10 (Saha and Tewari [15]). Let f1, . . . , fT be a sequence of loss functions where each ft : K 7! [0, 1] is differentiable, convex, H-smooth and L-Lipschitz, and where K  Rd is a convex body of diameter D > 0 and #-self-concordant barrier R. Assume that Algorithm 1 is run with perturbation parameter 2 (0, 1] and generates the sequences x1, . . . , xT and y1, . . . , yT . Then for any  2 (0, 1) it holds that (6a) + (6c)  (HD2 2 + L)T . If, additionally, the dual averaging weights (s,t) are all set to the constant learning rate  then (6b)  # log(1/) 1 + d2 2T .

The analysis in Saha and Tewari [15] goes on to obtain a regret bound of Oe(T 2/3) by choosing optimal values for the parameters , and  and plugging those values into Theorem 10. Our analysis uses the first part of Theorem 10 to bound (6a) + (6c) and shows that our careful choice of the dual averaging weights (s,t) results in the following improved bound on (6b). We begin our analysis by defining a moving average of the functions f1, . . . , fT , as follows:

8 t = 1, . . . , T ,

ft(x)

=

k

1 +

1

Xk

ft

i(x)

,

i=0

(7)

where, for soundness, we let fs  0 for s  0. Also, define a moving average of gradient estimates:

8 t = 1, . . . , T ,

gt

=

k

1 +

1

Xk

gt

i

,

i=0

6

aetdhgsuataaitimlnPa,avtwtsee=rioat1hfgirgnsfsg,ttgw=(sxitt=0h).fthoAerPlssgotsra=nd1oi0tege.nstthIfenoasrtStitaemhlcelatticto.ehnsTogih4c1ee,bre.oef.lfoo.trw,hege,,TwdthuueeanlsliahfasoovtrwemsrtaehlgypoiwnwingeeAwiagclehghitgoeghrditttsbhcy(mans1.,btbe)aiusnisceEadqll.ay(s5pa)eirbsfiosarusmcehds

We

use

the functions "XT

ft

to

rewrite #

Eq. (6b) as "XT

E ft(xt) ft(xt) + E ft(xt)

# "XT

ft(x?) + E

ft(x?)

# ft(x?) .

(8)

| t=1 {z

} | t=1 {z

} | t=1 {z

}

ab c

This decomposition essentially adds yet another layer of hallucination to the analysis: we pretend

that the loss functions are f1, . . . , fT instead of f1, . . . , fT (which are themselves pretend loss functions, as described above). Eq. (8b) is the regret in our new pretend scenario, while Eq. (8a) +

Eq. (8c) is the difference between this regret and the regret in Eq. (6b).

The following lemma bounds each of the terms in Eq. (8) separately, and summarizes the main technical contribution of our paper.

Lemma 11. Under the conditions of Theorem 9, for any  2 (0, 1) it holds that (8c)  0 , p
(8a)  12DLd kT + O (1 + )k , and

(8b)



#

log

1 



+

64d2T 2(k + 1)

+

p 12HD2d kT

 + O T  + T k .

4.1 Proof Sketch of Lemma 11

As mentioned above, the basic intuition of our technique is quite simple: average the gradients to decrease their variance. Yet, applying this idea in the analysis is tricky. We begin by describing the main source of difficulty in proving Lemma 11.

Recall that our strategy is to pretend that the vector gt as a biased estimator of rft(xt). is small. Recall that each gs is an unbiased

loss functions are f1, Naturally, one of our estimator of rfs(xs)

. . . , fT and to use the random goals is to show that this bias (conditioned on the history up

tatrohtferatocuduinir(frdxfeetnrt)et),.npptoSrpoipnvoetiic,dnixetfi.dtc.aYthlLelayut,t,cwwnkoieeltyesa,hvtfohetwraatigsteheHaactthh-xsemtvseeoicovtaotenhrcd,tiosnxrostthraaernfedsteccqliolu(asxeiemnt tcoeit)hegaastthchotkhuo,elt.ydh.ean.rc,ocgitnutbrEeiasutmecalluyigdcreheasadtdniimiefdnfaietstreteeasnrntticfmteht.aaatnet

To show that xt i and xt are close, we exploit the stability of the dual averaging algorithm. Particu-

larly, the first claim in Lemma

we need to at different

sphooiwntsth; aetakcghsgktxs

, i

7issstmateasll.thHatowkxesver,xgst+i1sktxhse is designed to have a small

is controlled average of k

b+y1kggrsakdxise,ntfeosrtiamllast,essotankoewn

norm with respect to its own local norm

kno*wkxwt ein,e;efdortoasllhwowe

know, it may that the local

be very large norms at xt i

with and

respect to the xt are similar.

current local norm k * We could prove this if

kwxet,k.neSwo

that xt i and xt are close to each other--which is exactly what we set out to prove in the beginning.

This chicken-and-egg situation complicates our analysis considerably.

Another non-trivial component of our proof is the variance reduction analysis. The motivation to average gt k, . . . , gt is to generate new gradient estimates with a smaller variance. While the random vectors g1, . . . , gT are not independent, we show that their randomness is uncorrelated. Therefore, the variance of gt is k + 1 times smaller than the variance of each gt. However, to make this argument formal, we again require the local norms at xt i and xt to be similar. To make things more complicated, there is the recurring need to move back and forth between local norms and the Euclidean norm, since the latter is used in the definition of Lipschitz and smoothness. All of this has to do with bounding Eq. (8b), the regret with respect to the pretend loss functions f1, . . . , fT - an additional bias term appears in the analysis of Eq. (8a). We conclude the paper by stating our main lemmas and sketching the proof Lemma 11. The full technical proofs are all deferred to the supplementary material and replaced with some high level commentary.

7

To break the chicken-and-egg situation described above, we begin with a crude which does not benefit at all from the averaging operation. We simultaneously

bound on prove that

kthgetklxotc,al,

norms at xt i and xt are similar.

Lemma 12. If the parameters k, , and are chosen such that 12kd  then for all t,

(i) kgtkxt,  2d/ ;

(ii) for any 0  i  k such that t

i

1

it

holds

that

1 2

kz

kxt

i,  kzkxt,  2kzkxt

i,.

Lemma 12 itself has a chicken-and-egg aspect, which we resolve using an inductive proof technique.

Armed with more refined

tbhoeuknndoownleEdtgekgthtkat2xtt,he,lowchailcnhodromess

at xt i benefit

and xt are similar, from averaging.

we

go

on

to

prove

the

Lemma 13. If the parameters k, , and are chosen such that 12kd  then

Et

 kgt

k2xt

,





2D2L2 +

32d2 2(k + 1)

.

The proof constructs a martingale difference sequence and uses the fact that its increments are un-

correlated. extra k + 1

Compare the above to Lemma 8, which proves that kgt in our denominator--all of our hard work was aimed at

giektt2xitngi

,  this

d2/ 2 factor.

and

note

the

Next, we set out to bound the expected Euclidean distance between xt i and xt. This bound is later

needed to exploit the L-Lipschitz

Lemma 12 is enough to satisfy the

is controlled the resulting

bboyuEnd[kogvsekrxsti,m]e. ,Twhee

calaontndtedHritei-onsnmjosyoosofttLhheeamsismmupmar7po,tviweodnhsibc.ohTuthnhedencdrtuueeldletsoubLsoeuthmnadmt Eoan[k1kx3g.ssIknxxtses,g+r1afkrtioxnmsg] obtain the following lemma.

Lemma 14. If the parameters k, , and are chosen such that 12kd  0  i  k such that t i 1, we have
p E[kxt i xtk2]  12Dd k + O(k) .

then for all t and any

Notice that xt i and xt may be k rounds apart, but the bound scales only with pk. Again, this is the work of the averaging technique.

Finally, we have all the tools in place to prove our main result, Lemma 11.

Proof sketch. then proving Lipschitz and

The first term, Eq. that ft i(xt) is not from Lemma 14. To

(bv8oearu)y,ndifsatrbhoefrusonemdcoedfndtb(tyxetrr)me.w, TrEihtqii.ns(g8fobfl)tl,(owxwtes)uf=sreomtkh+1et1hcPeonfkiva=ec0xt iftthytaotif(fxetat)cishaLnfdt-

to write

E "XT ft(xt)

# ft(x?)



E "XT rft(xt) * (xt

# x?) .

t=1
We relate the right-hand side above to

t=1

"XT

#

E gt * (xt x?) ,

t=1

using the fact that ft is H-smooth and again using Lemma 14. Then, we upper bound the above

using Lemma 7, Theorem 5, and Lemma 13.



Acknowledgments We thank Jian Ding for several critical contributions during the early stages of this research. Parts of this work were done while the second and third authors were at Microsoft Research, the support of which is gratefully acknowledged.

8

References

[1] J. Abernethy and A. Rakhlin. Beating the adaptive bandit with high probability. In Information Theory and Applications Workshop, 2009, pages 280-289. IEEE, 2009.

[2] J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), 2008.

[3] A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), 2010.

[4] A. Agarwal, D. P. Foster, D. Hsu, S. M. Kakade, and A. Rakhlin. Stochastic convex optimization with bandit feedback. In Advances in Neural Information Processing Systems (NIPS), 2011.

[5] S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.

[6] S. Bubeck and R. Eldan. The entropic barrier: a simple and optimal universal self-concordant barrier. arXiv preprint arXiv:1412.1587, 2015.

[7] S. Bubeck and R. Eldan. Multi-scale exploration of convex functions and bandit convex opti-

mization. arXiv preprint arXiv:1507.06580, 2015.

[8]

S. Bubeck, O. Dekel, T. Koren, and Y. Peres.

Bandit convex optimization:

p T

regret in one

dimension. In In Proceedings of the 28st Annual Conference on Learning Theory (COLT),

2015.

[9] V. Dani, T. Hayes, and S. M. Kakade. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems (NIPS), 2008.

[10] O. Dekel, J. Ding, T. Koren, and Y. Peres. Bandits with switching costs: T 2/3 regret. In Proceedings of the 46th Annual Symposium on the Theory of Computing, 2014.

[11] A. D. Flaxman, A. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACMSIAM symposium on Discrete algorithms, pages 385-394. Society for Industrial and Applied Mathematics, 2005.

[12] E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Advances in Neural Information Processing Systems (NIPS), 2014.

[13] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming, 120(1):221-259, 2009.

[14] Y. Nesterov and A. Nemirovskii. Interior-point polynomial algorithms in convex programming, volume 13. SIAM, 1994.

[15] A. Saha and A. Tewari. Improved regret guarantees for online smooth convex optimization with bandit feedback. In International Conference on Artificial Intelligence and Statistics (AISTAT), pages 636-642, 2011.

[16] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107-194, 2011.

[17] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML'03), pages 928-936, 2003.

9

