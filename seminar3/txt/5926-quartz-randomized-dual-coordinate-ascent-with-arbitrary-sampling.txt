Quartz: Randomized Dual Coordinate Ascent with Arbitrary Sampling

Zheng Qu Department of Mathematics The University of Hong Kong
Hong Kong zhengqu@maths.hku.hk

Peter Richtarik School of Mathematics The University of Edinburgh EH9 3FD, United Kingdom peter.richtarik@ed.ac.uk

Tong Zhang Department of Statistics
Rutgers University Piscataway, NJ, 08854 tzhang@stat.rutgers.edu

Abstract
We study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer. We propose and analyze a novel primal-dual method (Quartz) which at every iteration samples and updates a random subset of the dual variables, chosen according to an arbitrary distribution. In contrast to typical analysis, we directly bound the decrease of the primal-dual error (in expectation), without the need to first analyze the dual error. Depending on the choice of the sampling, we obtain efficient serial and mini-batch variants of the method. In the serial case, our bounds match the best known bounds for SDCA (both with uniform and importance sampling). With standard mini-batching, our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data.

Keywords: empirical risk minimization, dual coordinate ascent, arbitrary sampling, data-driven speedup.

1 Introduction

In this paper we consider a primal-dual pair of structured convex optimization problems which has in several variants of varying degrees of generality attracted a lot of attention in the past few years in the machine learning and optimization communities [4, 22, 20, 23, 21, 27].

Let A1, . . . , An be a collection of d-by-m real matrices and 1, . . . , n be 1/-smooth convex functions from Rm to R, where  > 0. Further, let g : Rd  R  {+} be a 1-strongly convex function and  > 0 a regularization parameter. We are interested in solving the following primal

problem:

minw=(w1,...,wd)Rd

P (w)

d=ef

1 n

n i=1

i(Ai

w)

+

g(w)

.

(1)

In the machine learning context, matrices {Ai} are interpreted as examples/samples, w is a (linear) predictor, function i is the loss incurred by the predictor on example Ai, g is a regularizer,  is a regularization parameter and (1) is the regularized empirical risk minimization problem. In

1

this paper we are especially interested in problems where n is very big (millions, billions), and much larger than d. This is often the case in big data applications. Stochastic Gradient Descent (SGD) [18, 11, 25] was designed for solving this type of large-scale optimization problems. In each iteration SGD computes the gradient of one single randomly chosen function i and approximates the gradient using this unbiased but noisy estimation. Because of the variance of the stochastic estimation, SGD has slow convergence rate O(1/ ). Recently, many methods achieving fast (linear) convergence rate O(log(1/ )) have been proposed, including SAG [19], SVRG [6], S2GD [8], SAGA [1], mS2GD [7] and MISO [10], all using different techniques to reduce the variance.

Another approach, such as Stochastic Dual Coordinate Ascent (SDCA) [22], solves (1) by considering its dual problem that is defined as follows. For each i, let i : Rm  R be the convex conjugate of i, namely, i (u) = maxsRm s u - i(s) and similarly let g : Rd  R be the convex conjugate of g. The dual problem of (1) is defined as:

max D() d=ef -f () - () ,
=(1,...,n)RN =Rnm

(2)

where  = (1, . . . , n)  RN = Rnm is obtained by stacking dual variables (blocks) i  Rm, i = 1, . . . , n, on top of each other and functions f and  are defined by

f () d=ef g

1 n

n i=1

Aii

;

()

d=ef

1 n

n i=1

i (-i).

(3)

SDCA [22] and its proximal extension Prox-SDCA [20] first solve the dual problem (2) by updating uniformly at random one dual variable at each round and then recover the primal solution by setting w = g(). Let Li = max(Ai Ai). It is known that if we run SDCA for at least

O

n

+

maxi Li 

log

n

+

maxi Li 

1

iterations, then SDCA finds a pair (w, ) such that E[P (w) - D()]  . By applying accelerated

randomized coordinate descent on the dual problem, APCG [9] needs at most O(n +

maxi 

Li

)

number of iterations to get -accuracy. ASDCA [21] and SPDC [26] are also accelerated and ran-

domized primal-dual methods. Moreover, they can update a mini-batch of dual variables in each

round.

We propose a new algorithm (Algorithm 1), which we call Quartz, for simultaneously solving the primal (1) and dual (2) problems. On the dual side, at each iteration our method selects and updates a random subset (sampling) S  {1, . . . , n} of the dual variables/blocks. We assume that these sets are i.i.d. throughout the iterations. However, we do not impose any additional assumptions on the distribution of S apart from the necessary requirement that each block i needs to be chosen with a positive probability: pi d=ef P(i  S) > 0. Quartz is the first SDCA-like method analyzed for an arbitrary sampling. The dual updates are then used to perform an update to the primal variable w and the process is repeated. Our primal updates are different (less aggressive) from those used in SDCA [22] and Prox-SDCA [20], thanks to which the decrease in the primal-dual error can be bounded directly without first establishing the dual convergence as in [20], [23] and [9]. Our analysis is novel and directly primal-dual in nature. As a result, our proof is more direct, and the logarithmic term in our bound has a simpler form.

Main result. We prove that starting from an initial pair (w0, 0), Quartz finds a pair (w, ) for which P (w) - D()  (in expectation) in at most

max
i

1 pi

+

vi pi  n

log

P (w0)-D(0)

(4)

iterations. The parameters v1, . . . , vn are assumed to satisfy the following ESO (expected separable overapproximation) inequality:

ES

iS Aihi 2 

n i=1

pivi

hi

2,

(5)

where * denotes the standard Euclidean norm. Moreover, the parameters v1, . . . , vn are needed to run the method (they determine stepsizes), and hence it is critical that they can be cheaply computed

2

before the method starts. We wish to point out that (5) always holds for some parameters {vi}. Indeed, the left hand side is a quadratic function of h and hence the inequality holds for largeenough vi. Having said that, the size of these parameters directly influences the complexity, and hence one would want to obtain as tight bounds as possible. As we will show, for many samplings of interest small enough parameter v can be obtained in time required to read the data {Ai}. In particular, if the data matrix A = (A1, . . . , An) is sufficiently sparse, our iteration complexity result (4) specialized to the case of standard mini-batching can be better than that of accelerated methods such as ASDCA [21] and SPDC [26] even when the condition number maxi Li/ is larger than n, see Proposition 4 and Figure 2.
As described above, Quartz uses an arbitrary sampling for picking the dual variables to be updated in each iteration. To the best of our knowledge, only two papers exist in the literature where a stochastic method using an arbitrary sampling was analyzed: NSync [16] for unconstrained minimization of a strongly convex function and ALPHA [15] for composite minimization of non-strongly convex function. Assumption (5) was for the first time introduced in [16]. However, NSync is not a primal-dual method. Besides NSync, the closest works to ours in terms of the generality of the sampling are PCDM [17], SPCDM [3] and APPROX [2]. All these are randomized coordinate descent methods, and all were analyzed for arbitrary uniform samplings (i.e., samplings satisfying P(i  S) = P(i  S) for all i, i  {1, . . . , n}). Again, none of these methods were analyzed in a primal-dual framework.
In Section 2 we describe the algorithm, show that it admits a natural interpretation in terms of Fenchel duality and discuss the flexibility of Quartz. We then proceed to Section 3 where we state the main result, specialize it to the samplings discussed in Section 2, and give detailed comparison of our results with existing results for related primal-dual stochastic methods in the literature. In Section 4 we demonstrate how Quartz compares to other related methods through numerical experiments.

2 The Quartz Algorithm

Throughout the paper we consider the standard Euclidean norm, denoted by * . A function  :

Rm  R is (1/)-smooth if it is differentiable and has Lipschitz continuous gradient with Lispchitz

constant 1/:

(x) - (y)



1 

x - y , for all x, y  Rm. A function g : Rd  R  {+}

is 1-strongly convex if g(w)  g(w ) + g(w ), w - w

+

1 2

w-w

2 for all w, w  dom(g),

where dom(g) denotes the domain of g and g(w ) is a subgradient of g at w .

The most important parameter of Quartz is a random sampling S, which is a random subset of [n] = {1, 2, . . . , n}. The only assumption we make on the sampling S in this paper is the following:

Assumption 1 (Proper sampling) S is a proper sampling; that is, pi d=ef P(i  S) > 0, i  [n].

(6)

This assumption guarantees that each block (dual variable) has a chance to get updated by the
method. Prior to running the algorithm, we compute positive constants v1, . . . , vn satisfying (5) to define the stepsize parameter  used throughout in the algorithm:



=

min
i

pi  n vi + n

.

(7)

Note from (5) that  depends on both the data matrix A and the sampling S. We shall show how to compute in less than two passes over the data the parameter v satisfying (5) for some examples of sampling in Section 2.2.

2.1 Interpretation of Quartz through Fenchel duality

3

Algorithm 1 Quartz

Parameters: proper random sampling S and a positive vector v  Rn

Initialization:

0



RN ;

w0



Rd;

pi

=

P(i



S);



=

min
i

pi  n vi + n

;

0

=

1 n

for t  1 do

wt = (1 - )wt-1 + g(t-1)

t = t-1

Generate a random set St  [n], following the distribution of S

for i  St do it = (1 - p-i 1)it-1 - p-i 1i(Ai wt)

end for

t = t-1 + (n)-1 iSt Ai(it - it-1) end for

Output: wt, t

n i=1

Aii0

Quartz (Algorithm 1) has a natural interpretation in terms of Fenchel duality. Let (w, )  Rd x RN

and

define



=

1 n

n i=1

Aii.

The

duality

gap

for

the

pair

(w,

)

can

be

decomposed

as:

P (w) - D()

(1)=+(2)



(g(w)

+

g

())

+

1 n

n i=1

i

(Ai

w) +

i (-i)

=

(g(w) + g () -

w, 

)

+

1 n

n i=1

i(Ai

w) + i (-i) +

Ai w, i

.

GAPg (w,)

GAPi (w,i)

By Fenchel-Young inequality, GAPg(w, )  0 and GAPi (w, i)  0 for all i, which proves weak duality for the problems (1) and (2), i.e., P (w)  D(). The pair (w, ) is optimal when both
GAPg and GAPi for all i are zero. It is known that this happens precisely when the following optimality conditions hold:

w = g()

(8)

i = -i(Ai w), i  [n].

(9)

We will now interpret the primal and dual steps of Quartz in terms of the above discussion. It is easy to see that Algorithm 1 updates the primal and dual variables as follows:

wt = (1 - )wt-1 + g(t-1)

it =

1 - p-i 1 it-1 + p-i 1 -i(Ai wt) it-1

, i  St , i / St

(10) (11)

where t-1

=

1 n

n i=1

Aiit-1,



is

a

constant

defined

in

(7)

and

St



S

is

a

random

subset

of

[n]. In other words, at iteration t we first set the primal variable wt to be a convex combination of

its current value wt-1 and a value reducing GAPg to zero: see (10). This is followed by adjusting a

subset of dual variables corresponding to a randomly chosen set of examples St such that for each example i  St, the i-th dual variable it is set to be a convex combination of its current value it-1

and a value reducing GAPi to zero, see (11).

2.2 Flexibility of Quartz
Clearly, there are many ways in which the distribution of S can be chosen, leading to numerous variants of Quartz. The convex combination constant  used throughout the algorithm should be tuned according to (7) where v1, . . . , vn are constants satisfying (5). Note that the best possible v is obtained by computing the maximal eigenvalue of the matrix (A A)  P where  denotes the Hadamard (component-wise) product of matrices and P  RNxN is an n-by-n block matrix with all elements in block (i, j) equal to P(i  S, j  S), see [14]. However, the worst-case complexity of computing directly the maximal eigenvalue of (A A)  P amounts to O(N 2), which requires unreasonable preprocessing time in the context of machine learning where N is assumed to be very large. We now describe some examples of sampling S and show how to compute in less than two passes over the data the corresponding constants v1, . . . , vn. More examples including distributed sampling are presented in the supplementary material.

4

Serial sampling. The most studied sampling in the literature on stochastic optimization is the serial sampling, which corresponds to the selection of a single block i  [n]. That is, |S| = 1 with
probability 1. The name "serial" is pointing to the fact that a method using such a sampling will
typically be a serial (as opposed to being parallel) method; updating a single block (dual variable) at
a time. A serial sampling is uniquely characterized by the vector of probabilities p = (p1, . . . , pn), where pi is defined by (6). For serial sampling S, it is easy to see that (5) is satisfied for

vi = Li d=ef max(Ai Ai), i  [n], where max(*) denotes the maximal eigenvalue.

(12)

Standard mini-batching. We now consider S which selects subsets of [n] of cardinality  , uniformly at random. In the terminology established in [17], such S is called  -nice. This sampling satisfies pi = pj for all i, j  [n]; and hence it is uniform. This sampling is well suited for parallel computing. Indeed, Quartz could be implemented as follows. If we have  processors available, then
at the beginning of iteration t we can assign each block (dual variable) in St to a dedicated processor. The processor assigned to i would then compute it and apply the update. If all processors have fast access to the memory where all the data is stored, as is the case in a shared-memory multicore
workstation, then this way of assigning workload to the individual processors does not cause any
major problems. For  -nice sampling, (5) is satisfied for

vi = max(Mi),

Mi =

d j=1

1

+

(j -1)( -1) n-1

AjiAji,

i  [n],

(13)

where for each j  [d], j is the number of nonzero blocks in the j-th row of matrix A, i.e.,

j d=ef |{i  [n] : Aji = 0}|, j  [d]. Note that (13) follows from an extension of a formula given in [2] from m = 1 to m  1.

(14)

3 Main Result

The complexity of our method is given by the following theorem. The proof can be found in the supplementary material.

Theorem 2 (Main Result) Assume that g is 1-strongly convex and that for each i  [n], i is convex

and (1/)-smooth. Let S be a proper sampling (Assumption 1) and v1, . . . , vn be positive scalars

satisfying (5). Then the sequence of primal and dual variables {wt, t}t0 of Quartz (Algorithm 1)

satisfies:

E[P (wt) - D(t)]  (1 - )t(P (w0) - D(0)),

(15)

where  is defined in (7). In particular, if we fix  P (w0) - D(0), then for

T  max
i

1 pi

+

vi pi  n

log

P (w0)-D(0)

 E[P (wT ) - D(T )]  .

(16)

In order to put the above result into context, in the rest of this section we will specialize the above result to two special samplings: a serial sampling, and the  -nice sampling.

3.1 Quartz with serial sampling

When S is a serial sampling, we just need to plug (12) into (16) and derive the bound

T  max
i

1 pi

+

Li pi  n

log

P (w0)-D(0)

= E[P (wT ) - D(T )]  .

(17)

If in addition, S is uniform, then pi = 1/n for all i  [n] and we refer to this special case of Quartz as Quartz-U. By replacing pi = 1/n in (17) we obtain directly the complexity of Quartz-U:

T

n

+

maxi Li 

log

P (w0)-D(0)

= E[P (wT ) - D(T )]  .

(18)

5

Otherwise, we can seek to maximize the right-hand side of the inequality in (17) with respect to the sampling probability p to obtain the best bound. A simple calculation reveals that the optimal probability is given by:

P(S = {i}) = pi d=ef (Li + n)/

n i=1

(Li

+

n)

.

(19)

We shall call Quartz-IP the algorithm obtained by using the above serial sampling probability. The

following complexity result of Quartz-IP can be derived easily by plugging (19) into (17):

T  n+

n i=1

Li

n

log

P (w0)-D(0)

= E[P (wT ) - D(T )]  .

(20)

Note that in contrast with the complexity result of Quartz-U (18), we now have dependence on the average of the eigenvalues Li.

Quartz-U vs Prox-SDCA. Quartz-U should be compared to Proximal Stochastic Dual Coordinate
Ascent (Prox-SDCA) [22, 20]. Indeed, the dual update of Prox-SDCA takes exactly the same form of Quartz-U1, see (11). The main difference is how the primal variable wt is updated: while Quartz performs the update (10), Prox-SDCA (see also [24, 5]) performs the more aggressive update wt = g(t-1) and the complexity result of Prox-SDCA is as follows:

T

n

+

maxi Li 

log

n

+

maxi Li 

D( )-D(0 )

 E[P (wT ) - D(T )]  , (21)

where  is the dual optimal solution. Notice that the dominant terms in (18) and (21) exactly match, although our logarithmic term is better and simpler. This is due to a direct bound on the decrease of the primal-dual error of Quartz, without the need to first analyze the dual error, in contrast to the typical approach for most of the dual coordinate ascent methods [22, 23, 20, 21, 9].

Quartz-IP vs Iprox-SDCA. The importance sampling (19) was previously used in the algorithm Iprox-SDCA [27], which extends Prox-SDCA to non-uniform serial sampling. The complexity of Quartz-IP (20) should then be compared with the following complexity result of Iprox-SDCA [27]:

T  n+

n i=1

Li

n

log

n+ n i=1

Li

n

D( )-D(0 )

 E[P (wT ) - D(T )]  . (22)

Again, the dominant terms in (20) and (22) exactly match but our logarithmic term is smaller.

3.2 Quartz with  -nice Sampling (standard mini-batching)

We now specialize Theorem 2 to the case of the  -nice sampling. We define w such that:

maxi max

d j=1

1

+

(j -1)( -1) n-1

AjiAji

=

1

+

(-1)( -1) n-1

maxi Li

It is clear that 1  w  maxj wj  n and can be considered as a measure of the density of the data. By plugging (13) into (16) we obtain directly the following corollary.

Corollary 3 Assume S is the  -nice sampling and v is chosen as in (13). If we let  P (w0) - D(0) and

 (-1)( -1) 

1+

T



n


+

n-1 

maxi Li
 log

P (w0)-D(0)

 E[P (wT ) - D(T )]  . (23)

Let us now have a detailed look at the above result, especially in terms of how it compares with the serial uniform case (18). For fully sparse data, we get perfect linear speedup: the bound in (23) is a 1/ fraction of the bound in (18). For fully dense data, the condition number ( d=ef maxi Li/()) is unaffected by mini-batching. For general data, the behaviour of Quartz with  -nice sampling interpolates these two extreme cases. It is important to note that regardless of the condition number , as long as   1 + (n - 1)/(w - 1) the bound in (23) is at most a 2/ fraction of the bound in (18). Hence, for sparser problems, Quartz can achieve linear speedup for larger mini-batch sizes.
1In [20] the authors proposed five options of dual updating rule. Our dual updating formula (11) should be compared with option V in Prox-SDCA. For the same reason as given in the beginning of [20, Appendix A.], Quartz implemented with the same other four options achieves the same complexity result as Theorem 2.

6

3.3 Quartz vs existing primal-dual mini-batch methods

We now compare the above result with existing mini-batch stochastic dual coordinate ascent methods. The mini-batch variants of SDCA, to which Quartz with  -nice sampling can be naturally compared, have been proposed and analyzed previously in [23], [21] and [26]. In [23], the authors proposed to use a so-called safe mini-batching, which is precisely equivalent to finding the stepsize parameter v satisfying (5) (in the special case of  -nice sampling). However, they only analyzed the case where the functions {i}i are non-smooth. In [21], the authors studied accelerated minibatch SDCA (ASDCA), specialized to the case when the regularizer g is the squared L2 norm. They showed that the complexity of ASDCA interpolates between that of SDCA and accelerated gradient descent (AGD) [13] through varying the mini-batch size  . In [26], the authors proposed a mini-batch extension of their stochastic primal-dual coordinate algorithm (SPDC). Both ASDCA and SPDC reach the same complexity as AGD when the mini-batch size equals to n, thus should be considered as accelerated algorithms 2. The complexity bounds for all these algorithms are summarized in Table 1. In Table 2 we compare the complexities of SDCA, ASDCA, SPDC and Quartz in several regimes.

Algorithm SDCA [22]
ASDCA [21]
SPDC [26] Quartz with  -nice
sampling

Iteration complexity

4 x max

n+

1 

1

n 

,

n 

,

1 

,

n3
2 ( ) 3

n 

+

n 

n 

+

1

+

( -1)( -1) n-1

1 

g

1 2

*

2

1 2

*

2

general

general

Table 1: Comparison of the iteration complexity of several primal-dual algorithms performing stochastic coordinate ascent steps in the dual using a mini-batch of examples of size  (with the exception of SDCA, which is a serial method using  = 1.

Algorithm
SDCA [22]
ASDCA [21]
SPDC [26] Quartz ( -nice)

n

=

(

1 n

)

n3/2/

n3/2 + n5/4

 /

+

nn45//34//2/3

n3/2/

+

  n

n = (1) n 
n/  
n/ 
n/ + 

n = ( ) n
n/
n/ n/

 n = ( n)
n
n/ + n3/4/ n/ + n3/4/
 n/ + / n

Table 2: Comparison of leading factors in the complexity bounds of several methods in 5 regimes.

Looking at Table 2, we see that in the n = ( ) regime (i.e., if the condition number is  = (n/ )), Quartz matches the linear speedup (when compared to SDCA) of ASDCA and SPDC. When the condition number is roughly equal to the sample size ( = (n)), then Quartz does better than both ASDCA and SPDC as long as n/ +   n/  . In particular, this is the case when the data is sparse:   n/  . If the data is even more sparse (and in many big data applications one has  = O(1)) and we have   n/ , then Quartz significantly outperforms both ASDCA and SPDC. Note that Quartz can be better than both ASDCA and SPDC even in the domain of accelerated methods, that is, when the condition number is larger than the number of examples:  = 1/()  n. Indeed, we have the following result:

Proposition 4 Assume that n  1 and that maxi Li = 1. If the data is sufficiently sparse so that

 n 

1

+

n

+

(-1)( -1) n-1

2
,

(24)

then the iteration complexity (in O order) of Quartz is better than that of ASDCA and SPDC.

The result can be interpreted as follows: if n     n/(1 + n/)2 (that is,    n  (1 + n)2), then there are sparse-enough problems for which Quartz is better than both ASDCA and SPDC.
2APCG [9] also reaches accelerated convergence rate but was not proposed in the mini-batch setting.

7

4 Experimental Results

In this section we demonstrate how Quartz specialized to different samplings compares with other methods. All of our experiments are performed with m = 1, for smoothed hinge-loss functions {i} with  = 1 and squared L2-regularizer g, see [20]. The experiments were performed on the three datasets reported in Table 3, and three randomly generated large dataset [12] with n = 100, 000 examples, d = 100, 000 features with different sparsity. In Figure 1 we compare Quartz specialized to serial sampling and for both uniform and optimal sampling with Prox-SDCA and Iprox-SDCA, previously discussed in Section 3.1, on three datasets. Due to the conservative primal date in Quartz, Quartz-U appears to be slower than Prox-SDCA in practice. Nevertheless, in all the experiments, Quartz-IP shows almost identical convergence behaviour to that of Iprox-SDCA. In Figure 2 we compare Quartz specialized to  -nice sampling with mini-batch SPDC for different values of  , in the domain of accelerated methods ( = 10n). The datasets are randomly generated following [13, Section 6]. When  = 1, it is clear that SPDC outperforms Quartz as the condition number is larger than n. However, as  increases, the number of data processed by SPDC is increased by  as predicted by its theory but the number of data processed by Quartz remains almost the same by taking advantage of the large sparsity of the data. Hence, Quartz is much better in the large  regime.

Dataset cov1 w8a ijcnn1

# Training size n 522,911 49,749 49,990

# features d 54 300 22

Sparsity (# nnz/(nd)) 22.22% 3.91% 59.09%

Table 3: Datasets used in our experiments.

100 10-5

Prox-SDCA Quartz-U Iprox-SDCA Quartz-IP

100 10-5

Prox-SDCA Quartz-U Iprox-SDCA Quartz-IP

100 10-5

Prox-SDCA Quartz-U Iprox-SDCA Quartz-IP

Primal dual gap Primal dual gap Primal dual gap

10-10

10-10

10-10

10-150

50 100 nb of epochs

150

10-150

100 200 nb of epochs

300

10-150

50 nb of epochs

100

(a) cov1; n = 522911;  = 1e-06 (b) w8a; n = 49749;  = 1e-05 (c) ijcnn1; n = 49990;  = 1e-05

Figure 1: Comparison of Quartz-U (uniform sampling), Quartz-IP (optimal importance sampling), ProxSDCA (uniform sampling) and Iprox-SDCA (optimal importance sampling).

Primal dual gap Primal dual gap Primal dual gap

100 10-2

Quartz- =1 SPDC- =1 Quartz- =100 SPDC- =100 Quartz- =1000 SPDC- =1000

100 10-2

Quartz- =1 SPDC- =1 Quartz- =10 SPDC- =10 Quartz- =100 SPDC- =100

100 10-2

Quartz- =1 SPDC- =1 Quartz- =10 SPDC- =10 Quartz- =40 SPDC- =40

10-4

10-4

10-4

10-6

10-6

10-6

10-8

10-8

10-8

10-100

100 200 nb of epochs

300

(a) Rand1; n = 105;  = 1e-06

10-100

100 200 nb of epochs

300

(b) Rand2; n = 105;  = 1e-06

10-100

100 200 nb of epochs

300

(c) Rand3; n = 105;  = 1e-06

Figure 2: Comparison of Quartz with SPDC for different mini-batch size  in the regime  = 10n. The three random datasets Random1, Random2 and Random2 have respective sparsity 0.01%, 0.1% and 1%.

8

References
[1] A. Defazio, F. Bach, and S. Lacoste-julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 27, pages 1646-1654. 2014.
[2] O. Fercoq and P. Richtarik. Accelerated, parallel and proximal coordinate descent. SIAM Journal on Optimization (after minor revision), arXiv:1312.5799, 2013.
[3] O. Fercoq and P. Richtarik. Smooth minimization of nonsmooth functions by parallel coordinate descent. arXiv:1309.5885, 2013.
[4] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S.S. Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear SVM. In Proc. of the 25th International Conference on Machine Learning, ICML '08, pages 408-415, 2008.
[5] M. Jaggi, V. Smith, M. Takac, J. Terhorst, S. Krishnan, T. Hofmann, and M.I. Jordan. Communicationefficient distributed dual coordinate ascent. In Advances in Neural Information Processing Systems 27, pages 3068-3076. Curran Associates, Inc., 2014.
[6] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In C.j.c. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 315-323. 2013.
[7] J. Konecny, J. Lu, P. Richtarik, and M. Takac. mS2GD: Mini-batch semi-stochastic gradient descent in the proximal setting. arXiv:1410.4744, 2014.
[8] J. Konecny and P. Richtarik. S2GD: Semi-stochastic gradient descent methods. arXiv:1312.1666, 2013.
[9] Q. Lin, Z. Lu, and L. Xiao. An accelerated proximal coordinate gradient method and its application to regularized empirical risk minimization. Technical Report MSR-TR-2014-94, July 2014.
[10] J. Mairal. Incremental majorization-minimization optimization with application to large-scale machine learning. SIAM J. Optim., 25(2):829-855, 2015.
[11] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim., 19(4):1574-1609, 2008.
[12] Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM J. Optim., 22(2):341-362, 2012.
[13] Y. Nesterov. Gradient methods for minimizing composite functions. Math. Program., 140(1, Ser. B):125- 161, 2013.
[14] Z. Qu and P. Richtarik. Coordinate descent methods with arbitrary sampling II: Expected separable overapproximation. arXiv:1412.8063, 2014.
[15] Z. Qu and P. Richtarik. Coordinate descent methods with arbitrary sampling I: Algorithms and complexity. arXiv:1412.8060, 2014.
[16] P. Richtarik and M. Takac. On optimal probabilities in stochastic coordinate descent methods. Optimization Letters, published online 2015.
[17] P. Richtarik and M. Takac. Parallel coordinate descent methods for big data optimization. Math. Program., published online 2015.
[18] H. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Statistics, 22:400-407, 1951.
[19] M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. arXiv:1309.2388, 2013.
[20] S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual coordinate ascent. arXiv:1211.2717, 2012.
[21] S. Shalev-shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems 26, pages 378-385. 2013.
[22] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss. J. Mach. Learn. Res., 14(1):567-599, February 2013.
[23] M. Takac, A.S. Bijral, P. Richtarik, and N. Srebro. Mini-batch primal and dual methods for svms. In Proc. of the 30th International Conference on Machine Learning (ICML-13), pages 1022-1030, 2013.
[24] T. Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems 26, pages 629-637. 2013.
[25] T. Zhang. Solving large scale l.ear prediction problems using stochastic gradient descent algorithms. In Proc. of the 21st International Conference on Machine Learning (ICML-04), pages 919-926, 2004.
[26] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization. In Proc. of the 32nd International Conference on Machine Learning (ICML-15), pages 353-361, 2015.
[27] P. Zhao and T. Zhang. Stochastic optimization with importance sampling. ICML, 2015.
9

