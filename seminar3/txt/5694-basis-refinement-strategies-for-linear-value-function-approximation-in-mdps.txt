Basis Refinement Strategies for Linear Value Function Approximation in MDPs

Gheorghe Comanici School of Computer Science
McGill University Montreal, Canada gcoman@cs.mcgill.ca

Doina Precup School of Computer Science
McGill University Montreal, Canada dprecup@cs.mcgill.ca

Prakash Panangaden School of Computer Science
McGill University Montreal, Canada prakash@cs.mcgill.ca

Abstract
We provide a theoretical framework for analyzing basis function construction for linear value function approximation in Markov Decision Processes (MDPs). We show that important existing methods, such as Krylov bases and Bellman-errorbased methods are a special case of the general framework we develop. We provide a general algorithmic framework for computing basis function refinements which "respect" the dynamics of the environment, and we derive approximation error bounds that apply for any algorithm respecting this general framework. We also show how, using ideas related to bisimulation metrics, one can translate basis refinement into a process of finding "prototypes" that are diverse enough to represent the given MDP.
1 Introduction
Finding optimal or close-to-optimal policies in large Markov Decision Processes (MDPs) requires the use of approximation. A very popular approach is to use linear function approximation over a set of features [Sutton and Barto, 1998, Szepesvari, 2010]. An important problem is that of determining automatically this set of features in such a way as to obtain a good approximation of the problem at hand. Many approaches have been explored, including adaptive discretizations [Bertsekas and Castanon, 1989, Munos and Moore, 2002], proto-value functions [Mahadevan, 2005], Bellman error basis functions (BEBFs) [Keller et al., 2006, Parr et al., 2008a], Fourier basis [Konidaris et al., 2011], feature dependency discovery [Geramifard et al., 2011] etc. While many of these approaches have nice theoretical guarantees when constructing features for fixed policy evaluation, this problem is significantly more difficult in the case of optimal control, where multiple policies have to be evaluated using the same representation.
We analyze this problem by introducing the concept of basis refinement, which can be used as a general framework that encompasses a large class of iterative algorithms for automatic feature extraction. The main idea is to start with a set of basis which are consistent with the reward function, i.e. which allow only states with similar immediate reward to be grouped together. One-step look-ahead is then used to find parts of the state space in which the current basis representation is inconsistent with the environment dynamics, and the basis functions are adjusted to fix this problem. The process continues iteratively. We show that BEBFs [Keller et al., 2006, Parr et al., 2008a] can be viewed as a special case of this iterative framework. These methods iteratively expand an existing set of basis functions in order to capture the residual Bellman error. The relationship between such features and augmented Krylov bases allows us to show that every additional feature in these sets is consistently refining intermediate bases. Based on similar arguments, it can be shown that other methods, such as those based on the concept of MDP homomorphisms [Ravindran and Barto, 2002], bisimulation metrics [Ferns et al., 2004], and partition refinement algorithms [Ruan et al., 2015], are also special cases of the framework. We provide approximation bounds for sequences of refinements, as
1

well as a basis convergence criterion, using mathematical tools rooted in bisimulation relations and metrics [Givan et al., 2003, Ferns et al., 2004].
A final contribution of this paper is a new approach for computing alternative representations based on a selection of prototypes that incorporate all the necessary information to approximate values over the entire state space. This is closely related to kernel-based approaches [Ormoneit and Sen, 2002, Jong and Stone, 2006, Barreto et al., 2011], but we do not assume that a metric over the state space is provided (which allows one to determine similarity between states). Instead, we use an iterative approach, in which prototypes are selected to properly distinguish dynamics according to the current basis functions, then a new metric is estimated, and the set of prototypes is refined again. This process relies on using pseudometrics which in the limit converge to bisimulation metrics.

2 Background and notation

We will use the framework of Markov Decision Processes, consisting of a finite state space S, a
finite action space A, a transition function P : (S x A)  P(S)1, where P (s, a) is a probability distribution over the state space S, a reward function2 R : (S xA)  R. For notational convenience, P a(s), Ra(s) will be used to denote P (s, a) and R(s, a), respectively. One of the main objectives

of MDP solvers is to determine a good action choice, also known as a policy, from every state that the system would visit. A policy  : S  P(A) determines the probability of choosing each action a given the state s (with aA (s)(a) = 1). The value of a policy  given a state s0 is defined as

V (s0) = E

 i=0



iRai

(si)

si+1  P ai (si), ai  (si) .

Note that V  is a real valued function [[S  R]]; the space of all such functions will be denoted

by FS. We will also call such functions features. Let R and P  denote the reward and tran-

sition probabilities corresponding to choosing actions according to . Note that R  FS and

P   [[FS  FS]], where3 R(s) = Ea(s)[Ra(s)] and P (f )(s) = Ea(s) EP a(s)[f ] . Let

T   [[FS  FS]] denote the Bellman operator: T (f ) = R + P (f ). This operator is linear

and V  is its fixed point, i.e. T (V ) = V . Most algorithms for solving MDPs will either use

the model (R, P ) to find V  (if this model is available and/or can be estimated efficiently), or

they will estimate V  directly associated with the best policy

using  is

samples of the the fixed point

model, {(si, ai, ri, si+1)}i=0. The of the Bellman optimality operator

value V T  (not


a

linear operator), defined as: T (f ) = maxaA (Ra + P a(f )).

The main problem we address in this paper is that of finding alternative representations for a given MDP. In particular, we look for finite, linearly independent subsets  of FS. These are bases for subspaces that will be used to speed up the search for V , by limiting it to span(). We say that a basis B is a partition if there exists an equivalence relation  on S such that B = {(C) | C  S/}, where  is the characteristic function (i.e. (X)(x) = 1 if x  X and 0 otherwise). Given any equivalence relation , we will use the notation () for the set of characteristic functions on the equivalence classes of , i.e. () = {(C) | C  S/}.4.

Our goal will be to find subsets   FS which allow a value function approximation with strong quality guarantees. More precisely, for any policy  we would like to approximate V  with

V the

= space

k i=1

wii

spanned

for some choice of wi's, by  = {1, 2, ..., k}.

which amounts to finding the best candidate inside A sufficient condition for V  to be an element of

span() (and therefore representable exactly using the chosen set of bases), is for  to span

the reward function and be an invariant subspace of the transition function: R  span() and

f  , P (f )  span(). Linear fixed point methods like TD, LSTD, LSPE [Sutton, 1988,

Bradtke and Barto, 1996, Yu and Bertsekas, 2006] can be used to find the least squares fixed point

approximation V of V  for a representation ; these constitute proper approximation schemes, as

1We will use P(X) to denote the set of probability distributions on a given set X. 2For simplicity, we assume WLOG that the reward is deterministic and independent of the state into which
the system arrives. 3We will use E[f ] = x f (x)(x) to mean the expectation of a function f wrt distribution . If the
function f is multivariate, we will use Ex[f (x, y)] = x f (x, y)(x) to denote expectation of f when y is fixed.
4The equivalence class of an element s  S is {s  S | s  s }. S/ is used for the quotient set of all equivalence classes of .

2

one can determine the number of iterations required to achieve a desired approximation error. Given

a representation as: Tf := 

, the approximate value function (R + P (f )), where  is the

V is the fixed point of the operator T, defined orthogonal projection operator on . Using the

linearity of , it directly the Bellman operator over

ftohlelotwrasntshfoatrmTed(fli)ne=armoRdel+(R,PP)

(f ) :=

a(ndVR

is the , P

fixed point of ). For more

details, see [Parr et al., 2008a,b].

The analysis tools that we will use to establish our results are based on probabilistic bisimulation and
its quantitative analogues. Strong probabilistic bisimulation is a notion of behavioral equivalence
between the states of a probabilistic system, due to [Larsen and Skou, 1991] and applied to MDPs
with rewards by [Givan et al., 2003]. The metric analog is due to [Desharnais et al., 1999, 2004] and
the extension of the metric to include rewards is due to [Ferns et al., 2004]. An equivalence relation  is a a bisimulation relation on the state space S if for every pair (s, s )  S xS, s  s if and only if a  A, C  S/, Ra(s) = Ra(s ) and P a(s)(C) = P a(s )(C) (we use here P a(s)(C) to denote the probability of transitioning into C, under transition s, a). A pseudo-metric is a bisimulation metric if there exists some bisimulation relation  such that s, s , d(s, s ) = 0  s  s .

The bisimulation metrics described by [Ferns et al., 2004] are constructed using the Kantorovich metric for comparing two probability distributions. Given a ground metric d over S, the Kantorovich metric over P(S) takes the largest difference in the expected value of Lipschitz-1 functions with respect to d: (d) := {f  FS | s, s , f (s) - f (s )  d(s, s )}. The distance between two probabilities  and  is computed as: K(d) : (, )  sup(d) E[] - E[]. For more details on the Kantorovich metric, see [Villani, 2003]. The following approximation scheme converges to a bisimulation metric (starting with d0 = 0, the metric that associates 0 to all pairs):
dk+1(s, s ) = T (dk)(s, s ) := max (1 - ) Ra(s) - Ra(s ) + K(dk) P a(s), P a(s ) . (1)
a
The operator T has a fixed point d, which is a bisimulation metric, and dk  d as k  . [Ferns et al., 2004] provide bounds which allow one to assess the quality of general state aggregations using this metric. Given a relation  and its corresponding partition (), one can define an MDP model over () as: Ra = ()Ra and Pa = ()P a, a  A. The approximation error between the true MDP optimal value function V  and its approximation using this reduced MDP model, denoted by V(), is bounded above by:

V()(s) - V (s)



1

1 -

 d(s)

+

max
s S

(1

 -

)2

d

(s

).

(2)

where d(s) is average distance over the uniform distribution U:

from a d(s)

state s to its -equivalence = EsU [d(s, s) | s  s].

class, defined as Similar bounds

an for

expectation representa-

tions that are not partitions can be found in [Comanici and Precup, 2011]. Note that these bounds

are minimized by aggregating states which are "close" in terms of the bisimulation distance d.

3 Basis refinement
In this section we describe the proposed basis refinement framework, which relies on "detecting" and "fixing" inconsistencies in the dynamics induced by a given set of features. Intuitively, states are dynamically consistent with respect to a set of basis functions if transitions out of these states are evaluated the same way by the model {P a | a  A}. Inconsistencies are "fixed" by augmenting a basis with features that are able to distinguish inconsistent states, relative to the initial basis. We are now ready to formalize these ideas.
Definition 3.1. Given a subset F  FS, two states s, s  S are consistent with respect to F , denoted s F s , if f  F, a  A, f (s) = f (s ) and EP a(s)[f ] = EP a(s )[f ]. Definition 3.2. Given two subspaces F, G  FS, G refines F in an MDP M , and write F G, if F  G and
s, s  S, s F s  [g  G, g(s) = g(s )].
Using the linearity of expectation, one can prove that, given two probability distributions , , and a finite subset   F , if span() = F , then f  F, E[f ] = E[f ]  b  , E[b] = E[b] . For the special case of Dirac distributions s and s , for which

3

Es [f ] = f (s), it also holds that f  F, f (s) = f (s )  b  , b(s) = b(s ) . Therefore, Def. 3.2 gives a relation between two subspaces, but the refinement conditions could be checked on any basis choice. It is the subspace itself rather than a particular basis that matters, i.e.   if span() span( ). To fix inconsistencies on a pair (s, s ), for which we can find f   and a  A such that either f (s) = f (s ) or EP a(s)[f ] = EP a(s )[f ], one should construct a new function  with (s) = (s ) and add it to  . To guarantee that all inconsistencies have been addressed, if (s) = (s ) for some    ,  must contain a feature f such that, for some a  A, either f (s) = f (s ) or EP a(s)[f ] = EP a(s )[f ].
In Sec. 5 we present an algorithmic framework consisting of sequential improvement steps, in which a current basis  is refined into a new one,  , with span() span( ). Def 3.2 guarantees that following such strategies expands span() and that the approximation error for any policy will be decreased as a result. We now discuss bounds that can be obtained based on these definitions.

3.1 Value function approximation results

One simple way to create a refinement is to add to  a single element that would address all inconsistencies: a feature that is valued differently for every element of (). Given  : ()  R,
b, b  (), b = b  (b) = (b )     b() (b)b . On the other hand, such a construction provides no approximation guarantee for the optimal value function (unless we make additional assumptions on the problem - we will discuss this further in Section 3.2). Although it addresses inconsistencies in the dynamics over the set of features spanned by , it does not necessarily provide the representation power required to properly approximate the value of the optimal policy. The main theoretical result in this section provides conditions for describing refining sequences of bases, which are not necessarily accurate, but have approximation errors bounded by an exponentially decreasing function. These results are based on (), the largest basis refining subspace: any feature that is constant over equivalence classes of  will be spanned by (), i.e. for any refinement V W , V  W  span((V )). These subsets are convenient as they can be analyzed using the bisimulation metric introduced in [Ferns et al., 2004].
Lemma 3.1. The bisimulation operator in Eq. 1) is a contraction with constant . That is, for any metric d over S, sups,s S |T (d)(s, s )|   sups,s S |d(s, s )|.
The proof relies on the Monge-Kantorovich duality (see [Villani, 2003]) to check that T satisfies sufficient conditions to be a contraction operator. An operator Z is a contraction (with constant  < 1) if Z(x)  Z(x ) whenever x  x , and if Z(x + c) = Z(x) + c for any constant c  R [Blackwell, 1965]. One could easily check these conditions on the operator in Equation 1.
Theorem 3.1. Let 0 represent reward consistency, i.e. s 0 s  a  A, Ra(s) = Ra(s ), and 1 = (0). Additionally, assume {n}n=1 is a sequence of bases such that for all n  1, n n+1 and n+1 is as large as the partition corresponding to consistency over n, i.e. |n+1| = |S/n |. If Vn is the optimal value function computed with respect to representation n, then Vn - V    n+1 sups,s ,a |Ra(s) - Ra(s )|/(1 - )2.

Proof. We will use the bisimulation metric defined in Eq. 1 and Eq. 2 applied to the special case of reduced models over bases {n}n=1.
First, note that Monge-Kantorovich duality is crucial in this proof. It basically states that the Kantorovich metric is a solution to the Monge-Kantorovich problem, when its cost function is equal to the base metric for the Kantorovich metric. Specifically, for two measures  and , and a cost function f  [S x S  R], the Monge-Kantorovich problem computes:

J (f )(, ) = inf{E[f (x, y)] |   P(S x S) s.t. ,  are the marginals corresponding to x and y}

The set of measures  with marginals  and  is also known as the set of couplings of  and . For any metric d over S, J (d)(, ) = K(d)(, ) (for proof, see [Villani, 2003]).

Next, we describe a relation between the metric T n(0) and n.

Since

|n+1| = |S/n | = |(n )| and n+1  span((n )), it must be the case that

span(n+1) = span((n )). It is not hard to see that for the special case of parti-

tions, a refinement can be determined based on transitions into equivalence classes. Given

4

two equivalence relations 1 and 2, the refinement (1) (2) holds if and only if s 2 s  s 1 s and s 2 s  a  A, C  S/1 P a(s)(C) = P a(s )(C) . In particular, s, s with s n+1 s , and C  S/n , P a(s)(C) = P a(s )(C). This equality is crucial in defining the following coupling for J (f )(P a(s), P a(s )): let C  P(S x S) be any coupling of P a(s)|C and P a(s )|C , the restrictions of P a(s) and P a(s ) to C; the latter is possible as the two distributions are equal. Next, define the coupling  of  and  as  = CS/n C . For any cost function f , if s n+1 s , then J (f )(P a(s), P a(s ))  CS/n EC [f ].
Using an inductive argument, we will now show that n, s n s  T n(0)(s, s ) = 0. The base case is clear from the definition: s 0 s  T (0)(s, s ) = 0. Now, assume the former holds for n; that is, C  S/n , s, s  C, T n(0)(s, s ) = 0. But C is zero everywhere except on the set C x C, so EC [T n(0)] = 0. Combining the last two results, we get the following upper bound:
s n+1 s  J (T n(0))(P a(s), P a(s ))  CS/n EC [T n(0)] = 0.
Since T n(0) is a metric, it also holds that J (T n(0))(P a(s), P a(s ))  0. Moreover, as s and s are consistent over n  (0), this pair of states agree on the reward function. Therefore, T n+1(0)(s, s ) = maxa((1 - )|Ra(s) - Ra(s )| + J (T n(0))(P a(s), P a(s ))) = 0.

Finally, for any b  (n ) and s  S with b(s) = 1, and any other state s with b(s) = 1, it must be the case that s n s and T n(0)(s, s) = 0. Therefore,

E
sU

[d(s, s)

|

s

n

s]

=

E
sU

[d(s, s)

-

T

n(0)(s, s)

|

s

n

s]



||d

-

T

n(0)||.

(3)

As span(n) = span((n)), Vn is the optimal value function for the MDP model over (n). Based on (2) and (3), we can conclude that

Vn - V    ||d - T n(0)||/(1 - )2.

(4)

But we already know from Lemma 3.1 that d (defined in Eq. 1) is the fixed point of a contraction

operator with constant . As J (0)(, ) = 0, the following holds for all n  1

||d - T n(0)||  n||T (0) - 0||/(1 - )  n sup |Ra(s) - Ra(s )|.
s,s ,a

(5)

The final result is easily obtained by putting together Equations 4 and 5.

The result of the theorem provides a strategy for constructing refining sequences with strong approximation guarantees. Still, it might be inconvenient to generate refinements as large as S/n , as this might be over-complete; although faithful to the assumptions of the theorem, it might generate features that distinguish states that are not often visited, or pairs of states which are only slightly different. To address this issue, we provide a variation on the concept of refinement that can be used to derive more flexible refining algorithms: refinements that concentrate on local properties.
Definition 3.3. Given a subset F  FS, and a subset   S, two states s, s  S are consistent on  with respect to F , denoted s F, s , if f  F, a  A, f (s) = f (s ) and s  , EP a(s)[f ] = EP a(s)[f ]  EP a(s)[f ] = EP a(s )[f ].
Definition 3.4. Given two subspaces F, G  FS, G refines F locally with respect to , denoted F  G, if F  G and s, s  S, s F, s  [g  G, g(s) = g(s )].
Definition 3.2 is the special case of Definition 3.4 corresponding to a refinement with respect to the whole state space S, i.e. F G  F S G. When the subset  is not important, we will use the notation V  W to say that W refines V locally with respect to some subset of S. The result below states that even if one provides local refinements , one will eventually generate a pair of subspaces which are related through a global refinement property .
Proposition 3.1. Let {i}ni=0 be a set of bases over S with i-1 i i, i = 1, ..., n, for some {i}ni=1 . Assume that n is the maximal refinement (i.e. |n| = |S/n-1,n |). Let  = ii. Then (0,)  span(n).
Proof. Assume s n-1,n s . We will check below all conditions necessary to conclude that s 0, s . First, let f  0. It is immediate from the definition of local refinements that j  n - 1, j  n-1, so that s 0,n s . It follows that f  0, f (s) = f (s ).

5

Next, fix f  0, a  A and s  . If s  n, then EP a(s)[f ] = EP a(s)[f ]  EP a(s)[f ] = EP a(s )[f ], by the assumption above on the pair s, s . Otherwise, j < n such that s  j and j-1 j j. But we already know that f  j, f (s) = f (s ), as j  n-1. We can use this result in the definition of local refinement j-1 j j to conclude that s j-1,j s . Moreover, as s  j , f  0  j-1, EP a(s)[f ] = EP a(s)[f ]  EP a(s)[f ] = EP a(s )[f ]. This completes the definition of consistency on , and it becomes clear that s n-1,n s  s 0, s , or (0,)  span((n-1,n )).
Finally, both n and (n-1,) are bases of the same size, and both refine n-1. It must be that span(n) = span((n-1,n ))  (0,).
3.2 Examples of basis refinement for feature extraction
The concept of basis refinement is not only applicable to the feature extraction methods we will present later, but to methods that have been studied in the past. In particular, methods based on Bellman error basis functions, state aggregation strategies, and spectral analysis using bisimulation metrics are all special cases of basis refinement. We briefly describe the refinement property for the first two cases, and, in the next section, we elaborate on the connection between refinement and bisimulation metrics to provide a new condition for convergence to self-refining bases.
Krylov bases: Consider the uncontrolled (policy evaluation) case, in which one would like to find a set of features that is suited to evaluating a single policy of interest. A common approach to automatic feature generation in this context computes Bellman error basis functions (BEBFs), which have been shown to generate a sequence of representations known as Krylov bases. Given a policy , a Krylov basis n of size n is built using the model (R, P ) (defined in Section 2 as elements of FS and [[FS  FS]], respectively): n = span{R, P R, (P )2R, ..., (P )nR}. It is not hard to check that n n+1, where is the refinement relational property in Def 3.2. Since the initial feature R  (0), the result in Theorem 3.1 holds for the Krylov bases.
Under the assumption of a finite-state MDP (i.e. |S| < ),  := {({s}) | s  S} is a basis for FS, therefore this set of features is finite dimensional. It follows that one can find N  |S| such that one of the Krylov bases is a self-refinement, i.e. N N . This would by no means be the only self-refining basis. In fact this property holds for the basis of characteristic functions,  . The purpose our framework is to determine other self-refining bases which are suited for function approximation methods in the context of controlled systems.
State aggregation: One popular strategy used for solving MDPs is that of computing state aggregation maps. Instead of working with alternative subspaces, these methods first compute equivalence relations on the state space. An aggregate/collapsed model is then derived, and the solution to this model is translated to one for the original problem: the resulting policy provides the same action choice for states that have originally been related. Given any equivalence relation  on S, a state aggregation map is a function from S to any set X,  : S  X, such that s, s , (s ) = (s)  s  s . In order to obtain a significant computational gain, one would like to work with aggregation maps  that reduce the size of the space for which one looks to provide action choices, i.e. |X| |S|. As discussed in Section 3.1, one could work with features that are defined on an aggregate state space instead of the original state space. That is, instead of computing a set of state features   FS, we could work instead with an aggregation map  : S  X and a set of features over X,   FX . If  is the relation such that s  s  (s) = (s ), then   ,     span(()).
4 Using bisimulation metrics for convergence of bases
In Section 3.2 we provide two examples of self-refining subspaces: the Krylov bases and the characteristic functions on single states. The latter is the largest and sparsest basis; it spans the entire state space and the features share no information. The former is potentially smaller and it spans the value of the fixed policy for which it was designed. In this section we will present a third self-refining construction, which is designed to capture bisimulation properties. Based on the results presented in Section 3.1, it can be shown that given a bisimulation relation , the partition it generates is self-refining, i.e. () ().
6

Desirable self-refining bases might be be computationally demanding and/or too complex to use or represent. We propose iterative schemes which ultimately provide a self-refining result - albeit we would have the flexibility of stopping the iterative process before reaching the final result. At the same time, we need a criterion to describe convergence of sequences of bases. That is, we would want to know how close an iterative process is to obtaining a self-refining basis. Inspired by the fixed point theory used to study bisimulation metrics [Desharnais et al., 1999], instead of using a metric over the set of all bases to characterize convergence of such sequences, we will use corresponding metrics over the original state space. This choice is better suited for generalizing previously existing methods that compare pairs of states for bisimilarity through their associated reward models and expected realizations of features over the next state distribution model associated with these states. We will study metric construction strategies based on a map D, defined below, which takes an element of the powerset P(FS) of FS and returns an element of all pseudo-metrics M (S) over S.
D() : (s, s )  maxa (1 - ) |Ra(s) - Ra(s )| +  sup EP a(s)[] - EP a(s )[] (6)  is a set of features whose expectation over next-state distributions should be matched. It is not hard to see that bases  for which D() is a bisimulation metric are by definition self-refining. For example, consider the largest bisimulation relation  on a given MDP. It is not hard to see that D(()) is a bisimulation. A more elaborate example involves the set (d) of Lipschitz-1 continuous functions on [[(S, d)  (R, L1)]] (recall definition and computation details from Section 2). Define d to be the fixed point of the operator T : d  D((d)), i.e. d = supnN T n(0). d has the same property as the bisimulation metric defined in Equation 1. Moreover, given any bisimulation metric d, D((d)) is a bisimulation metric. Definition 4.1. We say a sequence {n}n=1 is a a bisimulation sequence of bases if D(n) converges uniformly from below to a bisimulation metric. If one has the a sequence of refining bases with n n+1, n, then {D(n)}n=1 is an increasing sequence, but not necessarily a bisimulation sequence.
A bisimulation sequence of bases provide an approximation scheme for bases that satisfy two important properties studied in the past: self-refinement and bisimilarity. One could show that the approximation schemes presented in [Ferns et al., 2004], [Comanici and Precup, 2011], and [Ruan et al., 2015] are all examples of bisimulation sequences. We will present in the next section a framework that generalizes all these examples, but which can be easily extended to a broader set of approximation schemes that incorporate both refining and bisimulation principles.
5 Prototype based refinements
In this section we propose a strategy that iteratively builds sequences of refineing sets of features, based on the concepts described in the previous sections. This generates layered sets of features, where the nth layer in the construction will be dependent only on the (n - 1)th layer. Additionally, each feature will be associated with a reward-transition prototype: elements of Q := [[A  (R x P(S))]], associating to each action a reward and a next-state probability distribution. Prototypes can be viewed as "abstract" or representative states, such as used in KBRL methods [Ormoneit and Sen, 2002]. In the layered structure, the similarity between prototypes at the nth layer is based on a measure of consistency with respect to features at the (n - 1)th layer. The same measure of similarity is used to determine whether the entire state space is "covered" by the set of prototypes/features chosen for the nth layer. We say that a space is covered if every state of the space is close to at least one prototype generated by the construction, with respect to a predefined measure of similarity. This measure is designed to make sure that consecutive layers represent refining sets of features. Note that for any given MDP, the state space S is embedded into Q (i.e. S  Q), as (Ra(s), P a(s))  Q for every state s  S. Additionally, The metric generator D, as defined in Equation 6, can be generalized to a map from P(FS) to M (Q).
The algorithmic strategy will look for a sequence {Jn, n}n=1, where Jn  Q is a set of covering prototypes, and n : Jn  FS is a function that associates a feature to every prototype in Jn. Starting with J0 =  and 0 = , the strategy needs to find, at step n > 0, a cover Jn for S, based on the distance metric D(n-1). That is, it has to guarantee that s  S,   Jn with D(n-1)(s, ) = 0. With Jn = Jn  Jn-1 and using a strictly decreasing function  : R0  R (e.g. the energy-based Gibbs measure  (x) = exp(-x) for some  > 0), the framework constructs n : Jn  FS, a map that associates prototypes to features as n()(s) =  (D(n-1)(, s)).
7

Algorithm 1 Prototype refinement

1: J0 =  and 0 =  2: for n = 1 to  do

3: choose a representative subset n  S and a cover approximation error n  0 4: find an n-cover Jn for n 5: define Jn = Jn  Jn-1
6: choose a strictly decreasing function  : R0  R

7:

define n() =

s   (D(n-1)(, s)) n-1()

if s  n, such that D(n-1)(, s)  n otherwise

8: define n = {n() |   Jn} (note that n is a local refinement, n-1 n n)

It is not hard to see that the refinement property holds at every step, i.e. n n+1. First, every

equivalence class to make sure that

of n is represented by some prototype in Jn. a distinction is made between each prototype in

Second, n is purposely defined Jn+1. Moreover, {n}n=1 is a

bisimulation sequence of bases, as the metric generator D is the main tool used in "covering" the

state space with the set of prototypes Jn. Two states will be represented by the same prototype (i.e. they will be equivalent with respect to n ) if and only if the distance between their corresponding reward-transition models is 0.

Algorithm 1 provides pseudo-code for the framework described in this section. Note that it also con-
tains two additional modifications, used to illustrate the flexibility of this feature extraction process.
Through the first modification, one could use the intermediate results at time step n to determine a subset n  S of states which are likely to have a model with significantly distinct dynamics over n-1. As such, the prototypes Jn-1 can be specialized to cover only the significant subset n. Moreover Theorem 3.1 guarantees that if every state in S is picked in n infinitely often, as n  , then the approximation power of the final result is not be compromised. The second modification is based on using the values in the metric D(n-1) for more than just choosing feature activations: one could set at every step constants n  0 and then find Jn such that n is covered using n-balls, i.e. for every state in n, there exists a prototype   Jn with D(n-1)(, s)  n. One can easily show that the refinement property can be maintained using the modified defition of n described in Algorithm 1.

6 Discussion
We proposed a general framework for basis refinement for linear function approximation. The theoretical results show that any algorithmic scheme of this type satisfies strong bounds on the quality of the value function that can be obtained. In other words, this approach provides a "blueprint" for designing algorithms with good approximation guarantees. As discussed, some existing value function construction schemes fall into this category (such as state aggregation refinement, for example). Other methods, like BEBFs, can be interpreted in this way in the case of policy evaluation; however, the "traditional" BEBF approach in the case of control does not exactly fit this framework. However, we suspect that it could be adapted to exactly follow this blueprint (something we leave for future work).
We provided ideas for a new algorithmic approach to this problem, which would provide strong guarantees while being significantly cheaper than other existing methods with similar bounds (which rely on bisimulation metrics). We plan to experiment with this approach in the future. The focus of this paper was to establish the theoretical underpinnings of the algorithm. The algorithm structure we propose is close in spirit to [Barreto et al., 2011], which selects prototype states in order to represent well the dynamics of the system by means of stochastic factorization. However, their approach assumes a given metric which measures state similarity, and selects representative states using k-means clustering based on this metric. Instead, we iterate between computing the metric and choosing prototypes. We believe that the theory presented in this paper opens up the possibility of further development of algorithms for constructive function approximation that have quality guarantees in the control case, and which can be effective also in practice.
8

References
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
Cs. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool, 2010.
D. P. Bertsekas and D. A. Castanon. Adaptive Aggregation Methods for Infinite Horizon Dynamic Programming. IEEE Transactions on Automatic Control, 34, 1989.
R. Munos and A. Moore. Variable Resolution Discretization in Optimal Control. Machine Learning, 49(2-3):291-323, 2002.
S. Mahadevan. Proto-Value Functions: Developmental Reinforcement Learning. In ICML, pages 553-560, 2005.
P. W. Keller, S. Mannor, and D. Precup. Automatic Basis Function Construction for Approximate Dynamic Programming and Reinforcement Learning. In ICML, pages 449-456, 2006.
R. Parr, C. Painter-Wakefiled, L. Li, and M. L. Littman. Analyzing Feature Generation for Value Function Approximation. In ICML, pages 737-744, 2008a.
G. D. Konidaris, S. Osentoski, and P. S. Thomas. Value Function Approximation in Reinforcement Learning using the Fourier Basis. In AAAI, pages 380-385, 2011.
A. Geramifard, F. Doshi, J. Redding, N. Roy, and J. How. Online Discovery of Feature Dependencies. In ICML, pages 881-888, 2011.
B. Ravindran and A. G. Barto. Model Minimization in Hierarchical Reinforcement Learning. In Symposium on Abstraction, Reformulation and Approximation (SARA), pages 196-211, 2002.
N. Ferns, P. Panangaden, and D. Precup. Metrics for finite Markov Decision Processes. In UAI, pages 162-169, 2004.
S. Ruan, G. Comanici, P. Panangaden, and D. Precup. Representation Discovery for MDPs using Bisimulation Metrics. In AAAI, pages 3578-3584, 2015.
R. Givan, T. Dean, and M. Greig. Equivalence Notions and Model Minimization in Markov Decision Processes. Artificial Intelligence, 147(1-2):163-223, 2003.
D. Ormoneit and S. Sen. Kernel-Based Reinforcement Learning. Machine Learning, 49(2-3):161- 178, 2002.
N. Jong and P. Stone. Kernel-Based Models for Reinforcement Learning. In ICML Workshop on Kernel Machines and Reinforcement Learning, 2006.
A. S. Barreto, D. Precup, and J. Pineau. Reinforcement Learning using Kernel-Based Stochastic Factorization. In NIPS, pages 720-728, 2011.
R. S. Sutton. Learning to Predict by the Methods of Temporal Differences. Machine Learning, 3 (1):9-44, 1988.
S. J. Bradtke and A. G. Barto. Linear Least-Squares Algorithms for Temporal Difference Learning. Machine Learning, 22(1-3):33-57, 1996.
H. Yu and D. Bertsekas. Convergence Results for Some Temporal Difference Methods Based on Least Squares. Technical report, LIDS MIT, 2006.
R. Parr, L. Li, G. Taylor, C. Painter-Wakefield, and M. L. Littman. An Analysis of Linear Models, Linear Value-Function Approximation, and Feature Selection for Reinforcement Learning. In ICML, pages 752-759, 2008b.
K. G. Larsen and A. Skou. Bisimulation through Probabilistic Testing. Information and Computation, 94:1-28, 1991.
J. Desharnais, V. Gupta, R. Jagadeesan, and P. Panangaden. Metrics for Labeled Markov Systems. In CONCUR, 1999.
J. Desharnais, V. Gupta, R. Jagadeesan, and P. Panangaden. A metric for labelled Markov processes. Theoretical Computer Science, 318(3):323-354, 2004.
C. Villani. Topics in optimal transportation. American Mathematical Society, 2003.
G. Comanici and D. Precup. Basis Function Discovery Using Spectral Clustering and Bisimulation Metrics. In AAAI, 2011.
D. Blackwell. Discounted Dynamic Programming. Annals of Mathematical Statistics, 36:226-235, 1965.
9

