Halting in Random Walk Kernels

Mahito Sugiyama ISIR, Osaka University, Japan
JST, PRESTO
mahito@ar.sanken.osaka-u.ac.jp

Karsten M. Borgwardt D-BSSE, ETH Zurich
Basel, Switzerland
karsten.borgwardt@bsse.ethz.ch

Abstract
Random walk kernels measure graph similarity by counting matching walks in two graphs. In their most popular form of geometric random walk kernels, longer walks of length k are downweighted by a factor of k ( < 1) to ensure convergence of the corresponding geometric series. We know from the field of link prediction that this downweighting often leads to a phenomenon referred to as halting: Longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1. This is a naive kernel between edges and vertices. We theoretically show that halting may occur in geometric random walk kernels. We also empirically quantify its impact in simulated datasets and popular graph classification benchmark datasets. Our findings promise to be instrumental in future graph kernel development and applications of random walk kernels.
1 Introduction
Over the last decade, graph kernels have become a popular approach to graph comparison [4, 5, 7, 9, 12, 13, 14], which is at the heart of many machine learning applications in bioinformatics, imaging, and social-network analysis. The first and best-studied instance of this family of kernels are random walk kernels, which count matching walks in two graphs [5, 7] to quantify their similarity. In particular, the geometric random walk kernel [5] is often used in applications as a baseline comparison method on graph benchmark datasets when developing new graph kernels. These geometric random walk kernels assign a weight k to walks of length k, where  < 1 is set to be small enough to ensure convergence of the corresponding geometric series.
Related similarity measures have also been employed in link prediction [6, 10] as a similarity score between vertices [8]. However, there is one caveat regarding these approaches. Walk-based similarity scores with exponentially decaying weights tend to suffer from a problem referred to as halting [1]. They may downweight walks of lengths 2 and more, so much so that the similarity score is ultimately completely dominated by walks of length 1. In other words, they are almost identical to a simple comparison of edges and vertices, which ignores any topological information in the graph beyond single edges. Such a simple similarity measure could be computed more efficiently outside the random walk framework. Therefore, halting may affect both the expressivity and efficiency of these similarity scores.
Halting has been conjectured to occur in random walk kernels [1], but its existence in graph kernels has never been theoretically proven or empirically demonstrated. Our goal in this study is to answer the open question if and when halting occurs in random walk graph kernels.
We theoretically show that halting may occur in graph kernels and that its extent depends on properties of the graphs being compared (Section 2). We empirically demonstrate in which simulated datasets and popular graph classification benchmark datasets halting is a concern (Section 3). We conclude by summarizing when halting occurs in practice and how it can be avoided (Section 4).
1

We believe that our findings will be instrumental in future applications of random walk kernels and the development of novel graph kernels.

2 Theoretical Analysis of Halting

We theoretically analyze the phenomenon of halting in random walk graph kernels. First, we review the definition of graph kernels in Section 2.1. We then present our key theoretical result regarding halting in Section 2.2 and clarify the connection to linear kernels on vertex and edge label histograms in Section 2.3.

2.1 Random Walk Kernels

Let G = (V, E, ) be a labeled graph, where V is the vertex set, E is the edge set, and  is a mapping  : V  E   with the range  of vertex and edge labels. For an edge (u, v)  E, we identify (u, v) and (v, u) if G is undirected. The degree of a vertex v  V is denoted by d(v).

The direct (tensor) product Gx = (Vx, Ex, x) of two graphs G = (V, E, ) and G = (V , E, ) is defined as follows [1, 5, 14]:

Vx = { (v, v)  V x V  | (v) = (v) }, Ex = { ((u, u), (v, v))  Vx x Vx | (u, v)  E, (u, v)  E, and (u, v) = (u, v) },

and all labels are inherited, or x((v, v)) = (v) = (v) and x((u, u), (v, v)) = (u, v) = (u, v). We denote by Ax the adjacency matrix of Gx and denote by x and x the minimum and maximum degrees of Gx, respectively.

To measure the similarity between graphs G and G, random walk kernels count all pairs of matching walks on G and G [2, 5, 7, 11]. If we assume a uniform distribution for the starting and stopping probabilities over the vertices of G and G, the number of matching walks is obtained through the adjacency matrix Ax of the product graph Gx [14]. For each k  N, the k-step random walk kernel between two graphs G and G is defined as:

|Vx|

[ k

]

Kxk (G, G) =

lAlx

i,j=1 l=0

ij

with a sequence of positive, real-valued weights 0, 1, 2, . . . , k assuming identity matrix. Its limit Kx(G, G) is simply called the random walk kernel.

that

A0x

=

I,

the

Interestingly, Kx can be directly computed if weights are the geometric series, or l = l, resulting in the geometric random walk kernel:

|Vx| [

]

KGR(G, G) =

lAlx

=

|Vx|

[ (I

-

Ax

)-1]ij

.

i,j=1 l=0

ij i,j=1

In the above equation, let (I - Ax)x = 0 for some value of x. Then, Axx = x and (Ax)lx =

x for any l Therefore,

 (I

N. If (Ax)l - Ax)-1 =

coln=v0ergleAslxtofr0oams

l  , (I - Ax) is invertible since x becomes the equation (I - Ax)(I + Ax + 2A2x + . . . )

0. =

I [5]. It is well-known that the geometric series of matrices, often called the Neumann series,

I + Ax + (Ax)2 + * * * converges only if the maximum eigenvalue of Ax, denoted by x,max, is

strictly smaller than 1/. Therefore, the geometric random walk kernel KGR is well-defined only if

 < 1/x,max.

There is a relationship for the minimum and maximum degrees x and x of Gx [3]: x  dx  x,max  x, where dx is the average of the vertex degrees of Gx, or dx = (1/|Vx|) vVx d(v). In practice, it is sufficient to set the parameter  < 1/x.

In the inductive learning setting, since we do not know a priori target graphs that a learner will
receive in the future,  should be small enough so  < 1/x,max for any pair of unseen graphs. Otherwise, we need to re-compute the full kernel matrix and re-train the learner. In the transductive

2

setting, we are given a collection G of graphs beforehand. We can explicitly compute the upper bound of , which is (maxG,GG x,max)-1 with the maximum of the maximum eigenvalues over all pairs of graphs G, G  G.

2.2 Halting

The geometric random walk kernel KGR is one of the most popular graph kernels, as it can take walks of any length into account [5, 14]. However, the fact that it weights walks of length k by the kth power of , together with the condition that  < (x,max)-1 < 1, immediately tells us that the contribution of longer walks is significantly lowered in KGR. If the contribution of walks of length 2 and more to the kernel value is even completely dominated by the contribution of walks of length
1, we would speak of halting. It is as if the random walks halt after one step.

Here, we analyze under which conditions this halting phenomenon may occur in geometric random walk kernels. We obtain the following key theoretical statement by comparing KGR to the one-step random walk kernel Kx1 .

Theorem 1 Let 0 = 1 and 1 =  in the random walk kernel. For a pair of graphs G and G,

Kx1 (G, G)  KGR(G, G)  Kx1 (G, G) + ,

where



=

|Vx|

(x)2 1 - x

,

and  monotonically converges to 0 as   0.

Proof. Let d(v) be the degree of a vertex v in Gx and N (v) be the set of neighboring vertices of v, that is, N (v) = {u  Vx | (u, v)  Ex}. Since Ax is the adjacency matrix of Gx, the following
relationships hold:

|Vx|



[Ax]ij =

d(v)  |Vx|x,

i,j=1

vVx

|Vx|  

[A2x]ij =

d(v)  |Vx|2x,

i,j=1

vVx vN (v)

|Vx|   [A3x]ij =

 |Vx|

d(v)  |Vx|3x , . . . ,

[Anx]ij  |Vx|nx.

i,j=1

vVx vN (v) vN (v)

i,j=1

From the assumption that x < 1, we have

|Vx |

|Vx |

KGR(G, G) =

[I + Ax + 2A2x + . . . ]ij = Kx1 (G, G) +

[2A2x + 3A3x + . . . ]ij

i,j=1

i,j=1

 Kx1 (G, G) + |Vx|22x(1 + x + 22x + . . . ) = Kx1 (G, G) + .

It is clear that  monotonically goes to 0 when   0.



Moreover, we can normalize  by dividing KGR(G, G) by Kx1 (G, G).

Corollary 1 Let 0 = 1 and 1 =  in the random walk kernel. For a pair of graphs G and G,

1



KGR(G, G) Kx1 (G, G)



1

+

,

where

 =

(x)2

(1 - x)(1 + dx)

and dx is the average of vertex degrees of Gx.

Proof. Since we have



Kx1 (G, G) = |Vx| + 

d(v) = |Vx|(1 + dx),

vVx

it follows that /Kx1 (G, G) = .

Theorem 1 can be easily generalized to any k-step random walk kernel Kxk .



3

Corollary 2 Let (k) = |Vx|(x)k/(1 - x). For a pair of graphs G and G, we have Kxk (G, G)  KGR(G, G)  Kxk (G, G) + (k + 1).
Our results imply that, in the geometric random walk kernel KGR, the contribution of walks of length longer than 2 diminishes for very small choices of . This can easily happen in real-world graph data, as  is upper-bounded by the inverse of the maximum degree of the product graph.

2.3 Relationships to Linear Kernels on Label Histograms

Next, we clarify the relationship between KGR and basic linear kernels on vertex and edge label histograms. We show that halting KGR leads to the convergence of it to such linear kernels.

Given a pair of graphs G and G, let us introduce two linear kernels on vertex and edge histograms.

Assume that the range of labels  = {1, 2, . . . , s} without loss of generality. The vertex label

histogram of a graph G = (V, E, ) is a vector f = (f1, f2, . . . , fs), such that fi = |{v  V | (v) = i}| for each i  . Let f and f  be the vertex label histograms of graphs G and G,

respectively. The vertex label histogram kernel KVH(G, G) is then defined as the linear kernel between f and f :

KVH(G,

G)

=

f

,

f



=

s
i=1

fifi.

Similarly, the edge label histogram is a vector g = (g1, g2, . . . , gs), such that gi = |{(u, v)  E | (u, v) = i}| for each i  . The edge label histogram kernel KEH(G, G) is defined as the linear kernel between g and g, for respective histograms:

KEH(G,

G)

=

g,

g

=

s
i=1

gi gi .

Finally, we introduce the vertex-edge label histogram. Let h = (h111, h211, . . . , hsss) be a histogram vector, such that hijk = |{(u, v)  E | (u, v) = i, (u) = j, (v) = k}| for each i, j, k  . The vertex-edge label histogram kernel KVEH(G, G) is defined as the linear kernel between h and h for the respective histograms of G and G:

KVEH(G,

G)

=

h,

h

=

s
i,j,k=1

hij k hij k .

Notice that KVEH(G, G) = KEH(G, G) if vertices are not labeled.

From the definition of the direct product of graphs, we can confirm the following relationships between histogram kernels and the random walk kernel.

Lemma 1 For a pair of graphs G, G and their direct product Gx, we have

KVH(G, G)

=

1 0

Kx0

(G,

G)

=

|Vx|.

KVEH(G, G)

=

1 1

Kx1

(G,

G)

-

0 1

Kx0 (G,

G)

=

|Vx | [Ax]ij .
i,j=1

Proof.

The

first

equation 

KVH(G,

G)

=

|Vx|

can

be

proven

from

the

following:

KVH(G, G) = |{ v  V  | (v) = (v) }| = |{ (v, v)  V x V  | (v) = (v) }|

vV

=

|Vx|

=

1 0

Kx0 (G,

G

).

We can prove the second equation in a similar fashion:



KVEH(G, G) = 2

|{ (u, v)  E | (u, v) = (u, v), (u) = (u), (v) = (v) }|

=

2

(u{,v()E (u,

v),

(u,

v))



E

x

E

(u, v) = (u, v),

}

(u) = (u), (v) = (v)

=

2|Ex|

=

|Vx| [Ax]ij
i,j=1

=

1 1

Kx1

(G,

G)

-

0 1

Kx0 (G,

G).



4

Finally, let us define a new kernel

KH(G, G) := KVH(G, G) + KVEH(G, G)

(1)

with a parameter . From Lemma 1, since KH(G, G) = Kx1 (G, G) holds if 0 = 1 and 1 =  in the one-step random walk kernel Kx1 , we have the following relationship from Theorem 1.

Corollary 3 For a pair of graphs G and G, we have

KH(G, G)  KGR(G, G)  KH(G, G) + ,

where  is given in Theorem 1.

To summarize, our results show that if the parameter  of the geometric random walk kernel KGR is small enough, random walks halt, and KGR reduces to KH, which finally converges to KVH. This is based on vertex histograms only and completely ignores the topological structure of the graphs.

3 Experiments
We empirically examine the halting phenomenon of the geometric random walk kernel on popular real-world graph benchmark datasets and semi-simulated graph data.
3.1 Experimental Setup
Environment. We used Amazon Linux AMI release 2015.03 and ran all experiments on a single core of 2.5 GHz Intel Xeon CPU E5-2670 and 244 GB of memory. All kernels were implemented in C++ with Eigen library and compiled with gcc 4.8.2.
Datasets. We collected five real-world graph classification benchmark datasets:1 ENZYMES, NCI1, NCI109, MUTAG, and D&D, which are popular in the graph-classification literature [13, 14]. ENZYMES and D&D are proteins, and NCI1, NCI109, and MUTAG are chemical compounds. Statistics of these datasets are summarized in Table 1, in which we also show the maximum of maximum degrees of product graphs maxG,GG x for each dataset G. We consistently used max = (maxG,GG x)-1 as the upper bound of  in geometric random walk kernels, in which the gap was less than one order as the lower bound of . The average degree of the product graph, the lower bound of , were 18.17, 7.93, 5.60, 6.21, and 13.31 for ENZYMES, NCI1, NCI109, MUTAG, and DD, respectively.
Kernels. We employed the following graph kernels in our experiments: We used linear kernels on vertex label histograms KVH, edge label histograms KEH, vertex-edge label histograms KVEH, and the combination KH introduced in Equation (1). We also included a Gaussian RBF kernel between vertex-edge label histograms, denoted as KVEH,G. From the family of random walk kernels, we used the geometric random walk kernel KGR and the k-step random walk kernel Kxk . Only the number k of steps were treated as a parameter in Kxk and k was fixed to 1 for all k. We used fix-point iterations [14, Section 4.3] for efficient computation of KGR. Moreover, we employed the Weisfeiler-Lehman subtree kernel [13], denoted as KWL, as the state-of-the-art graph kernel, which has a parameter h of the number of iterations.
3.2 Results on Real-World Datasets
We first compared the geometric random walk kernel KGR to other kernels in graph classification. The classification accuracy of each graph kernel was examined by 10-fold cross validation with multiclass C-support vector classification (libsvm2 was used), in which the parameter C for CSVC and a parameter (if one exists) of each kernel were chosen by internal 10-fold cross validation (CV) on only the training dataset. We repeated the whole experiment 10 times and reported average
1The code and all datasets are available at: http://www.bsse.ethz.ch/mlcb/research/machine-learning/graph-kernels.html
2http://www.csie.ntu.edu.tw/cjlin/libsvm/

5

Accuracy

Table 1: Statistics of graph datasets, |V | and |E| denote the number of vertex and edge labels.

Dataset
ENZYMES NCI1 NCI109 MUTAG D&D

Size
600 4110 4127 188 1178

#classes
6 2 2 2 2

avg.|V |
32.63 29.87 29.68 17.93 284.32

avg.|E|
62.14 32.3 32.13 19.79
715.66

max|V |
126 111 111
28 5748

max|E|
149 119 119
33 14267

|V |
3 37 38
7 82

|E |
1 3 3 11 1

maxx
65 16 17 10 50

(i) Comparison of various graph kernels (ii) 50 50

Comparison of KGR with KH KGR KH

40 40

(iii) 50 40

k-step Kxk

Accuracy

Accuracy

30 30 30

20 KVH KEH KVEHKH KVEH,G KGR Kxk KWL
Label histogram Random walk

20 10-

10- 10- Parameter 

10-

(a) ENZYMES

(i) Comparison of various graph kernels (ii) Comparison of KGR with KH

85 85 KGR

80

KH 80

20 1
(iii) 85 80

3579 Number of steps k
k-step Kxk

Accuracy

Accuracy

75 75 75

70 70 70

65 65 65

KVH KEH KVEHKH KVEH,G KGR Kxk KWL Label histogram Random walk

10- 10- 10- 10- 0.0625 Parameter 

1

(b) NCI1

(i) Comparison of various graph kernels (ii) Comparison of KGR with KH (iii)

85 80

85

KGR KH

80

85 80

3579 Number of steps k
k-step Kxk

Accuracy

Accuracy

75 75 75

70 70 70 65 65 65

KVH KEH KVEHKH KVEH,G KGR Kxk KWL Label histogram Random walk

10- 10- 10- 10- 0.0588 Parameter 
(c) NCI109

13579 Number of steps k

Figure 1: Classification accuracy on real-world datasets (Means  SD).

Accuracy

Accuracy

classification accuracies with their standard errors. The list of parameters optimized by the internal CV is as follows: C  {2-7, 2-5, . . . , 25, 27} for C-SVC, the width   {10-2, . . . , 102} in the RBF kernel KVEH,G, the number of steps k  {1, . . . , 10} in Kxk , the number of iterations h  {1, . . . , 10} in KWL, and   {10-5, . . . , 10-2, max} in KH and KGR, where max = (maxG,GG x)-1.
Results are summarized in the left column of Figure 1 for ENZYMES, NCI1, and NCI109. We present results on MUTAG and D&D in the Supplementary Notes, as different graph kernels do not give significantly different results (e.g., [13]). Overall, we could observe two trends. First, the Weisfeiler-Lehman subtree kernel KWL was the most accurate, which confirms results in [13],

6

Percentage

(a)
50

ENZYMES

(b)
50

NCI1

(c)
50

NCI109

40 40 40

Percentage

Percentage

30 30 30

20 20 20

10 10 10

0 -4 -3 -2 -1 0 1 2 log '

0 -4 -3 -2 -1 0 1 2 log '

0 -4 -3 -2 -1 0 1 2 log '

Figure 2: Distribution of log10 , where  is defined in Corollary 1, in real-world datasets.

(a) 50
45
40
35
30

Sim-ENZYMES KGR KH KVH

Accuracy

(b) 80 75 70

Sim-NCI1

(c)

KGR 80 KH KVH 75

Accuracy

70

Sim-NCI109

KGR KH KVH

25

0 10 20 50

100

Number of added edges

65

0 10 20

50

100

Number of added edges

65

0 10 20 50

100

Number of added edges

Figure 3: Classification accuracy on semi-simulated datasets (Means  SD).

Accuracy

Second, the two random walk kernels KGR and Kxk show greater accuracy than naive linear kernels on edge and vertex histograms, which indicates that halting is not occurring in these datasets. It is also noteworthy that employing a Gaussian RBF kernel on vertex-edge histograms leads to a clear improvement over linear kernels on all three datasets. On ENZYMES, the Gaussian kernel is even on par with the random walks in terms of accuracy.

To investigate the effect of halting in more detail, we show the accuracy of KGR and KH in the center column of Figure 1 for various choices of , from 10-5 to its upper bound. We can clearly see that halting occurs for small , which greatly affects the performance of KGR. More specifically, if it is chosen to be very small (smaller than 10-3 in our datasets), the accuracies are close to the naive baseline KH that ignores the topological structure of graphs. However, accuracies are much closer to that reached by the Weisfeiler-Lehman kernel if  is close to its theoretical maximum. Of course, the theoretical maximum of  depends on unseen test data in reality. Therefore, we often have to set  conservatively so that we can apply the trained model to any unseen graph data.

Moreover, we also investigated the accuracy of the random walk kernel as a function of the number of steps k of the random walk kernel Kxk . Results are shown in the right column of Figure 1. In all datasets, accuracy improves with each step, up to four to five steps. The optimal number of steps in Kxk and the maximum  give similar accuracy levels. We also confirmed Theorem 1 that conservative choices of  (10-3 or less) give the same accuracy as a one-step random walk.

In addition, Figure 2 shows (max x)-1 for all pairs of

histograms of log10 , graphs in the respective

where  datasets.

is given in Corollary 1 for  The value  can be viewed as

= the

deviation of KGR from KH in percentages. Although  is small on average (about 0.1 percent in

ENZYMES and NCI datasets), we confirmed the existence of relatively large  in the plot (more

than 1 percent), which might cause the difference between KGR and KH.

3.3 Results on Semi-Simulated Datasets
To empirically study halting, we generated semi-simulated graphs from our three benchmark datasets (ENZYMES, NCI1, and NCI109) and compared the three kernels KGR, KH, and KVH. In each dataset, we artificially generated denser graphs by randomly adding edges, in which the number of new edges per graph was determined from a normal distribution with the mean

7

m  {10, 20, 50, 100} and the distribution of edge labels was unchanged. Note that the accuracy of the vertex histogram kernel KVH stays always the same, as we only added edges.
Results are plotted in Figure 3. There are two key observations. First, by adding new false edges to the graphs, the accuracy levels drop for both the random walk kernel and the histogram kernel. However, even after adding 100 new false edges per graph, they are both still better than a naive classifier that assigns all graphs to the same class (accuracy of 16.6 percent on ENZYMES and approximately 50 percent on NCI1 and NCI109). Second, the geometric random walk kernel quickly approaches the accuracy level of KH when new edges are added. This is a strong indicator that halting occurs. As graphs become denser, the upper bound for  gets smaller, and the accuracy of the geometric random walk kernel KGR rapidly drops and converges to KH. This result confirms Corollary 3, which says that both KGR and KH converge to KVH as  goes to 0.
4 Discussion
In this work, we show when and where the phenomenon of halting occurs in random walk kernels. Halting refers to the fact that similarity measures based on counting walks (of potentially infinite length) often downweight longer walks so much that the similarity score is completely dominated by walks of length 1, degenerating the random walk kernel to a simple kernel between edges and vertices. While it had been conjectured that this problem may arise in graph kernels [1], we provide the first theoretical proof and empirical demonstration of the occurrence and extent of halting in geometric random walk kernels.
We show that the difference between geometric random walk kernels and simple edge kernels depends on the maximum degree of the graphs being compared. With increasing maximum degree, the difference converges to zero. We empirically demonstrate on simulated graphs that the comparison of graphs with high maximum degrees suffers from halting. On real graph data from popular graph classification benchmark datasets, the maximum degree is so low that halting can be avoided if the decaying weight  is set close to its theoretical maximum. Still, if  is set conservatively to a low value to ensure convergence, halting can clearly be observed, even on unseen test graphs with unknown maximum degrees.
There is an interesting connection between halting and tottering [1, Section 2.1.5], a weakness of random walk kernels described more than a decade ago [11]. Tottering is the phenomenon that a walk of infinite length may go back and forth along the same edge, thereby creating an artificially inflated similarity score if two graphs share a common edge. Halting and tottering seem to be opposing effects. If halting occurs, the effect of tottering is reduced and vice versa. Halting downweights these tottering walks and counteracts the inflation of the similarity scores. An interesting point is that the strategies proposed to remove tottering from walk kernels did not lead to a clear improvement in classification accuracy [11], while we observed a strong negative effect of halting on the classification accuracy in our experiments (Section 3). This finding stresses the importance of studying halting.
Our theoretical and empirical results have important implications for future applications of random walk kernels. First, if the geometric random walk kernel is used on a graph dataset with known maximum degree,  should be close to the theoretical maximum. Second, simple baseline kernels based on vertex and edge label histograms should be employed to check empirically if the random walk kernel gives better accuracy results than these baselines. Third, particularly in datasets with high maximum degree, we advise using a fixed-length-k random walk kernel rather than a geometric random walk kernel. Optimizing the length k by cross validation on the training dataset led to competitive or superior results compared to the geometric random walk kernel in all of our experiments. Based on these results and the fact that by definition the fixed-length kernel does not suffer from halting, we recommend using the fixed-length random walk kernel as a comparison method in future studies on novel graph kernels.
Acknowledgments. This work was supported by JSPS KAKENHI Grant Number 26880013 (MS), the Alfried Krupp von Bohlen und Halbach-Stiftung (KB), the SNSF Starting Grant `Significant Pattern Mining' (KB), and the Marie Curie Initial Training Network MLPM2012, Grant No. 316861 (KB).
8

References
[1] Borgwardt, K. M. Graph Kernels. PhD thesis, Ludwig-Maximilians-University Munich, 2007. [2] Borgwardt, K. M., Ong, C. S., Schonauer, S., Vishwanathan, S. V. N., Smola, A. J., and Kriegel,
H.-P. Protein function prediction via graph kernels. Bioinformatics, 21(suppl 1):i47-i56, 2005. [3] Brualdi, R. A. The Mutually Beneficial Relationship of Graphs and Matrices. AMS, 2011. [4] Costa, F. and Grave, K. D. Fast neighborhood subgraph pairwise distance kernel. In Proceed-
ings of the 27th International Conference on Machine Learning (ICML), 255-262, 2010. [5] Gartner, T., Flach, P., and Wrobel, S. On graph kernels: Hardness results and efficient alterna-
tives. In Learning Theory and Kernel Machines (LNCS 2777), 129-143, 2003. [6] Girvan, M. and Newman, M. E. J. Community structure in social and biological networks.
Proceedings of the National Academy of Sciences (PNAS), 99(12):7821-7826, 2002. [7] Kashima, H., Tsuda, K., and Inokuchi, A. Marginalized kernels between labeled graphs. In
Proceedings of the 20th International Conference on Machine Learning (ICML), 321-328, 2003. [8] Katz, L. A new status index derived from sociometric analysis. Psychometrika, 18(1):39-43, 1953. [9] Kriege, N., Neumann, M., Kersting, K., and Mutzel, P. Explicit versus implicit graph feature maps: A computational phase transition for walk kernels. In Proceedings of IEEE International Conference on Data Mining (ICDM), 881-886, 2014. [10] Liben-Nowell, D. and Kleinberg, J. The link-prediction problem for social networks. Journal of the American Society for Information Science and Technology, 58(7):1019-1031, 2007. [11] Mahe, P., Ueda, N., Akutsu, T., Perret, J.-L., and Vert, J.-P. Extensions of marginalized graph kernels. In Proceedings of the 21st International Conference on Machine Learning (ICML), 2004. [12] Shervashidze, N. and Borgwardt, K. M. Fast subtree kernels on graphs. In Advances in Neural Information Processing Systems (NIPS) 22, 1660-1668, 2009. [13] Shervashidze, N., Schweitzer, P., van Leeuwen, E. J., Mehlhorn, K., and Borgwardt, K. M. Weisfeiler-Lehman graph kernels. Journal of Machine Learning Research, 12:2359-2561, 2011. [14] Vishwanathan, S. V. N., Schraudolph, N. N., Kondor, R., and Borgwardt, K. M. Graph kernels. Journal of Machine Learning Research, 11:1201-1242, 2010.
9

