Subspace Clustering with Irrelevant Features via Robust Dantzig Selector

Chao Qu Department of Mechanical Engineering
National University of Singapore
A0117143@u.nus.edu

Huan Xu Department of Mechanical Engineering
National University of Singapore
mpexuh@nus.edu.sg

Abstract
This paper considers the subspace clustering problem where the data contains irrelevant or corrupted features. We propose a method termed "robust Dantzig selector" which can successfully identify the clustering structure even with the presence of irrelevant features. The idea is simple yet powerful: we replace the inner product by its robust counterpart, which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features. We establish theoretical guarantees for the algorithm to identify the correct subspace, and demonstrate the effectiveness of the algorithm via numerical simulations. To the best of our knowledge, this is the first method developed to tackle subspace clustering with irrelevant features.

1 Introduction
The last decade has witnessed fast growing attention in research of high-dimensional data: images, videos, DNA microarray data and data from many other applications all have the property that the dimensionality can be comparable or even much larger than the number of samples. While this setup appears ill-posed in the first sight, the inference and recovery is possible by exploiting the fact that high-dimensional data often possess low dimensional structures [3, 14, 19]. On the other hand, in this era of big data, huge amounts of data are collected everywhere, and such data is generally heterogeneous. Clean data and irrelevant or even corrupted information are often mixed together, which motivates us to consider the high-dimensional, big but dirty data problem. In particular, we study the subspace clustering problem in this setting.
Subspace clustering is an important subject in analyzing high-dimensional data, inspired by many real applications[15]. Given data points lying in the union of multiple linear spaces, subspace clustering aims to identify all these linear spaces, and cluster the sample points according to the linear spaces they belong to. Here, different subspaces may correspond to motion of different objects in video sequence [11, 17, 20], different rotations, translations and thickness in handwritten digit or the latent communities for the social graph [15, 5].
A variety of algorithms of subspace clustering have been proposed in the last several years including algebraic algorithms [16], iterative methods [9, 1], statistical methods [11, 10], and spectral clustering-based methods [6, 7]. Among them, sparse subspace clustering (SSC) not only achieves state-of-art empirical performance, but also possesses elegant theoretical guarantees. In [12], the authors provide a geometric analysis of SSC which explains rigorously why SSC is successful even when the subspaces are overlapping [12]. [18] and [13] extend SSC to the noisy case, where data are contaminated by additive Gaussian noise. Different from these work, we focus on the case where some irrelevant features are involved.
1

Mathematically, SSC indeed solves for each sample a sparse linear regression problem with the dictionary being all other samples. Many properties of sparse linear regression problem are well understood in the clean data case. However, the performance of most standard algorithms deteriorates (e.g. LASSO and OMP) even only a few entries are corrupted. As such, it is well expected that standard SSC breaks for subspace clustering with irrelevant or corrupted features (see Section 5 for numerical evidences). Sparse regression under corruption is a hard problem, and few work has addressed this problem [8][21] [4]. Our contribution: Inspired by [4], we use a simple yet powerful tool called robust inner product and propose the robust Dantzig selector to solve the subspace clustering problem with irrelevant features. While our work is based upon the robust inner product developed to solve robust sparse regression, the analysis is quite different from the regression case since both the data structures and the tasks are completely different: for example, the RIP condition - essential for sparse regression - is hardly satisfied for subspace clustering [18]. We provide sufficient conditions to ensure that the Robust Dantzig selector can detect the true subspace clustering. We further demonstrate via numerical simulation the effectiveness of the proposed method. To the best of our knowledge, this is the first attempt to perform subspace clustering with irrelevant features.
2 Problem setup and method
2.1 Notations and model
The clean data matrix is denoted by XA  RDxN , where each column corresponds to a data point, normalized to a unit vector. The data points are lying on a union of L subspace S = Ll=1Sl. Each subspace Sl is of dimension dl which is smaller than D and contains Nl data samples with N1+N2+* * *+NL = N . We denote the observed dirty data matrix by X  R(D+D1)xN . Out of the D + D1 features, up to D1 of them are irrelevant. Without loss of generality, let X = [XOT , XAT ]T , where XO  RD1xN denotes the irrelevant data. The subscript A and O denote the set of row indices corresponding to true and irrelevant features and the superscript T denotes the transpose. Notice that we do not know O a priori except its cardinality is D1. The model is illustrated in Figure 1. Let XA(l)  RDxNl denote the selection of columns in XA that belongs to Sl. Similarly, denote the corresponding columns in X by X(l). Without loss of generality, let X = [X(1), X(2), ..., X(L)] be ordered. Further more, we use the subscript "-i"to describe a matrix that excludes the column i, e.g., (XA)(-l)i = [(xA)(1l), ..., (xA)(i-l)1, (xA)(i+l)1, ..., (xA)(Nl)l ]. We use the superscript lc to describe a matrix that excludes column in subspace l, e.g., (XA)lc = [XA(1), ..., XA(l-1), XA(l+1), ..., XA(L)]. For a matrix , we use s, to denote the submatrix with row indices in set s and column indices in set . For any matrix Z, P (Z) denotes the symmetrized convex hull of its column, i.e., P (Z) = conv(z1, z2, ...., zN ) . We define P-l i := P ((XA)(-l)i) for simplification, i.e., the symmetrized convex hull of clean data in subspace l except data i. Finally we use * 2 to denote the l2 norm of a vector and *  to denote infinity norm of a vector or a matrix. Caligraphic letters such as X , Xl represent the set containing all columns of the corresponding clean data matrix.
Figure 1: Illustration of the model of irrelevant features in the subspace clustering problem. The left one is the model addressed in this paper: Among total D + D1 features, up tp D1 of them are irrelevant. The right one illustrates a more general case, where the value of any D1 element of each column can be arbitrary (e.g., due to corruptions). It is a harder case and left for future work.
2

Figure 2: Illustration of the Subspace Detection Property. Here, each figure corresponds to a matrix where each column is ci, and non-zero entries are in white. The left figure satisfies this property. The right one does not.

2.2 Method

In this secion we present our method as well as the intuition that derives it. When all observed data are clean, to solve the subspace clustering problem, the celebrated SSC [6] proposes to solve the following convex programming

min ci 1 s.t. xi = X-ici,
ci

(1)

for each data point xi. When data are corrupted by noise of small magnitude such as Gaussian noise, a straightforward extension of SSC is the Lasso type method called "Lasso-SSC" [18, 13]

min
ci

ci

 1+ 2

xi - X-ici

2 2

.

(2)

Note that while Formulation (2) has the same form as Lasso, it is used to solve the subspace clustering task. In particular, the support recovery analysis of Lasso does not extend to this case, as X-i typically does not satisfy the RIP condition [18].

This paper considers the case where X contains irrelevant/gross corrupted features. As we discussed above, Lasso is not robust to such corruption. An intuitive idea is to consider the following formulation first proposed for sparse linear regression [21].

min
ci ,E

ci

 1+ 2

xi - (X-i - E)ci

2 2

+



E

,

(3)

where *  is some norm corresponding to the sparse type of E. One major challenge of this formulation is that it is not convex. As such, it is not clear how to efficiently find the optimal solution, and how to analyze the property of the solution (typically done via convex analysis) in the subspace clustering task.

Our method is based on the idea of robust inner product. The robust inner product a, b k is defined as follows: For vector a  RD, b  RD, we compute qi = aibi, i = 1, ..., N . Then {|qi|} are sorted and the smallest (D - k) are selected. Let  be the set of selected indices, then a, b k = i qi, i.e., the largest k terms are truncated. Our main idea is to replace all inner products involved by
robust counterparts a, b D1 , where D1 is the upper bound of the number of irrelevant features. The intuition is that the irrelevant features with large magnitude may affect the correct subspace
clustering. This simple truncation process will avoid this. We remark that we do not need to know
the exact number of irrelevant feature, but instead only an upper bound of it.

Extending (2) using the robust inner product leads the following formulation:

min
ci

ci

1

+

 2

cTi

 ci

-

T

ci,

(4)

where  and  are robust counterparts of X-TiX-i and X-Tixi. Unfortunately,  may not be a positive semidefinite matrix, thus (4) is not a convex program. Unlike the work [4][8] which studies

3

non-convexity in linear regression, the difficulty of non-convexity in the subspace clustering task appears to be hard to overcome.

Instead we turn to the Dantzig Selector, which is essentially a linear program (and hence no positive semidefiniteness is required):

min
ci

ci

1 +  X-Ti(X-ici - xi) .

(5)

Replace all inner product by its robust counterpart, we propose the following Robust Dantzig Selector, which can be easily recast as a linear program:

Robust Dantzig Selector:

min ci 1 +   ci -  ,
ci

(6)

Subspace Detection Property: To measure whether the algorithm is successful, we define the
criterion Subspace Detection Property following [18]. We say that the Subspace Detection Property holds, if and only if for all i, the optimal solution to the robust Dantzig Selector satisfies (1) Non-triviality: ci is not a zero vector; (2) Self-Expressiveness Property: nonzeros entries of ci correspond to only columns of X sampled from the same subspace as xi. See Figure 2 for illustrations.

3 Main Results

To avoid repetition and cluttered notations, we denote the following primal convex problem by

P (, )

min
c

c

1+

c - 

.

Its dual problem, denoted by D(, ), is

max ,  subject to  1 =     1.


(7)

Before we presents our results, we define some quantities.

The dual direction is an important geometric term introdcued in analyzing SSC [12]. Here we define

similarly the dual direction of the robust Dantzig selector: Notice that the dual of robust Dantzig

problem is D( , ), where  and  are robust counterparts of X-Tixi and X-TiX-i respectively
(recall that X-i and xi are the dirty data). We decompose  into two parts  = (XA)T-i(XA)-i + , where the first term corresponds to the clean data, and the second term is due to the irrelevant features

and truncation from the robust inner product. Thus, the second constraint of the dual problem

becomes ((XA)T-i(XA)-i +  )   1. Let  be the optimal solution to the above optimization

problem, we define v(xi, X-i, ) := (XA)-i and the dual direction as vl =

.v (xli ,X-(li) ,)
v(xli,X-(li),) 2

Similarly as SSC [12], we define the subspace incoherence. Let V l = [v1l , v2l , ..., vNl l ]. The incoherence of a point set Xl to other clean data points is defined as (Xl) = maxk:k=l (XA(k))T V l .
Recall that we decompose  and  as  = (XA)T-i(XA)-i +  and  = (XA)T-i(xA)i + . Intuitively, for robust Dantzig selecter to succeed, we want  and  not too large. Particularly, we assume (xA)i   1 and (XA)-i   2.
Theorem 1 (Deterministic Model). Denote l := (Xl), rl := mini:xiXl r(P-l i), r := minl=1,...,L rl and suppose l < rl for all l. If

r2 - 4D1

1 1 2r

- 2D1

2 2

<

min
l

rl

2D1

2 2

- ul (ul +

rl

)

,

(8)

then the subspace detection property holds for all  in the range

r2

- 4D1

1 1 2r - 2D1

2 2

<



<

min
l

rl - ul 2D1 22(ul +

rl

)

.

(9)

4

In an ideal case when D1 = 0, the condition of the upper bound of  reduces to rl > ul, similar to the condition for SSC in the noiseless case [12].

Based on Condition (8), under a randomized generative model, we can derive how many irrelevant features can be tolerated.

Theorem 2 (Random model). Suppose there are L subspaces and for simplicity, all subspaces have

same dimension d and are chosen uniformly at random. For each subspace there are d + 1 points

chosen independently and uniformly at random. Up to D1 features of data are irrelevant. Each

data point (including true and irrelevant features) is independent from other data points. Then for

s1o-meN4u-nivNerseaxlpc(-onstadn)tsifC1, C2, the subspace detection property holds with probability at least

d



Dc2() log() ,

12 log N

and

1 2

c2

()

log d



-

 ( 2c()

1

1-

D

<<

,

log  d

+

1)

C1 D1 (log

D+C2 D

log

N

)

1 +  C1D1(log D + C2 log N )

where  =

12d log N Dc2() log 

;

c()

is

a

constant

only

depending

on

the

density

of

data

points

on

subspace and satisfies (1) c() >0 for all  > 1, (2) there is a numerical value 0, such that for all  > 0, one can take c() = 1/ 8.

Simplifying the above conditions, we can determine the number of irrelevant features that can be tolerated. In particular, if d  2c2() log  and we choose the  as

4d  = c2() log  ,

then the maximal number of irrelevant feature D1 that can be torelated is

D1

=

c()D min{
8C1d(log(D)

log  + C2

log

N)

,

1 1

- +

 

C0Dc2() log  C1d(log(D) + C2 log

}, N)

with

probability

at

least

1

-

4 N

-N

exp(-d).

If d  2c2() log , and we choose the same , then the number of irrelevant feature we can tolerate is

D1

=

min{

Dc()  4 2C1(log(D)

log  d
+ C2

log

, N)

1 1

- +

 

C0Dc2() log  C1d(log(D) + C2 log

}, N)

with

probability

at

least

1

-

4 N

-N

exp(-d).

Remark 1. If D is much larger than D1, the lower bound of  is proportional to the subspace

dimension

d.

When d increases,

the upper

bound

of



decreases,

since

1- 1+

decreases.

Thus the

valid range of  shrinks when d increases.

Remark 2. Ignoring the logarithm terms, when d is large, the tolerable D1 is proportional to

min(C1

1- 1+

D d

,

C2

D d

).

When

d

is

small,

D1

is

proportional

to

min(C1

1- 1+

D d

,

C2D/

d) .

4 Roadmap of the Proof

In this section, we lay out the roadmap of proof. In specific we want to establish the condition with the number of irrelevant features, and the structure of data (i.e., the incoherence  and inradius r) for the algorithm to succeed. Indeed, we provide a lower bound of  such that the optimal solution ci is not trivial; and an upper bound of  so that the Self-Expressiveness Property holds. Combining them together established the theorems.

5

4.1 Self-Expressiveness Property

The Self-Expressiveness Property is related to the upper bound of . The proof technique is inspired by [18] and [12], we first establish the following lemma, which provides a sufficient condition such that Self-Expressiveness Property holds of the problem 6.
Lemma 1. Consider a matrix   RNxN and   RNx1, If there exist a pair (c, ) such that c has a support S  T and

sgn(cs) + s, = 0, scT,   1,  1 = , T c,  < 1,

(10)

where  is the set of indices of entry i such that |(c - )i| = c -  , then for all optimal solution c to the problem P (, ), we have cT c = 0.

The variable  in Lemma 1 is often termed the "dual certificate". We next consider an oracle problem P ( l,l, l), and use its dual optimal variable denoted by , to construct such a dual certificate. This candidate satisfies all conditions in the Lemma 1 automatically except to show

 lc,  < 1,

(11)

where lc denotes the set of indices expect the ones corresponding to subspace l. We can compare this condition with the corresponding one in analyzing SSC, in which one need (X)(lc)T v  < 1, where v is the dual certificate. Recall that we can decompose  lc, = (XA)(lc)T (XA) +  lc,. Thus Condition 11 becomes

(XA)(lc)T ((XA)) +  lc,  < 1.

(12)

To show this holds, we need to bound two terms (XA) 2 and  lc, .

Bounding  ,  

The following lemma relates D1 with   and  .

Lemma 2. Suppose  and  are robust counterparts of X-TiX-i and X-Tixi respectively and among

D + D1 features, up to D1 are irrelevant. We can decompose  and  into following form  =

(XA)T-i(XA)-i +  and  = (XA)T-i(xA)i + . We define 1 :=   and 2 :=   .If

(xA)i  

1 and

(XA)-i  

2, then 2



2D1

2 2

,

1



2D1

1

2.

We then bound 1 and 2 in the random model using the upper bound of the spherical cap [2]. Indeed we have 1  C1(log D + C2 log N )/ D and 2  C1(log D + C2 log N )/ D with high probability.

Bounding X 2

By exploiting the feasible condition in the dual of the oracle problem, we obtain the following

bound:

X

2

1 + 2D1 r(P-l i)

2
2.

Furthermore, r(P-l i)

can

be

lower

bound

by

c() 2

log  d

and

2 can be upper bounded by C1(log D+

C2 log N )/ D in the random model with high probability. Thus the RHS can be upper bounded.

Plugging this upper bound into (12), we obtain the upper bound of .

4.2 Non-triviality with sufficiently large  To ensure that the solution is not trivial (i.e., not all-zero), we need a lower bound on .

6

If  satisfies the following condition, the optimal solution to problem 6 can not be zero

1

>

r2(P-l i) - 2D1

2 2

-

4r(P-l i)D1

1

.
2

(13)

The proof idea is to show when  is large enough, the trivial solution c = 0 can not be optimal. In
particular, if c = 0, the corresponding value in the primal problem is  l . We then establish a lower bound of l  and a upper bound of c 1 +   l,lc - l  so that the following inequality always holds by some carefully choosen c.

c 1 +   l,lc - l  <  l .

(14)

We then further lower bound the RHS of Equation (13) using the bound of 1, 2 and r(P-l i). Notice that condition (14) requires that  > A and condition (11) requires  < B, where A and B are some
terms depending on the number of irrelevant features. Thus we require A < B to get the maximal
number of irrelevant features that can be tolerated.

5 Numerical simulations

In this section, we use three numerical experiments to demonstrate the effectiveness of our method to handle irrelevant/corrupted features. In particular, we test the performance of our method and effect of number of irrelevant features and dimension subspaces d with respect to different . In all experiments, the ambient dimension D = 200, sample density  = 5, the subspace are drawn uniformly at random. Each subspace has d+1 points chosen independently and uniformly random. We measure the success of the algorithms using the relative violation of the subspace detection property defined as follows,

RelV iolation(C, M) =

(i,j)/M |C|i,j , (i,j)M |C|i,j

where C = [c1, c2, ..., cN ], M is the ground truth mask containing all (i, j) such that xi, xj belong to a same subspace. If RelV iolation(C, M) = 0, then the subspace detection property is satisfied.
We also check whether we obtain a trivial solution, i.e., if any column in C is all-zero.

We first compare the robust Dantzig selector( = 2) with SSC and LASSO-SSC (  = 10). The results are shown in Figure 3. The X-axis is the number of irrelevant features and the Y-axis is the Relviolation defined above. The ambient dimension D = 200, L = 3, d = 5, the relative sample density  = 5. The values of irrelevant features are independently sampled from a uniform distribution in the region [-2.5, 2.5] in (a) and [-10, 10] in (b). We observe from Figure 3 that both SSC and Lasso SSC are very sensitive to irrelevant information. (Notice that RelViolation=0.1 is pretty large and can be considered as clustering failure.) Compared with that, the proposed Robust Dantzig Selector performs very well. Even when D1 = 20, it still detects the true subspaces perfectly. In the same setting, we do some further experiments, our method breaks when D1 is about 40. We also do further experiment for Lasso-SSC with different  in the supplementary material to show Lasso-SSC is not robust to irrelevant features.

We also examine the relation of  to the performance of the algorithm. In Figure 4a, we test the subspace detection property with different  and D1. When  is too small, the algorithm gives a trivial solution (the black region in the figure). As we increase the value of , the corresponding solutions satisfy the subspace detection property (represented as the white region in the figure). When  is larger than certain upper bound, RelV iolation becomes non-zero, indicating errors in subspace clustering. In Figure 4b, we test the subspace detection property with different  and d. Notice we rescale  with d, since by Theorem 3,  should be proportional to d. We observe that the valid region of  shrinks with increasing d which matches our theorem.

6 Conclusion and future work
We studied subspace clustering with irrelevant features, and proposed the "robust Dantzig selector" based on the idea of robust inner product, essentially a truncated version of inner product to avoid

7

Original SSC Lasso SSC 1 Robust Dantzig Selector
0.8

Original SSC Lasso SSC 1 Robust Dantzig Selector
0.8

RelViolation RelViolation

0.6 0.6

0.4 0.4

0.2 0.2

0 0 5 10 15 20 Number of irrelevant features
(a)

0 0 5 10 15 20 Number of irrelevant features
(b)

Figure 3: Relviolation with different D1. Simulated with D = 200, d = 5, L = 3,  = 5,  = 2, and D1 from 1 to 20.

 /d

10.5 10 9.5 9 8.5 8 7.5 7 6.5 6 5.5 5 4.5 4 3.5 3 2.5 2 1.5 1
1 2 3 4 5 6 7 8 9 10 Number of irrelevant features D1

2.5 2.3 2.1 1.9 1.7 1.5 1.3 1.1 0.9 0.7 0.5 0.3 0.1
4

6 8 10 12 14 Subspace dimension d

16

(a) Exact recovery with different number of (b) Exact recovery with different subspace

irrelevant features. Simulated with D = dimension d. Simulated with D = 200,

200, d = 5, L = 3,  = 5 with an in- L = 3,  = 5, D1 = 5 and an increas-

creasing D1 from 1 to 10. Black region: ing d from 4 to 16. Black region: triv-

trivial solution. White region: Non-trivial ial solution. White region: Non-trivial so-

solution with RelViolation=0. Gray region: lution with RelViolation=0. Gray region:

RelViolation> 0.02.

RelViolation> 0.02.

Figure 4: Subspace detection property with different , D1, d.

any single entry having too large influnce on the result. We established the sufficient conditions for the algorithm to exactly detect the true subspace under the deterministic model and the random model. Simulation results demonstrate that the proposed method is robust to irrelevant information whereas the performance of original SSC and LASSO-SSC significantly deteriorates. We now outline some directions of future research. An immediate future work is to study theoretical guarantees of the proposed method under the semi-random model, where each subspace is chosen deterministically, while samples are randomly distributed on the respective subspace. The challenge here is to bound the subspace incoherence, previous methods uses the rotation invariance of the data, which is not possible in our case as the robust inner product is invariant to rotations.
Acknowledgments
This work is partially supported by the Ministry of Education of Singapore AcRF Tier Two grant R-265-000-443-112, and A*STAR SERC PSF grant R-265-000-540-305.
References
[1] Pankaj K Agarwal and Nabil H Mustafa. k-means projective clustering. In Proceedings of the 23rd ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages
8

155-165, 2004.
[2] Keith Ball. An elementary introduction to modern convex geometry. Flavors of geometry, 31:1-58, 1997.
[3] Emmanuel J Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal of the ACM, 58(3):11, 2011.
[4] Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial corruption. In Proceedings of the 30th International Conference on Machine Learning, pages 774-782, 2013.
[5] Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs via convex optimization. The Journal of Machine Learning Research, 15(1):2213-2238, 2014.
[6] Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering. In CVPR 2009, pages 2790- 2797.
[7] Guangcan Liu, Zhouchen Lin, and Yong Yu. Robust subspace segmentation by low-rank representation. In Proceedings of the 27th International Conference on Machine Learning, pages 663-670, 2010.
[8] Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. In Advances in Neural Information Processing Systems, pages 2726-2734, 2011.
[9] Le Lu and Rene Vidal. Combined central and subspace clustering for computer vision applications. In Proceedings of the 23rd international conference on Machine learning, pages 593-600, 2006.
[10] Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data via lossy data coding and compression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(9):1546-1562, 2007.
[11] Shankar R Rao, Roberto Tron, Rene Vidal, and Yi Ma. Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories. In CVPR 2008.
[12] Mahdi Soltanolkotabi, Emmanuel J Candes, et al. A geometric analysis of subspace clustering with outliers. The Annals of Statistics, 40(4):2195-2238, 2012.
[13] Mahdi Soltanolkotabi, Ehsan Elhamifar, Emmanuel J Candes, et al. Robust subspace clustering. The Annals of Statistics, 42(2):669-699, 2014.
[14] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society. Series B, pages 267-288, 1996.
[15] Rene Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine, 28(2):52- 68, 2010.
[16] Rene Vidal, Yi Ma, and Shankar Sastry. Generalized principal component analysis (gpca). IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(12):1945-1959, 2005.
[17] Rene Vidal, Roberto Tron, and Richard Hartley. Multiframe motion segmentation with missing data using powerfactorization and gpca. International Journal of Computer Vision, 79(1):85- 105, 2008.
[18] Yu-Xiang Wang and Huan Xu. Noisy sparse subspace clustering. In Proceedings of The 30th International Conference on Machine Learning, pages 89-97, 2013.
[19] H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. IEEE Transactions on Information Theory, 58(5):3047-3064, 2012.
[20] Jingyu Yan and Marc Pollefeys. A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate. In ECCV 2006, pages 94-106. 2006.
[21] Hao Zhu, Geert Leus, and Georgios B Giannakis. Sparsity-cognizant total least-squares for perturbed compressive sampling. IEEE Transactions on Signal Processing, 59(5):2002-2016, 2011.
9

