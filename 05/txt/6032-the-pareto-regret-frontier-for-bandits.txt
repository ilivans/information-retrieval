The Pareto Regret Frontier for Bandits

Tor Lattimore Department of Computing Science
University of Alberta, Canada tor.lattimore@gmail.com

Abstract
Given a multi-armed bandit problem it may be desirable to achieve a smallerthan-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least (nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors.

1 Introduction

The multi-armed bandit is the simplest class of problems that exhibit the exploration/exploitation dilemma. In each time step the learner chooses one of K actions and receives a noisy reward signal for the chosen action. A learner's performance is measured in terms of the regret, which is the (expected) difference between the rewards it actually received and those it would have received (in expectation) by choosing the optimal action.

Prior work on the regret criterion for finite-armed bandits has treated all actions uniformly and has aimed for bounds on the regret that do not depend on which action turned out to be optimal. I take a different approach and ask what can be achieved if some actions are given special treatment. Focussing on worst-case bounds, I ask whether or not it is possible to achieve improved worst-case regret for some actions, and what is the cost in terms of the regret for the remaining actions. Such results may be useful in a variety of cases. For example, a company that is exploring some new strategies might expect an especially small regret if its existing strategy turns out to be (nearly) optimal.

This problem has previously been considered in the experts setting where the learner is allowed

to observe the reward for all actions in every round, not only for the action actually chosen. The

earliest a prior

work seems to weight to each

be by Hutter and Poland [2005] where action and pays a worst-case regret of

it isshown that O( -n log i)

the for

learner expert

can assign i where i

is the 1/K ,

pwrhioicrhbeleliaedfsintoexthpeerwt ielaln-kdnnowisnthOe(honrizloogn.KT)hbeouunnidfoarmchireevgeredt

is by

obtained by choosing i = the exponential weighting

algorithm [Cesa-Bianchi, 2006]. The consequence of this is that an algorithm can enjoy a constant

regret with respect to a single action while suffering minimally on the remainder. The problem was

studied in more detail by Koolen [2013] where (remarkably) the author was able to exactly describe

the pareto regret frontier when K = 2.

Other related work (also in the experts setting) is where the objective is to obtain an improved regret against a mixture of available experts/actions [Even-Dar et al., 2008, Kapralov and Panigrahy, 2011]. In a similar vain, Sani et al. [2014] showed that algorithms for prediction with expert advice can be combined with minimal cost to obtain the best of both worlds. In the bandit setting I am only aware

1

of the work by Liu and Li [2015] who study the effect of the prior on the regret of Thompson sampling in a special case. In contrast the lower bound given here applies to all algorithms in a relatively standard setting.

The main contribution of this work is a characterisation of the pareto regret frontier (the set of achievable worst-case regret bounds) for stochastic bandits.

Let i  R be the unknown mean of the ith arm and assume that supi,j i - j  1. In each time

step the learner chooses an action It is the noise term that I assume to be

 {1, . . sampled

i.n,dKep}eannddenrtelcyefirvoems raew1a-srudbggIatu,tss=ianidi+stribt uwtihoenrethatt

may depend on It. This model subsumes both Gaussian and Bernoulli (or bounded) rewards. Let  be a bandit strategy, which is a function from histories of observations to an action It. Then the n-step expected pseudo regret with respect to the ith arm is

n
R,i = ni - E It ,
t=1
where the expectation is taken with respect to the randomness in the noise and the actions of the policy. Throughout this work n will be fixed, so is omitted from the notation. The worst-case expected pseudo-regret with respect to arm i is

Ri = sup R,i .


(1)

This means that R  RK is a vector of worst-case pseudo regrets with respect to each of the arms. Let B  RK be a set defined by

  

B

=

 B




[0, n]K

:

Bi



 min n,


j=i

n Bj 

 for all i


.

(2)

The boundary of B is denoted by B. The following theorem shows that B describes the pareto regret frontier up to constant factors.

Theorem There exist universal constants c1 = 8 and c2 = 252 such that:
Lower bound: for t  N (0, 1) and all strategies  we have c1(R + K)  B Upper bound: for all B  B there exists a strategy  such that Ri  c2Bi for all i

Observe that the lower bound relies on the assumption that the noise term be Gaussian while the upper bound holds for subgaussian noise. The lower bound may be generalised to other noise models such as Bernoulli, but does not hold for all subgaussian noise models. For example, it does not hold if there is no noise (t = 0 almost surely). The lower bound also applies to the adversarial framework where the rewards may be chosen arbitrarily. Although I was not able to derive a matching upper bound in this case, a simple modification of the Exp- algorithm [Bubeck and Cesa-Bianchi, 2012] leads to an algorithm with

R1  B1 and Rk

nK B1

log

nK B12

for all k  2 ,

where the regret is the adversarial version of the expected regret. Details are in the supplementary

material.

The new results seem elegant, but disappointing. In the experts setting we have seen that the learner can distribute a prior amongst the actions and obtain a bound on the regret depending in a natural way on the prior weight of the optimal action. In contrast, in the bandit setting the learner pays an enormously higher price to obtain a small regret with respect to even a single arm. In fact, the learner must essentially choose a single arm to favour, after which the regret for the remaining arms has very limited flexibility. Unlike in the experts setting, if even a single arm enjoys constant worst-case regret, then the worst-case regret with respect to all other arms is necessarily linear.

2

2 Preliminaries

I use the same notation as Bubeck and Cesa-Bianchi [2012]. Define Ti(t) to be the number of times

action i has been chosen after time step t and i,s to be the empirical estimate of i from the first s

times action i the tth round.

wI uassesathmepcloendv. eTnhtiiosnmtehaantstih,0at=i0,T. iS(ti-nc1e)

is the empirical the noise model

estimate of i at is 1-subgaussian

the we

start of have

 > 0

P {s  t : i,s - i  /s}  exp

-

2 2t

.

(3)

This result is presumably well known, but a proof is included in the supplementary material for

convenience. The optimal arm is i optimal reward is  = maxi i. The

=gaparbgetmwaexeni thi ewmitehantiersewbarordksenofinthesojmtheaarrmbiatrnadrythweaoyp.tiTmhael

arm is j =  - j and ji = i - j. The vector of worst-case regrets is R  RK and has

been defined already in Eq. (1). I write R  B  RK vector R and x  R we have (R + x)i = Ri + x.

if

Ri



Bi

for

all

i



{1, . . . , K}.

For

3 Understanding the Frontier

Before proving the main theorem I briefly describe the features of the regret frontier. First notice that if Bi = n(K - 1) for all i, then

Bi =

n(K - 1) =
j=i

n/(K

-

1)

=

j=i

n Bj

.

Thus B  B as expected. This particular B is witnessed up to constant factors by MOSS [Audibert aRniudcbBube(ck,n2K00l9o]gann)d. OC-UCB [Lattimore, 2015], but not UCB [Auer et al., 2002], which suffers Of course the uniform choice of B is not the only option. Suppose the first arm is special, so B1 should be chosen especially small. Assume without loss of generality that B1  B2  . . .  BK  n. Then by the main theorem we have

Therefore

B1



K i=2

n Bi



k i=2

n Bi



(k - 1)n Bk

.

Bk



(k

- 1)n B1

.

(4)

This also proves the claim in the abstract, since it implies that BK  (K - 1)n/B1. If B1 is fixed, then choosing Bk = (k - 1)n/B1 does not lie on the frontier because

K k=2

n Bk

=

K k=2

B1 k-1



(B1 log K)

However, if H = the frontier and is

a

fKka=ct2o1r /o(fklo-g

1) K

 (log K), then choosing away from the lower bound

Bk = (k - 1)nH/B1 does lie on given in Eq. (4). Therefore up the

a log K factor, points on the regret frontier are characterised entirely by a permutation determining

the order of worst-case regrets and the smallest worst-case regret.

Perhaps the most natural choice of B (assuming again that B1  . . .  BK ) is

B1 = np and Bk = (k - 1)n1-pH for k > 1 .

OFoCr-pU=CB1/w2htihleisbleeiandgsatofaactbooruonfdthKat

is

at

most

 K

log

K

worse

better for a select few.

than

that

obtained

by

MOSS

and

3

Assumptions

The assumption that i  [0, 1] is used to avoid annoying boundary problems caused by the fact that

time is discrete. This means that if i is extremely large, then even a single sample from this arm can

coafuse(a Kbign)recglreeatrlbyoduoneds.

This assumption is already quite common, for example a worst-case regret not hold if the gaps are permitted to be unbounded. Unfortunately there is

no perfect resolution to this annoyance. Most elegant would be to allow time to be continuous with

actions taken up to stopping times. Otherwise you have to deal with the discretisation/boundary

problem with special cases, or make assumptions as I have done here.

4 Lower Bounds

Theorem 1. Assume t  N (0, 1) is sampled from a standard Gaussian. Let  be an arbitrary strategy, then 8(R + K)  B.

Proof. Assume without re-order the actions). If c = 4 and define

Rlo1ss>ofng/e8n,etrhaelintythtehartesRu1lt

= is

trmiviinail.RFiro(mif

this is not the case, now on assume R1

then simply  n/8. Let

k = min

1 2

,

cRk n



1 2

.

Define K vectors 1, . . . , K  RK by

(k )j

=

1 2

0  + k -j

if j = 1 if j = k = 1 otherwise .

Therefore the optimal action for the bandit with means k is k. A = {k : k / A} and assume k  A. Then

Let A

=

{k : Rk  n/8} and



(a) (b)
Rk  Rk,k  kEk 

Tj (n) (=c) k

n - Ek Tk(n)

(=d) cRk(n - Ek Tk(n)) , n

j=k

where (a) follows the means of the

ksitnhcearRmkainsdthaenwy oortshte-craasremreigsreatt

with respect to least k (Note

arm that

k, (b) since this is also

the gap between true for k = 1

since 1 = mink k. (c) follows from the fact that Therefore

i Ti(n) = n and (d) from the definition of k.

n

1

-

1 c

 Ek Tk(n) .

(5)

Therefore for k = 1 with k  A we have

n

1

-

1 c

(a)
 Ek Tk(n)  E1 Tk(n) + nk

E1 Tk(n)

(b)
 n - E1 T1(n) + nk

E1 Tk(n)

(c)


n c

+ nk

E1 Tk(n) ,

where (a) follows from standard entropy inequalities and a similar argument as used by Auer et al.

[1995] by Eq.

(details in supplementary (5). Therefore

material),

(b)

since

k

=

1

and

E1 T1(n)

+

E1 Tk(n)



n,

and

(c)

which implies that

E1 Tk(n)



1

-

2 c

2k

,

R1



R1,1

=

K
kE1 Tk(n)
k=2



kA-{1}

1

- k

2 c

=

1n 8 kA-{1} Rk

.

4

Therefore for all i  A we have

8Ri



kA-{1}

n Rk

*

Ri R1



kA-{i}

n Rk

.

Therefore

8Ri

+ 8K



k=i

n Rk

+

8K

-

kA

-{i}

n Rk



k=i

n Rk

,

which implies that 8(R + K)  B as required.

5 Upper Bounds
I now show that the lower bound derived in the previous section is tight up to constant factors. The algorithm is a generalisation MOSS [Audibert and Bubeck, 2009] with two modifications. First, the width of the confidence bounds are biased in a non-uniform way, and second, the upper confidence bounds are shifted. The new algorithm is functionally identical to MOSS in the special case that Bi is uniform. Define log+(x) = max {0, log(x)}.

1: Input: n and B1, . . . , BK

2: 3:

ni = for t

n2/Bi2  1, . . .

for ,n

all do

i

4: It = arg max i,Ti(t-1) +
i
5: end for

4 Ti(t -

1)

log+

ni Ti(t - 1)

Algorithm 1: Unbalanced MOSS

-

1 ni

Theorem 2. Let B  B, then the strategy  given in Algorithm 1 satisfies R  252B. Corollary 3. For all  the following hold:

1. R,i  252Bi .

2. R,i  mini(ni + 252Bi)

The second part of the corollary is useful when Bi is large, but there exists an arm for which ni and Bi are both small. The proof of Theorem 2 requires a few lemmas. The first is a somewhat standard concentration inequality that follows from a combination of the peeling argument and Doob's maximal inequality. The proof may be found in the supplementary material.

Lemma 4.

Let

Zi

=

max
1sn

i

-

i,s

-

4 s

log+

ni s

.

Then

P {Zi



}



20 ni 2

for

all



>

0.

In the analysis of traditional bandit algorithms the gap ji measures how quickly the algorithm can detect the difference between arms i and j. By design, however, Algorithm 1 is negatively biasing its estimate of the empirical mean of arm i by 1/ni. This has the effect of shifting the gaps, which I denote by  ji and define to be

 ji = ji + 1/nj - 1/ni = i - j + 1/nj - 1/ni . Lemma 5. Define stopping time ji by

ji = min s : j,s +

4 s

log+

nj s

 j +  ji/2

.

If Zi <  ji/2, then Tj(n)  ji.

5

Proof. Let t be the first time step such that Tj(t - 1) = ji. Then

j,Tj (t-1)+

Tj

4 (t -

1)

log+

nj Tj(t - 1)

-

= j +  ji -  ji/2 - 1/nj

= i - 1/ni -  ji/2

1/nj  j +  ji/2 -

1/nj

< i,Ti(t-1) +

4 Ti(t -

1)

log+

ni Ti(t - 1)

-

1/ni ,

which implies that arm j will not be chosen at time step t and so also not for any subsequent time steps by the same argument and induction. Therefore Tj(n)  ji.

Lemma 6.

If  ji > 0, then Eji



40  2ji

+

64  2ji

ProductLog

nj  2ji 64

.

Proof. Let s0 be defined by

s0 = Therefore

64  2ji

ProductLog

nj  2ji 64

=

4 s0

log+

nj s0



 ji 4

.

n n-1
Eji = P {ji  s}  1 + P

i,s

-

i,s



 ji 2

-

s=1

s=1

4 s

log+

nj s

n-1

 1 + s0 +

P

s=s0 +1

i,s - i,s



 ji 4

 1 + s0 +



exp

- s 2ji 32

s=s0 +1



1 + s0

+

32  2ji



40  2ji

+

64  2ji

ProductLog

nj  2ji 64

,

where the last inequality follows since  ji  2.

Proof 2 ji

of Theorem and  ji 

2. Let 1/ni

+=1/2n/j. Lnietatinndg

A 

= {j =

: ji > }. Then 1/ni we have

for

j



A

we

have

ji




K
R,i = E  jiTj (n)
j=1




 n + E  jiTj(n)
jA



(a)
 2Bi + E 

jiji

+

n

max
jA

ji : Zi   ji/2 

jA

(b)
 2Bi +
jA

80  ji

+

128  ji

ProductLog

nj  2ji 64

+ 4nE[Zi1{Zi   }]

(c)
 2Bi +

90nj + 4nE[Zi1{Zi   }] ,

jA

where (a) follows by using Lemma 5 to bound Tj(n)  ji when Zi <  ji. On the other hand, the total number of pulls for arms j for which Zi   ji/2 is at most n. (b) follows by bounding

6

ji in expectation using Lemma 6. (c) follows from basic calculus and because for j  A we have  ji  1/ni. All that remains is to bound the expectation.

4nE[Zi1{Zi   }]  4n P {Zi   } + 4n


P {Zi




z} dz



160n  ni

=

160nni

=

160Bi ,

where I have used Lemma 4 and simple identities. Putting it together we obtain

R,i  2Bi +

90nj + 160B1  252Bi ,

jA

where I applied the assumption B  B and so j=1 nj = j=1 n/Bj  Bi.

The above proof may be simplified in the special case that B is uniform where we recover the minimax regret of MOSS, but with perhaps a simpler proof than was given originally by Audibert and Bubeck [2009].

On Logarithmic Regret

In a recent technical report I demonstrated empirically that MOSS suffers sub-optimal problemdependent regret in terms of the minimum gap [Lattimore, 2015]. Specifically, it can happen that

Rm,oiss  

K min

log

n

,

(6)

where min = icantly smaller.

mSpineic:ifiic>a0lly, iU. COBn

the by

other Auer

hand, the order-optimal et al. [2002] satisfies

asymptotic

regret

can

be

signif-

Ruc,bi  O

1 log n i:i>0 i

,

(7)

which for unequal gaps can be much smaller than Eq. (6) and is asymptotically order-optimal [Lai and Robbins, 1985]. The problem is that MOSS explores only enough to obtain minimax regret, but sometimes obtains minimax regret even when a more conservative algorithm would do better. It is worth remarking that this effect is harder to observe than one might think. The example given in the afforementioned technical report is carefully tuned to exploit this failing, but still requires n = 109 and K = 103 before significant problems arise. In all other experiments MOSS was performing admirably in comparison to UCB.

AOl(lthloegsenp).robTlheemaslgcaonritbhemavisoisdiemdilbayr

modifying UCB to Algorithm 1,

rather than but chooses

MOSS. The cost is a factor of the action that maximises the

following index.

It = arg max i,Ti(t-1) +
i

(2 + ) log Ti(t - 1)

t

-

log n ni

,

where  > 0 is a fixed arbitrary constant.

Theorem 7. If  is the strategy of unbalanced UCB with ni = n2/Bi2 and B  B, then the regret of the unbalanced UCB satisfies:

1.

(problem-independent regret). R,i  O

 Bi log n .

2. (problem-dependent regret). Let A = i : i  2 1/ni log n . Then

R,i  O

Bi

log

n1{A

=

}

+

iA

1 i

log

n

.

The proof is deferred to the supplementary material. The indicator function in the problemdependent bound vanishes for sufficiently large n provided ni  (log(n)), which is equivalent to

7

Bi



 o(n/ log

n).

Thus

for

reasonable

choices

of

B1, . . . , BK

the

algorithm

is

going

to

enjoy

the

same asymptotic performance as UCB. Theorem 7 may be proven for any index-based algorithm for

which it can be shown that

ETi(n)  O

1 2i

log

n

,

which includes (for example) KL-UCB [Cappe et al., 2013] and Thompson sampling (see analysis by Agrawal and Goyal [2012a,b] and original paper by Thompson [1933]), but not OC-UCB [Lattimore, 2015] or MOSS [Audibert and Bubeck, 2009].

Experimental Results

I compare MOSS and unbalanced MOSS in two simple simulated examples, both with horizon

n = 5000. Each data point is an empirical average of 104 i.i.d. samples, so error bars are too small

to see. Code/data is available in the supplementary material. The first experiment has K = 2 arms

and

B1

=

n1 3

and

B2

=

n

2 3

.

I

plotted

the

results

for



=

(0, -)

for

varying

.

As

predicted,

ito(hkteh-enrew{1w1)isH,ae.l.g(.Fon,ir1giwt0.h}i1mt.h).ApHegTrahf=ioenr,mstehsc9keo=srin1gedsn1ui/efilxktcsp.aeaRnrgtielrmyseueeblntwesttitahtehrarestthshKheaontwh=MenoOfr1oyS0.rSaTrfhkmoer=su.pnoTbs1aihtli{iavsknetci=emd,eiaaBl}ngd1foorsri=itghnmifiincs[a0sanu,nt1lpdy/eBr2wi]okoarrns=tdoe

MOSS for i  {1, 2} and inferior otherwise (Fig. 2).

800 MOSS

U. MOSS

2,000

600

Regret Regret

400 1,000 200

0 -0.4 -0.2

0 

0.2 0.4

0 012345 

Figure 1

Figure 2:  =  + (i - 1)/2

Sadly the experiments serve only to highlight the plight of the biased learner, which suffers significantly worse results than its unbaised counterpart for most actions.

6 Discussion

I have shown that the cost of favouritism for multi-armed bandit algorithms is rather serious. If

an algorithm exhibits a small worst-case regret for a specific action, then the worst-case regret of

tbhoeunredmoafini(ng Kacnti)o.nTshiiss

necessarily unfortunate

significantly larger than result is in stark contrast

the well-known uniform worst-case to the experts setting for which there

exist algorithms that suffer constant regret with respect to a single expert at almost no cost for the

remainder. Surprisingly, the best achievable (non-uniform) worst-case bounds are determined up to

a permutation almost entirely by the value of the smallest worst-case regret.

There are some interesting open questions. Most notably, in the adversarial setting I am not sure if the upper or lower bound is tight (or neither). It would also be nice to know if the constant factors can be determined exactly asymptotically, but so far this has not been done even in the uniform case. For the stochastic setting it is natural to ask if the OC-UCB algorithm can also be modified. Intuitively one would expect this to be possible, but it would require re-working the very long proof.

Acknowledgements I am indebted to the very careful reviewers who made many suggestions for improving this paper. Thank you!

8

References
Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In Proceedings of International Conference on Artificial Intelligence and Statistics (AISTATS), 2012a.
Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In Proceedings of Conference on Learning Theory (COLT), 2012b.
Jean-Yves Audibert and Sebastien Bubeck. Minimax policies for adversarial and stochastic bandits. In COLT, pages 217-226, 2009.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on, pages 322-331. IEEE, 1995.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47:235-256, 2002.
Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multiarmed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorporated, 2012. ISBN 9781601986269.
Olivier Cappe, Aurelien Garivier, Odalric-Ambrym Maillard, Remi Munos, and Gilles Stoltz. Kullback-Leibler upper confidence bounds for optimal sequential allocation. The Annals of Statistics, 41(3):1516-1541, 2013.
Nicolo Cesa-Bianchi. Prediction, learning, and games. Cambridge University Press, 2006. Eyal Even-Dar, Michael Kearns, Yishay Mansour, and Jennifer Wortman. Regret to the best vs.
regret to the average. Machine Learning, 72(1-2):21-37, 2008. Marcus Hutter and Jan Poland. Adaptive online prediction by following the perturbed leader. The
Journal of Machine Learning Research, 6:639-660, 2005. Michael Kapralov and Rina Panigrahy. Prediction strategies without loss. In Advances in Neural
Information Processing Systems, pages 828-836, 2011. Wouter M Koolen. The pareto regret frontier. In Advances in Neural Information Processing Sys-
tems, pages 863-871, 2013. Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances
in applied mathematics, 6(1):4-22, 1985. Tor Lattimore. Optimally confident UCB : Improved regret for finite-armed bandits. Technical
report, 2015. URL http://arxiv.org/abs/1507.07880. Che-Yu Liu and Lihong Li. On the prior sensitivity of thompson sampling. arXiv preprint
arXiv:1506.03378, 2015. Amir Sani, Gergely Neu, and Alessandro Lazaric. Exploiting easy data in online optimization. In
Advances in Neural Information Processing Systems, pages 810-818, 2014. William Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
9

