On the consistency theory of high dimensional variable screening

Xiangyu Wang Dept. of Statistical Science
Duke University, USA xw56@stat.duke.edu

Chenlei Leng Dept. of Statistics University of Warwick, UK C.Leng@warwick.ac.uk

David B. Dunson Dept. of Statistical Science
Duke University, USA dunson@stat.duke.edu

Abstract
Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the restricted diagonally dominant (RDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods SIS and HOLP are both strong screening consistent (subject to additional constraints) with large probability if n > O((s+/ )2 log p) under random designs. In addition, we relate the RDD condition to the irrepresentable condition, and highlight limitations of SIS.
1 Introduction
The rapidly growing data dimension has brought new challenges to statistical variable selection, a crucial technique for identifying important variables to facilitate interpretation and improve prediction accuracy. Recent decades have witnessed an explosion of research in variable selection and related fields such as compressed sensing [1, 2], with a core focus on regularized methods [3-7]. Regularized methods can consistently recover the support of coefficients, i.e., the non-zero signals, via optimizing regularized loss functions under certain conditions [8-10]. However, in the big data era when p far exceeds n, such regularized methods might fail due to two reasons. First, the conditions that guarantee variable selection consistency for convex regularized methods such as lasso might fail to hold when p >> n; Second, the computational expense of both convex and non-convex regularized methods increases dramatically with large p.
Bearing these concerns in mind, [11] propose the concept of "variable screening", a fast technique that reduces data dimensionality from p to a size comparable to n, with all predictors having nonzero coefficients preserved. They propose a marginal correlation based fast screening technique "Sure Independence Screening" (SIS) that can preserve signals with large probability. However, this method relies on a strong assumption that the marginal correlations between the response and the important predictors are high [11], which is easily violated in the practice. [12] extends the marginal correlation to the Spearman's rank correlation, which is shown to gain certain robustness but is still limited by the same strong assumption. [13] and [14] take a different approach to attack the screening problem. They both adopt variants of a forward selection type algorithm that includes one variable at a time for constructing a candidate variable set for further refining. These methods
1

eliminate the strong marginal assumption in [11] and have been shown to achieve better empirical performance. However, such improvement is limited by the extra computational burden caused by their iterative framework, which is reported to be high when p is large [15]. To ameliorate concerns in both screening performance and computational efficiency, [15] develop a new type of screening method termed "High-dimensional ordinary least-square projection" (HOLP ). This new screener relaxes the strong marginal assumption required by SIS and can be computed efficiently (complexity is O(n2p)), thus scalable to ultra-high dimensionality.
This article focuses on linear models for tractability. As computation is one vital concern for designing a good screening method, we primarily focus on a class of linear screeners that can be efficiently computed, and study their theoretical properties. The main contributions of this article lie in three aspects.
1. We define the notion of strong screening consistency to provide a unified framework for analyzing screening methods. In particular, we show a necessary and sufficient condition for a screening method to be strong screening consistent is that the screening matrix is restricted diagonally dominant (RDD). This condition gives insights into the design of screening matrices, while providing a framework to assess the effectiveness of screening methods.
2. We relate RDD to other existing conditions. The irrepresentable condition (IC) [8] is necessary and sufficient for sign consistency of lasso [3]. In contrast to IC that is specific to the design matrix, RDD involves another ancillary matrix that can be chosen arbitrarily. Such flexibility allows RDD to hold even when IC fails if the ancillary matrix is carefully chosen (as in HOLP ). When the ancillary matrix is chosen as the design matrix, certain equivalence is shown between RDD and IC, revealing the difficulty for SIS to achieve screening consistency. We also comment on the relationship between RDD and the restricted eigenvalue condition (REC) [6] which is commonly seen in the high dimensional literature. We illustrate via a simple example that RDD might not be necessarily stronger than REC.
3. We study the behavior of SIS and HOLP under random designs, and prove that a sample size of n = O (s + / )2 log p is sufficient for SIS and HOLP to be screening consistent, where s is the sparsity,  measures the diversity of signals and  / evaluates the signal-to-noise ratio. This is to be compared to the sign consistency results in [9] where the design matrix is fixed and assumed to follow the IC.
The article is organized as follows. In Section 1, we set up the basic problem and describe the framework of variable screening. In Section 2, we provide a deterministic necessary and sufficient condition for consistent screening. Its relationship with the irrepresentable condition is discussed in Section 3. In Section 4, we prove the consistency of SIS and HOLP under random designs by showing the RDD condition is satisfied with large probability, although the requirement on SIS is much more restictive.
2 Linear screening
Consider the usual linear regression
Y = X + ,
where Y is the n x 1 response vector, X is the n x p design matrix and is the noise. The regression task is to learn the coefficient vector . In the high dimensional setting where p >> n, a sparsity assumption is often imposed on  so that only a small portion of the coordinates are non-zero. Such an assumption splits the task of learning  into two phases. The first is to recover the support of , i.e., the location of non-zero coefficients; The second is to estimate the value of these non-zero signals. This article mainly focuses on the first phase.
As pointed out in the introduction, when the dimensionality is too high, using regularization methods methods raises concerns both computationally and theoretically. To reduce the dimensionality, [11] suggest a variable screening framework by finding a submodel
Md = {i : |i| is among the largest d coordinates of ||} or M = {i : |i| > }.
2

Let Q = {1, 2, * * * , p} and define S as the true model with s = |S| being its cardinarlity. The
hope is that the submodel size |Md| or |M| will be smaller or comparable to n, while S  Md or S  M. To achieve this goal two steps are usually involved in the screening analysis. The first is to show there exists some  such that miniS |i| >  and the second step is to bound the size of |M| such that |M| = O(n). To unify these steps for a more comprehensive theoretical
framework, we put forward a slightly stronger definition of screening consistency in this article.

Definition 2.1. (Strong screening consistency) An estimator  (of ) is strong screening consistent if it satisfies that

min |i| > max |i|
iS iS

(1)

and

sign(i) = sign(i), i  S.

(2)

Remark 2.1. This definition does not differ much from the usual screening property studied in the literature, which requires miniS |i| > max(in-S s) |i|, where max(k) denotes the kth largest item.

The key of strong screening consistency is the property (1) that requires the estimator to preserve
consistent ordering of the zero and non-zero coefficients. It is weaker than variable selection consis-
tency in [8]. The requirement in (2) can be seen as a relaxation of the sign consistency defined in [8], as no requirement for i, i  S is needed. As shown later, such relaxation tremendously reduces the restriction on the design matrix, and allows screening methods to work for a broader choice of X.

The focus of this article is to study the theoretical properties of a special class of screeners that take the linear form as
 = AY
for some pxn ancillary matrix A. Examples include sure independence screening (SIS) where A = XT /n and high-dimensional ordinary least-square projection (HOLP ) where A = XT (XXT )-1. We choose to study the class of linear estimators because linear screening is computationally efficient and theoretically tractable. We note that the usual ordinary least-squares estimator is also a special case of linear estimators although it is not well defined for p > n.

3 Deterministic guarantees

In this section, we derive the necessary and sufficient condition that guarantees  = AY to be strong screening consistent. The design matrix X and the error are treated as fixed in this section and we will investigate random designs later. We consider the set of sparse coefficient vectors defined by
B(s, ) =   Rp : |supp()|  s, maxisupp() |i|   . minisupp() |i|
The set B(s, ) contains vectors having at most s non-zero coordinates with the ratio of the largest and smallest coordinate bounded by . Before proceeding to the main result of this section, we introduce some terminology that helps to establish the theory.
Definition 3.1. (restricted diagonally dominant matrix) A p x p symmetric matrix  is restricted diagonally dominant with sparsity s if for any I  Q, |I|  s - 1 and i  Q \ I

ii > C0 max

|ij + kj |,

|ij - kj |

jI jI

where C0  1 is a constant.

+ |ik|

k = i, k  Q \ I,

Notice this definition implies that for i  Q \ I

ii  C0

|ij + kj | + |ij - kj | /2  C0 |ij |,

jI jI

jI

(3)

which is related to the usual diagonally dominant matrix. The restricted diagonally dominant matrix provides a necessary and sufficient condition for any linear estimators  = AY to be strong
screening consistent. More precisely, we have the following result.

3

Theorem 1. For the noiseless case where = 0, a linear estimator  = AY is strong screening consistent for every   B(s, ), if and only if the screening matrix  = AX is restricted diagonally dominant with sparsity s and C0  .

Proof. Assume  is restricted diagonally dominant with sparsity s and C0  . Recall  = . Suppose S is the index set of non-zero predictors. For any i  S, k  S, if we let I = S \ {i}, then
we have

|i| = |i|

ii

+

jI

j i

ij

= |i|

ii

+

jI

j i

(ij

+ kj ) + ki -

jI

j i

kj

- ki

> -|i|

jI

j i

kj

+ ki

= - |i| i

j kj + iki
jI

= -sign(i) * k,

and

|i| = |i|

ii

+

jI

j i

ij

= |i|

ii

+

jI

j i

(ij

- kj ) - ki +

jI

j i

kj

+ ki

> |i|

jI

j i

kj

+ ki

= sign(i) * k.

Therefore, whatever value sign(i) is, it always holds that |i| > |k| and thus miniS |i| > maxkS |k|.

To prove the sign consistency for non-zero coefficients, we notice that for i  S,

ii = iii2 + ij j i = i2
jI

ii

+

jI

j i

ij

> 0.

The proof of necessity is left to the supplementary materials.

The noiseless case is a good starting point to analyze . Intuitively, in order to preserve the correct order of the coefficients in  = AX, one needs AX to be close to a diagonally dominant matrix, so that i, i  MS will take advantage of the large diagonal terms in AX to dominate i, i  MS that is just linear combinations of off-diagonal terms.
When noise is considered, the condition in Theorem 1 needs to be changed slightly to accommodate extra discrepancies. In addition, the smallest non-zero coefficient has to be lower bounded to ensure a certain level of signal-to-noise ratio. Thus, we augment our previous definition of B(s, ) to have a signal strength control
B (s, ) = {  B(s, )| min |i|   }.
isupp()
Then we can obtain the following modified Theorem. Theorem 2. With noise, the linear estimator  = AY is strong screening consistent for every   B (s, ) if  = AX - 2 -1 A Ip is restricted diagonally dominant with sparsity s and C0  .
The proof of Theorem 2 is essentially the same as Theorem 1 and is thus left to the supplementary materials. The condition in Theorem 2 can be further tailored to a necessary and sufficient version with extra manipulation on the noise term. Nevertheless, this might not be useful in practice due to the randomness in noise. In addition, the current version of Theorem 2 is already tight in the sense that there exists some noise vector such that the condition in Theorem 2 is also necessary for strong screening consistency.
Theorems 1 and 2 establish ground rules for verifying consistency of a given screener and provide practical guidance for screening design. In Section 4, we consider some concrete examples of ancillary matrix A and prove that conditions in Theorems 1 and 2 are satisfied by the corresponding screeners with large probability under random designs.
4

4 Relationship with other conditions

For some special cases such sure independence screening ("SIS"), the restricted diagonally dominant

(RDD) condition is related to the strong irrepresentable condition (IC) proposed in [8]. Assume each column of X is standardized to have mean zero. Letting C = XT X/n and  be a given coefficient

vector, the IC is expressed as

CSc,S CS-,1S * sign(S )   1 - 

(4)

for some  > 0, where CA,B represents the sub-matrix of C with row indices in A and column indices in B. The authors enumerate several scenarios of C such that IC is satisfied. We verify some

of these scenarios for screening matrix .

Corollary 1. If ii = 1, i and |ij| < c/(2s), i = j for some 0  c < 1 as defined in Corollary 1 and 2 in [8], then  is a restricted diagonally dominant matrix with sparsity s and C0  1/c.

If |ij| < r|i-j|, i, j for some 0 < r < 1 as defined in Corollary 3 in [8], then  is a restricted diagonally dominant matrix with sparsity s and C0  (1 - r)2/(4r).

A more explicit but nontrivial relationship between IC and RDD is illustrated below when |S| = 2.

Theorem 3. Assume ii = 1, i and |ij| < r, i = j. If  is restricted diagonally dominant with sparsity 2 and C0  , then  satisfies

Sc,S -S,1S * sign(S )



-1 1-r

for all   B(2, ). On the other hand, if  satisfies the IC for all   B(2, ) for some , then  is

a restricted diagonally dominant matrix with sparsity 2 and

C0



1

1 -



1 1

- +

r .
r

Theorem 3 demonstrates certain equivalence between IC and RDD. However, it does not mean
that RDD is also a strong requirement. Notice that IC is directly imposed on the covariance matrix XT X/n. This makes IC a strong assumption that is easily violated; for example, when the predictors are highly correlated. In contrast to IC, RDD is imposed on matrix AX where there is flexibility in choosing A. Only when A is chose to be X/n, RDD is equivalently strong as IC, as shown in next theorem. For other choices of A, such as HOLP defined in next section, the estimator satisfies RDD
even when predictors are highly correlated. Therefore, RDD is considered as weak requirement.

For "SIS", the screening matrix  = XT X/n coincides with the covariance matrix, making RDD

and IC effectively equivalent. The following theorem formalizes this.

Theorem 4. Let A = XT /n and standardize columns of X to have sample variance one. Assume

X satisfies the sparse Riesz condition [16], i.e,

min
Q, ||s

min(XT

X /n)



,

for some  > 0. Now if AX is restricted diagonally dominant with sparsity s + 1 and C0   with  > s/, then X satisfies the IC for any   B(s, ).
 In other words, under the condition  > s/, the strong screening consistency of SIS for B(s +
1, ) implies the model selection consistency of lasso for B(s, ).

Theorem 4 illustrates the difficulty of SIS. The necessary condition that guarantees good screening performance of SIS also guarantees the model selection consistency of lasso. However, such a strong necessary condition does not mean that SIS should be avoided in practice given its substantial advantages in terms of simplicity and computational efficiency. The strong screening consistency defined in this article is stronger than conditions commonly used in justifying screening procedures as in [11].
Another common assumption in the high dimensional literature is the restricted eigenvalue condition (REC). Compared to REC, RDD is not necessarily stronger due to its flexibility in choosing the ancillary matrix A. [17, 18] prove that the REC is satisfied when the design matrix is sub-Gaussian. However, REC might not be guaranteed when the row of X follows heavy-tailed distribution. In contrast, as the example shown in next section and in [15], by choosing A = XT (XXT )-1, the resulting estimator satisfies RDD even when the rows of X follow heavy-tailed distributions.

5

5 Screening under random designs

In this section, we consider linear screening under random designs when X and are Gaussian.
The theory developed in this section can be easily extended to a broader family of distributions, for example, where follows a sub-Gaussian distribution [19] and X follows an elliptical distribution [11, 15]. We focus on the Gaussian case for conciseness. Let  N (0, 2) and X  N (0, ). We prove the screening consistency of SIS and HOLP by verifying the condition in Theorem 2. Recall the ancillary matrices for SIS and HOLP are defined respectively as

ASIS = X/n,

AHOLP = XT (XXT )-1.

For simplicity, we assume ii = 1 for i = 1, 2, * * * , p. To verify the RDD condition, it is essential to quantify the magnitude of the entries of AX and A .

Lemma 1. Let  = ASISX, then for any t > 0 and i = j  Q, we have

t2n tn P |ii - ii|  t  2 exp - min 8e2K , 2eK

,

and

P |ij - ij |  t

 6 exp

- min

t2n tn 72e2K , 6eK

,

where K = X 2(1) - 1 1 is a constant, X 2(1) is a chi-square random variable with one degree of freedom and the norm * 1 is defined in [19].

Lemma 1 states that the screening matrix  = ASISX for SIS will eventually converge to the covariance matrix  in l when n tends to infinity and log p = o(n). Thus, the screening performance of SIS strongly relies on the structure of . In particular, the (asymptotically) necessary and sufficient condition for SIS being strong screening consistent is  satisfying the RDD condition. For
the noise term, we have the following lemma.

Lemma 2. Let  = ASIS . For any t > 0 and i  Q, we have

P (|i|  t)  6 exp

- min

t2n tn 72e2K , 6eK

,

where K is defined the same as in Lemma 1.

The proof of Lemma 2 is essentially the same as the proof of off-diagonal terms in Lemma 1 and is thus omitted. As indicated before, the necessary and sufficient condition for SIS to be strong screening consistent is that  follows RDD. As RDD is usually hard to verify, we consider a stronger sufficient condition inspired by Corollary 1.

Theorem 5.

Let r = maxi=j |ij|. If r <

1 2s

,

then

for

any



>

0,

if

the

sample

size

satisfies

1 + 2s + 2/ 2

n > 144K

log(3p/),

1 - 2sr

(5)

where K is defined in Lemma 1, then with probability at least 1 - ,  = ASISX - 2 -1 ASIS Ip is restricted diagonally dominant with sparsity s and C0  . In other words, SIS is screening consistent for any   B (s, ).

Proof. Taking union bound on the results from Lemma 1 and 2, we have for any t > 0 and p > 2,

P

min
iQ

ii



1

-

t

or

max |ij|
i=j



r

+

t

or

   t

 7p2 exp

- n min K

t2 t 72e2 , 6e

.

In other words, for any  > 0, when n  K log(7p2/), with probability at least 1 - , we have

 min ii  1 - 6 2e
iQ

K log(7p2/) ,
n

 max |ij|  r + 6 2e
i=j

K log(7p2/) ,
n

6

 K log(7p2/)

max
iQ

|i|



6

2e

. n

A sufficient condition for  to be restricted diagonally dominant is that

min ii > 2s max |ij| + 2 -1 max |i|.

i i=j

i

Plugging in the values we have

 1 - 6 2e

K log(7p2/)



> 2s(r + 6 2e

K

log(7p2

/)

)

+

 12 2e

-1



K log(7p2/) .

n nn

Solving the above inequality (notice that 7p2/ < 9p2/2 and  > 1) completes the proof.

The requirement that maxi=j |ij| < 1/(sr) or the necessary and sufficient condition that  is RDD strictly constrains the correlation structure of X, causing the difficulty for SIS to be strong
screening consistent. For HOLP we instead have the following result.

Lemma 3. Let  = AHOLP X. Assume p > c0n for some c0 > 1, then for any C > 0 there exists some 0 < c1 < 1 < c2 and c3 > 0 such that for any t > 0 and any i  Q, j = i, we have

P

|ii|

<

c1-1

n p

 2e-Cn,

n P |ii| > c2 p

 2e-Cn

and



where c4 =

c2(c0-c1) .
c3 (c0 -1)

 n
P |ij| > c4t p

 5e-Cn + 2e-t2/2,

Proof. The proof of Lemma 3 relies heavily on previous results for the Stiefel Manifold provided in

the supplementary materials. We only sketch the basic idea here and leave the complete proof to the

supplementary materials. Defining H = XT (XXT )-1/2, then we have  = HHT and H follows the Matrix Angular Central Gaussian (MACG) with covariance . The diagonal terms of HHT

can be bounded similarly via the Johnson-Lindenstrauss lemma, by using the fact that HHT =

1/2U (U T U )-1U , where U is a p x n random projection matrix. Now for off-diagonal terms,

we decompose the Stiefel manifold as H = (G(H2)H1 H2), where H1 is a (p - n + 1) x 1 vector, H2 is a p x (n - 1) matrix and G(H2) is chosen so that (G(H2) H2)  O(p), and show that H1 follows Angular Central Gaussian (ACG) distribution with covariance G(H2)T G(H2) conditional

on

H2.

It

can

be

shown

that

e2HHT e1

(d)
=

e2G(H2)H1|eT1 H2 = 0. Let t21 = eT1 HHT e1, then

eT1 H2 = 0 is equivalent to eT1 G(H2)H1 = t1, and we obtain the desired coupling distribution as

eT2 HHT e1 (=d) eT2 G(H2)H1|eT1 G(H2)H1 = t1. Using the normal representation of ACG(), i.e.,

if x = (x1, * * * , xp)  N (0, ), then x/ x  ACG(), we can write G(H2)H1 in terms of

normal variables and then bound all terms using concentration inequalities.

Lemma 3 quantifies the entries of the screening matrix for HOLP . As illustrated in the lemma,

regardless


of

the

covariance

,

diagonal

terms

of



are

always

O(

n p

)

and

the

off-diagonal

terms

are

O(

n p

).

Thus, with n 

O(s2),  is likely to satisfy the RDD condition with large probability.

For

the noise vector we have the following result.

Lemma 4. Let  = AHOLP . Assume p > c0n for some c0 > 1, then for any C > 0 there exist the

same c1, c2, c3 as in Lemma 3 such that for any t > 0 and i  Q,



P

|i|



2 c2t 1 - c-0 1

n p

< 4e-Cn + 2e-t2/2,

if n  8C/(c0 - 1)2.

The proof is almost identical to Lemma 2 and is provided in the supplementary materials. The following theorem results after combining Lemma 3 and 4.

7

Theorem 6. Assume p > c0n for some c0 > 1. For any  > 0, if the sample size satisfies

n > max

2C

4(s + / )2 log(3p/),

8C (c0 - 1)2

,

(6)

where C

=

max{

4c24 c21

,

4c2 c21(1-c-0 1)2

}

and

c1, c2,

c3,

c4,

C

are

the

same

constants

defined

in

Lemma

3,

then with probability at least 1 - ,  = AHOLP X - 2 -1 AHOLP Ip is restricted diagonally

dominant with sparsity s and C0  . This implies HOLP is screening consistent for any  

B (s, ).

Proof. Notice that if min |ii| > 2s max |ij| + 2 -1 XT (XXT )-1 ,
i ij

(7)

then the proof is completebecause  - 2 -1 XT (XXT )-1  is already a restricted diagonally

dominant matrix. Let t = Cn/. The above equation then requires





c1-1

n p

-

2c4

C s 

n p

-

2 (1 -

c2 C t c-0 1) 

n p

=

c1-1 - 2c4

C s 

-

2 (1 -

c2 C  c-0 1) 

n > 0,
p

which implies that



 > 2c4

C 2 s c1

+

2 c2C2 c1(1 - c-0 1)

= C12s + C22 -1

> 1,



where C1

=

2c4 c1

C

,

C2

=

.2 c2C
c1(1-c-0 1)

Therefore, taking union bounds on all matrix entries, we

have

P (7) does not hold < (p + 5p2)e-Cn + 2p2e-Cn/ < (7 + 1 )p2e-Cn/2 , n

where the second inequality is due to the fact that p > n and  > 1. Now for any  > 0, (7) holds with probability at least 1 -  if

2 n  log(7 + 1/n) + 2 log p - log  ,
C

which

is

satisfied

provided

(noticing

 8

<

3)

n



2 2 C

log

3p 

.

Now

pushing



to

the

limit

gives

(6),

the precise condition we need.

There are several interesting observations on equation (5) and (6). First, (s + / )2 appears in both expressions. We note that s evaluates the sparsity and the diversity of the signal  while / is closely related to the signal-to-noise ratio. Furthermore, HOLP relaxes the correlation constraint r < 1/(2s) or the covariance constraint ( is RDD) with the conditional number constraint. Thus for any , as long as the sample size is large enough, strong screening consistency is assured. Finally, HOLP provides an example to satisfy the RDD condition in answer to the question raised in Section 4.

6 Concluding remarks
This article studies and establishes a necessary and sufficient condition in the form of restricted diagonally dominant screening matrices for strong screening consistency of a linear screener. We verify the condition for both SIS and HOLP under random designs. In addition, we show a close relationship between RDD and the IC, highlighting the difficulty of using SIS in screening for arbitrarily correlated predictors. For future work, it is of interest to see how linear screening can be adapted to compressed sensing [20] and how techniques such as preconditioning [21] can improve the performance of marginal screening and variable selection.
Acknowledgments This research was partly support by grant NIH R01-ES017436 from the National Institute of Environmental Health Sciences.

8

References
[1] David L Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289-1306, 2006.
[2] Richard Baraniuk. Compressive sensing. IEEE Signal Processing Magazine, 24(4), 2007. [3] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society. Series B (Statistical Methodology), 58(1):267-288, 1996. [4] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its
oracle properties. Journal of the American Statistical Association, 96(456):1348-1360, 2001. [5] Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is
much larger than n. The Annals of Statistics, 35(6):2313-2351, 2007. [6] Peter J Bickel, Ya'acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and
dantzig selector. The Annals of Statistics, 37(4):1705-1732, 2009. [7] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The
Annals of Statistics, 38(2):894-942, 2010. [8] Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine
Learning Research, 7:2541-2563, 2006. [9] Martin J Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity
using l1-constrained quadratic programming. IEEE Transactions on Information Theory, 2009. [10] Jason D Lee, Yuekai Sun, and Jonathan E Taylor. On model selection consistency of m-
estimators with geometrically decomposable penalties. Advances in Neural Processing Information Systems, 2013. [11] Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849- 911, 2008. [12] Gaorong Li, Heng Peng, Jun Zhang, Lixing Zhu, et al. Robust rank correlation based screening. The Annals of Statistics, 40(3):1846-1877, 2012. [13] Hansheng Wang. Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512-1524, 2009. [14] Haeran Cho and Piotr Fryzlewicz. High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593-622, 2012. [15] Xiangyu Wang and Chenlei Leng. High-dimensional ordinary least-squares projection for screening variables. https://stat.duke.edu/xw56/holp-paper.pdf, 2015. [16] Cun-Hui Zhang and Jian Huang. The sparsity and bias of the lasso selection in highdimensional linear regression. The Annals of Statistics, 36(4):1567-1594, 2008. [17] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated gaussian designs. The Journal of Machine Learning Research, 11:2241-2259, 2010. [18] Shuheng Zhou. Restricted eigenvalue conditions on subgaussian random matrices. arXiv preprint arXiv:0912.4045, 2009. [19] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010. [20] Lingzhou Xue and Hui Zou. Sure independence screening and compressed random sensing. Biometrika, 98(2):371-380, 2011. [21] Jinzhu Jia and Karl Rohe. Preconditioning to comply with the irrepresentable condition. arXiv preprint arXiv:1208.5584, 2012.
9

