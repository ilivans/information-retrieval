Individual Planning in Infinite-Horizon Multiagent Settings: Inference, Structure and Scalability

Xia Qu Epic Systems Verona, WI 53593 quxiapisces@gmail.com

Prashant Doshi THINC Lab, Dept. of Computer Science University of Georgia, Athens, GA 30622
pdoshi@cs.uga.edu

Abstract
This paper provides the first formalization of self-interested planning in multiagent settings using expectation-maximization (EM). Our formalization in the context of infinite-horizon and finitely-nested interactive POMDPs (I-POMDP) is distinct from EM formulations for POMDPs and cooperative multiagent planning frameworks. We exploit the graphical model structure specific to I-POMDPs, and present a new approach based on block-coordinate descent for further speed up. Forward filtering-backward sampling - a combination of exact filtering with sampling - is explored to exploit problem structure.
1 Introduction
Generalization of bounded policy iteration (BPI) to finitely-nested interactive partially observable Markov decision processes (I-POMDP) [1] is currently the leading method for infinite-horizon selfinterested multiagent planning and obtaining finite-state controllers as solutions. However, interactive BPI is acutely prone to converge to local optima, which severely limits the quality of its solutions despite the limited ability to escape from these local optima. Attias [2] posed planning using MDP as a likelihood maximization problem where the "data" is the initial state and the final goal state or the maximum total reward. Toussaint et al. [3] extended this to infer finite-state automata for infinite-horizon POMDPs. Experiments reveal good quality controllers of small sizes although run time is a concern. Given BPI's limitations and the compelling potential of this approach in bringing advances in inferencing to bear on planning, we generalize it to infinite-horizon and finitely-nested I-POMDPs. Our generalization allows its use toward planning for an individual agent in noncooperation where we may not assume common knowledge of initial beliefs or common rewards, due to which others' beliefs, capabilities and preferences are modeled. Analogously to POMDPs, we formulate a mixture of finite-horizon DBNs. However, the DBNs differ by including models of other agents in a special model node. Our approach, labeled as I-EM, improves on the straightforward extension of Toussaint et al.'s EM to I-POMDPs by utilizing various types of structure. Instead of ascribing as many level 0 finite-state controllers as candidate models and improving each using its own EM, we use the underlying graphical structure of the model node and its update to formulate a single EM that directly provides the marginal of others' actions across all models. This rests on a new insight, which considerably simplifies and speeds EM at level 1. We present a general approach based on block-coordinate descent [4, 5] for speeding up the nonasymptotic rate of convergence of the iterative EM. The problem is decomposed into optimization subproblems in which the objective function is optimized with respect to a small subset (block) of variables, while holding other variables fixed. We discuss the unique challenges and present the first effective application of this iterative scheme to multiagent planning. Finally, sampling offers a way to exploit the embedded problem structure such as information in distributions. The exact forward-backward E-step is replaced with forward filtering-backward sampling
1

(FFBS) that generates trajectories weighted with rewards, which are used to update the parameters of the controller. While sampling has been integrated in EM previously [6], FFBS specifically mitigates error accumulation over long horizons due to the exact forward step.

2 Overview of Interactive POMDPs

A finitely-nested I-POMDP [7] for an agent i with strategy level, l, interacting with agent j is: I-POMDPi,l = ISi,l, A, Ti, i, Oi, Ri, OCi
* ISi,l denotes the set of interactive states defined as, ISi,l = S x Mj,l-1, where Mj,l-1 = {j,l-1  SMj}, for l  1, and ISi,0 = S, where S is the set of physical states. j,l-1 is the set of computable, intentional models ascribed to agent j: j,l-1 = bj,l-1, j. Here bj,l-1 is agent j's level l - 1 belief, bj,l-1  (ISj,l-1) where (*) is the space of distributions, and j = A, Tj, j, Oj, Rj, OCj, is j's frame. At level l=0, bj,0  (S) and a intentional model reduces to a POMDP. SMj is the set of subintentional models of j, an example is a finite state automaton.
* A = Ai x Aj is the set of joint actions of all agents. * Other parameters - transition function, Ti, observations, i, observation function, Oi, and prefer-
ence function, Ri - have their usual semantics analogously to POMDPs but involve joint actions. * Optimality criterion, OCi, here is the discounted infinite horizon sum.
An agent's belief over its interactive states is a sufficient statistic fully summarizing the agent's observation history. Given the associated belief update, solution to an I-POMDP is a policy. Using the Bellman equation, each belief state in an I-POMDP has a value which is the maximum payoff the agent can expect starting from that belief and over the future.

3 Planning in I-POMDP as Inference

We may represent the policy of agent i for the infinite horizon case as a stochastic finite state nTcooi nt:etNsroailglxeernAt(Fii'SxsCa)c,tiidoxenfiNdnieisdtriabs[u:0t,io1in]=raetperNaecsihe,nTntiso,dLtheie;,aVnnoiddeawnthraiennriestiitaNiloidniissfutrtnhibceutitsoieontn;oLofvine:orNdtheisexinnoAdtihees icso[0dn,etr1no]oldlteeerd-. by, Vi : Ni  [0, 1]. For convenience, we group Vi, Ti and Li in fi. Define a controller at level l for agent i as, i,l =  Ni,l, fi,l , where Ni,l is the set of nodes in the controller and fi,l groups remaining parameters of the controller as mentioned before. Analogously to POMDPs [3], we formulate planning in multiagent settings formalized by I-POMDPs as a likelihood maximization problem:

i,l

=

arg max i,l

 (1 - )

T

T =0

P r(riT

=

1|T ; i,l)

(1)

where i,l are all level-l emitted after T time steps

wFiSthCsproofbaabgielnittyip, rroiTpoirstiaonbainl atorythreanrdewomardv,aRriai(bsle, awi,haojs)e.

value

is

0

or

1

n0i,l

n0i,l n1i,l n2i,l

nTi,l

a0i s0 ri0

a0j

Mj0,0

a0k

a0i s0

o1i a1i

o2i a2i

s1 s2

oTi aTi

s0

sT riT

a0j a1j

a2j

aTj

Mj0,0

a0k

Mj1,0

a1k

Mj2,0

a2k

MjT,0

a0k

m0j,0

s1 a0j o1j
m1j,0

sT

a1j oTj

aTj rjT

mTj,0

Mk0,0

Mk0,0

Mk1,0

Mk2,0

MkT,0

Figure 1: (a) Mixture of DBNs with 1 to T time slices for I-POMDPi,1 with i's level-1 policy represented as a standard FSC whose "node state" is denoted by ni,l. The DBNs differ from those for POMDPs by containing special model nodes (hexagons) whose values are candidate models of other agents. (b) Hexagonal model nodes and edges in bold for one other agent j in (a) decompose into this level-0 DBN. Values of the node mtj,0 are the candidate models. CPT of chance node atj denoted by j,0(mtj,0, atj) is inferred using likelihood maximization.

2

The planning problem is modeled as a mixture of DBNs of increasing time from T =0 onwards

(Fig. 1). The transition and observation functions of I-POMDPi,l parameterize the chance nodes s

and oi, respectively, along with P r(riT |aTi , aTj , sT )  are the maximum and minimum reward values in Ri.

.Ri(sT ,aTi ,aTj )-Rmin
Rmax -Rmin

Here,

Rmax

and

Rmin

The networks include nodes, ni,l, of agent i's level-l FSC. Therefore, functions in fi,l parameterize the network as well, which are to be inferred. Additionally, the network includes the hexagonal model nodes - one for each other agent - that contain the candidate level 0 models of the agent. Each model node provides the expected distribution over another agent's actions. Without loss of generality, no edges exist between model nodes in the same time step. Correlations between agents could be included as state variables in the models.

Agent j's model nodes and the edges (in bold) between them, and between the model and chance

action nodes represent a the candidate models of

DagBeNntojf.lAenggetnhtTi'sasinsihtioawl nbeinlieFfigo.v1e(rbt)h.eVsatlauteesaonfdthmeocdhealnscoefnjobdeec, omm0j,e0s,

are the

parameters of s0 and m0j,0. The likelihood maximization at level 0 seeks to obtain the distribution,

P r(aj|m0j,0), for each candidate model in node, m0j,0, using EM on the DBN.

Proposition 1 (Correctness). The likelihood maximization problem as defined in Eq. 1 with the

mixture models as given in Fig. 1 is equivalent to the problem of solving the original I-POMDPi,l

with discounted infinite horizon whose solution assumes the form of a finite state controller.

All proofs are given in the supplement. Given the unique mixture models above, the challenge is to generalize the EM-based iterative maximization for POMDPs to the framework of I-POMDPs.

3.1 Single EM for Level 0 Models

The straightforward approach is to infer a likely FSC for each level 0 model. However, this approach

does not scale to many predictive information

ambooduetlso.thPeror paogseintitonfro2mbeiltoswcasnhdoiwdsattehamt othdeeldsyantamtimicePtr, (taotj

|st) is obtain

sufficient the most

likely policy of agent i. This is markedly different from using behavioral equivalence [8] that clusters

models with identical solutions. The latter continues to require the full solution of each model.

Psurfofipcoiesnittiporned2ic(tSivueffiicnifeonrcmya)t.ioDniastbroibuuttoiothnesrParg(eantjt

|st) j to

across obtain

athcetimonosstaltjikelyApjolfiocry

each of i.

state

st

is

In the context of Proposition 2, we seek to infer P r(atj|mtj,0) for each (updated) model of j at all time steps, which is denoted as j,0. Other terms in the computation of P r(atj|st) are known parameters of the level 0 DBN. The likelihood maximization for the level 0 DBN is:

j,0

 

= arg max (1 - )

j,0

T =0

mj,0 MjT,0

T P r(rjT

= 1|T , mj,0; j,0)

As the trajectory consisting of states, models, actions and observations of the other agent is hidden at planning time, we may solve the above likelihood maximization using EM.

E-step Let zj0:T = {st, mtj,0, atj, otj}T0 where the observation at t = 0 is null, be the hidden trajectory. The log likelihood is obtained as an expectation of these hidden trajectories:

Q(j,0|j,0)

=

 

T =0

zj0:T

P r(rjT

=

1, zj0:T , T ; j,0) log

P r(rjT

=

1, zj0:T , T ; j,0)

(2)

The "data" in the level 0 DBN consists of the initial belief over the state and models, b0i,1, and the observed reward at T . Analogously to EM for POMDPs, this motivates forward filtering-backward smoothing on a network with joint state (st,mtj,0) for computing the log likelihood. The transition function for the forward and backward steps is:



P r(st, mtj,0|st-1, mtj-,01) =

atj-1,otj j,0(mtj-,01, atj-1) Tmj (st-1, atj-1, st) P r(mtj,0|mtj-,01, atj-1, otj )

x Omj (st, atj-1, otj )

(3)

where mj in the subscripts is j's model at t - 1. Here, P r(mtj,0|atj-1, otj, mtj-,01) is the Kronecker-

delta function otherwise 0.

that

is

1

when

j's

belief

in

mtj-,01

updated

using

atj-1

and

otj

equals

the

belief

in

mtj,0;

3

Forward filtering gives the probability of the next state as follows:



t(st, mtj,0) =

st-1,mtj-,01 P r(st, mtj,0|st-1, mtj-,01) t-1(st-1, mtj-,01)

pwrohbearbeilit0y(os0f ,thme0js,0ta)teisatnhde

initial model

belief of agent i. The smoothing by at t - 1 from the distribution at t is:

which

we

obtain

the

joint



h(st-1, mtj-,01) =

st,mtj,0 P r(st, mtj,0|st-1, mtj-,01) h-1(st, mtj,0)

where h denotes the horizon to T and 0(sT , mTj,0) = EaTj |mTj,0 [P r(rjT = 1|sT , mTj,0)]. Messages t and h give the probability of a state at some time slice in the DBN. As we consider a mixture of BNs, we seek probabilities for all states in the mixture model. Subsequently, we may compute the forward and backward messages at all states for the entire mixture model in one sweep.

 (s, mj,0) =

P r(T = t) t(s, mj,0)

t=0

(s, mj,0) =  P r(T = h) h(s, mj,0) h=0

(4)

Model growth As the other agent performs its actions and makes observations, the space

of j's models grows exponentially: starting from a finite set of |Mj0,0| models, we obtain

WO(e|Mlimj0,i0t|t(h|Ae jg|r|owjt|h)ti)nmthoedmelsodaetltsimpaecet.bTyhsiasmgpreliantlgyminocdreelassaets

the number of trajectories the next time step from the

dinisZtrij0b:Tu-.

ttihoens, trutc(tsutr,emptjr,e0s)e,natsinwe jp,0erafnodrmOje,awchhiscthepguoifdfeohrwowardthfie lmteoridnegl.s

It limits grow.

the

growth

by

exploiting

M-step We obtain the updated j,0 from the full log likelihood in Eq. 2 by separating the terms:

Q(j,0|j,0)

=

terms

independent

of

j,0 

+


T =0


zj0:T

P r(riT

=

1, zj0:T , T ; j,0)

T
t=0

j,0 (atj

|mtj,0

)

and maximizing it w.r.t. j,0:

j,0(atj , mtj,0) 



j,0(atj , mtj )

st Rmj (st, atj ) (st, mtj,0) +

st ,st+1 ,mtj+,01 ,otj+1

1

 -



(st+1,

mtj+,01 )

x (st, mtj,0) Tmj (st, atj , st+1) P r(mtj+,01|mtj,0, atj , otj+1) Omj (st+1, atj , otj+1)

3.2 Improved EM for Level l I-POMDP

At strategy levels l  1, Eq. 1 defines the likelihood maximization problem, which is iteratively solved using EM. We show the E- and M -steps next beginning with l = 1.

E-step In a multiagent setting, the hidden variables additionally include what the other agent may observe and how it acts over time. However, a key insight is that Prop. 2 allows us to limit

attention to the marginal distribution over other agents' actions given the state. Thus, let zi0:T = {st, oti, nti,l, ati, atj, . . . , atk}T0 , where the observation at t = 0 is null, and other agents are labeled j to k; this group is denoted -i. The full log likelihood involves an expectation over hidden variables:

Q(i,l|i,l)

=

 

T =0

zi0:T

P r(riT

=

1, zi0:T , T ; i,l)

log

P r(riT

=

1, zi0:T , T ; i,l)

(5)

Due to the subjective perspective in I-POMDPs, Q computes the likelihood of agent i's FSC only (and not of joint FSCs as in team planning [9]).

bIfnuellltihejeof.iTnWt-esintseetpheeDk eBthxNepelocikftaeFtliyiognd. i1us,tsrioinbbgusetairovfnoesrd,wVeavir,iddT-ebina,ccaeknwdinaLcrldiu,daaelcgsrootrhsisethtrmiemwoeanrsdlai,chreiisTd.d,WeantetMhmeaaryeknaodgvaaminndordtehealeliwzinehiottihsaeel

state is (st, nti,l). The transition function of this model is,



P r(st, nti,l|st-1, nti,-l 1) =

ati-1,at--i1,oti Li(nti,-l 1, ati-1)

-i P r(at--i1|st-1) Ti(nti,-l 1, ati-1, oti, nti,l)

x Ti(st-1, ati-1, at--i1, st) Oi(st, ati-1, at--i1, oti)

(6)

In addition to parameters of I-POMDPi,l, which are given, parameters of agent i's controller and those relating to other agents' predicted actions, -i,0, are present in Eq. 6. Notice that in consequence of Proposition 2, Eq. 6 precludes j's observation and node transition functions.

4

The forward message, t = P r(st, nti,l), represents the probability of being at some state of the

DBN at time t:



t(st, nti,l) =

st-1,nti,-l 1 P r(st, nti,l|st-1, nti,-l 1) t-1(st-1, nti,-l 1)

(7)

where, 0(s0, n0i,l) = Vi(n0i,l)b0i,l(s0). The backward message gives the probability of observing the

reward in the final T th time step given a state of the Markov model, t(st, nti,l) = P r(riT = 1|st, nti,l):



where,

h(st, nti,l) =

0(sT

,

nTi,l)

=


aTi ,aT-i

st+1,nti,+l 1
P r(riT =

P r(st+1, 1|sT , aTi

nti,+l 1|st, nti,l) h-1(st+1, nti,+l 1)

,

aT-i)

Li(nTi,l,

aTi )


-i

P r(aT-i

|sT

),

and

(8) 1

h  T is the horizon. Here, P r(riT = 1|sT , aTi , aT-i)  Ri(sT , aTi , aT-i).

A side effect of P r(at-i|st) being dependent on t is that we can no longer conveniently define  and  for use in M -step at level 1. Instead, the computations are folded in the M -step.

M-step We update the parameters, Li, Ti and Vi, of i,l to obtain i,l based on the expectation

in the E-step. Specifically, take log of the likelihood P r(rT = with i,l and focus on terms involving the parameters of i,l:

1,

zi0:T

,

T

;

i,l)

with

i,l

substituted

log P r(rT

=

1, zi0:T , T ; i,l)

=terms independent of i,l +

T -1
t=0

log

Ti (nti,l ,

ati ,

oti+1 ,

T log
t=0

Li (nti,l ,

nti,+l 1) + log Vi(ni,l

ati )

)+

In order facilitate

dtioffeurpednattiea,tioLni,,

we partially differentiate the Q-function of Eq. 5 with we focus on the terms involving Li, as shown below.

respect

to

Li.

To

Q(i,l|i,l)

=

terms

indep.

of

Li

+


T =0

Pr(T

)

T 

t=0

zi0:T

Pr(riT

=

1, zi0:t|T ; i,l) log Li(nti,l, ati)

Li on maximizing the above equation is:

Li (nti,l ,

ati )



Li(nti,l,

ati )


T =0


-i


sT ,aT-i

T 1-

P r(riT

= 1|sT , aTi , aT-i) P r(aT-i|sT ) T (sT , nTi,l)

Node transition probabilities Ti and node distribution Vi for i,l, is updated analogously to Li.

Because a FSC is inferred at level 1, at strategy levels l = 2 and greater, lower-level candidate models are FSCs. EM at these higher levels proceeds by replacing the state of the DBN, (st, nti,l) with (st, nti,l, ntj,l-1, . . . , ntk,l-1).

3.3 Block-Coordinate Descent for Non-Asymptotic Speed Up

Block-coordinate descent (BCD) [4, 5, 10] is an iterative scheme to gain faster non-asymptotic rate of convergence in the context of large-scale N -dimensional optimization problems. In this scheme, within each iteration, a set of variables referred to as coordinates are chosen and the objective function, Q, is optimized with respect to one of the coordinate blocks while the other coordinates are held fixed. BCD may speed up the non-asymptotic rate of convergence of EM for both I-POMDPs and POMDPs. The specific challenge here is to determine which of the many variables should be grouped into blocks and how.

We empirically show in Section 5 that grouping the number of time slices, t, and horizon, h, in

Eqs. 7 and 8, respectively, at each level, into coordinate blocks of equal size is beneficial. In other

words, we decompose the mixture model into blocks containing equal numbers of BNs. Alternately,

grouping controller nodes is ineffective because distribution Vi cannot be optimized for subsets of

nBBottdi=essb.{oFuot1nr,mdeadt2ll,by,ylt3se,ot.m. .et1}C.bIenaps1rua. bcAtsinectaelo,ofbge{ocTuasul=yse, 1wb,oeTtdhe=tfian2ne,d.th.h.e,asTreet

= Tmax}. Then, the set of blocks finite (say, Tmax), the cardinality of blocks of h, denoted by Bh.

is, of

In the M -step now, we compute t for the time steps in a single coordinate block tc only, while

using the values of t from the previous iteration for the

Analogously, we compute iteration for the remaining

hhorfiozor nths.eWhoericzyocnliscainllychchoonolsye,

q  {0, 1, 2, . . .}.

complementary coordinate while using  values from

thbelopckresv, ioutcs.

a block, tc, at iterations c + qC where

5

3.4 Forward Filtering - Backward Sampling

An approach for exploiting embedded structure in the transition and observation functions is to replace the exact forward-backward message computations with exact forward filtering and backward sampling (FFBS) [11] to obtain a sampled reverse trajectory consisting of sT , nTi,l, aTi , nTi,l-1, aTi -1, oTi , nTi,l, and so on from T to 0. Here, P r(riT = 1|sT , aTi , aT-i) is the likelihood weight of this trajectory sample. Parameters of the updated FSC, i,l, are obtained by summing and normalizing the weights.

Each trajectory is obtained by first sampling T  P r(T ), which becomes the length of i's DBN for

this sample. Forward message, t(st, nti,l), t = 0 . . . T is computed exactly (Eq. 7) followed by the backward message, h(st, nti,l), h = 0 . . . T and t = T - h. Computing h differs from Eq. 8 by utilizing the forward message:



h(st, nti,l|st+1, nti,+l 1) =

ati,at-i,oti+1 t(st, nti,l) Li(nti,l, ati )

-i P r(at-i|st) Ti(st, ati, at-i, st+1)

Ti(nti,l, ati, oti+1, nti,+l 1) Oi(st+1, ati, at-i, oti+1)

(9)

where

0(sT

,

nTi,l,

riT

)

=


ati ,at-i

T

(sT

,

nTi,l)


-i

P r(aT-i|sT

)

L(nTi,l,

aTi

)

P

r(riT

|sT

,

aTi

,

aT-i).

Subsequently, we may easily sample sT , nTi,l, riT  followed by sampling sTi -1, nTi,l-1 from Eq. 9.

We sample aTi -1, oTi  P r(ati, oti+1|st, nti,l, st+1, nti,+l 1), where: 
P r(ati, oti+1|st, nti,l, st+1, nti,+l 1)  -i P r(at-i|st) Li(nti,l, ati) Ti(nti,l, ati, oti+1, nti,+l 1) Ti(st, ati, at-i, st+1)
Oi(st+1, ati, at-j , oti+1)

4 Computational Complexity

Our EM at level 1 is significantly quicker compared to ascribing FSCs to other agents. In the latter, nodes of others' controllers must be included alongside s and ni,l. Proposition 3 (E-step speed up). Each E-step at level 1 using the forward-backward pass as shown previously results in a net speed up of O((|M ||N-i,0|)2K |-i|K ) over the formulation that ascribes |M | FSCs each to K other agents with each having |N-i,0| nodes.

Analogously, updating the parameters Li and Ti in the M-step exhibits a speedup of O((|M ||N-i,0|)2K |-i|K ), while Vi leads to O((|M ||N-i,0|)K ). This improvement is exponential in the number of other agents. On the other hand, the E-step at level 0 exhibits complexity that is typically greater compared to the total complexity of the E-steps for |M | FSCs.

Proposition 4 (E-step ratio at level 0). E-steps when |M | FSCs are inferred for K agents exhibit a

ratio

of

complexity,

O(

|N-i,0 |2 |M |

),

compared

to

the

E-step

for

obtaining

-i,0.

The ratio in Prop. 4 is < 1 when smaller-sized controllers are sought and there are several models.

5 Experiments
Five variants of EM are evaluated as appropriate: the exact EM inference-based planning (labeled as I-EM); replacing the exact M-step with its greedy variant analogously to the greedy maximization in EM for POMDPs [12] (I-EM-Greedy); iterating EM based on coordinate blocks (I-EM-BCD) and coupled with a greedy M-step (I-EM-BCD-Greedy); and lastly, using forward filtering-backward sampling (I-EM-FFBS). We use 4 problem domains: the noncooperative multiagent tiger problem [13] (|S|= 2, |Ai|= |Aj|= 3, |Oi|= |Oj|= 6 for level l  1, |Oj|= 3 at level 0, and  = 0.9) with a total of 5 agents and 50 models for each other agent. A larger noncooperative 2-agent money laundering (ML) problem [14] forms the second domain. It exhibits 99 physical states for the subject agent (blue team), 9 actions for blue and 4 for the red team, 11 observations for subject and 4 for the other, with about 100 models

6

Level 1 Value

Level 1 Value

0 -50 -100 -150 -200 -250 -300
10

5-agent Tiger
I-EM I-EM-Greedy
I-EM-BCD I-EM-FFBS 100 1000 time(s) in log scale
(I-a) EM methods

0

-50

-100

-150

-200

-250 I-EM-BCD I-BPI
-300 10 100

1000

10000

time(s) in log scale

(II-a) I-EM-BCD, I-BPI

2-agent ML
-90

-100

-110

-120 -130 -140

I-EM I-EM-Greedy I-EM-BCD-Greedy
I-EM-FFBS

100 1000 time(s) in log scale

10000

(I-b) EM methods

-90 -100

I-EM-BCD-Greedy I-BPI

-110

-120

-130

-140

100 1000 time(s) in log scale

(II-b) I-EM-BCD-Greedy, I-BPI

400 350 300 250 200 150 100
50 0 0

3-agent UAV
I-EM-Greedy I-EM-BCD-Greedy
I-EM-FFBS 10000 20000 30000 40000 50000 60000 70000
time(s)

(I-c) EM methods

400 350 300 250 200 150 100
50 0 0

10000

I-EM-BCD-Greedy I-BPI

20000 time(s)

30000

40000

(II-c) I-EM-BCD-Greedy, I-BPI

1100 1000
900 800 700 600 500
0

5-agent policing
1200

1100

I-EM I-EM-Greedy
I-EM-BCD I-EM-BCD-Greedy

1000 900 800

5000

10000 time(s)

15000

700

600

20000

0

5000

I-EM-BCD I-BPI

10000 time(s)

15000

(I-d) EM methods

(II-d) I-EM-BCD, I-BPI

20000

Figure 2: FSCs improve with time for I-POMDPi,1 in the (I-a) 5-agent tiger, (I-b) 2-agent money laundering, (I-c) 3-agent UAV, and (I-d) 5-agent policing contexts. Observe that BCD causes substantially larger improvements in the initial iterations until we are close to convergence. I-EM-BCD or its greedy variant converges significantly quicker than I-BPI to similar-valued FSCs for all four problem domains as shown in (II-a, b, c and d), respectively. All experiments were run on Linux with Intel Xeon 2.6GHz CPUs and 32GB RAM.

for red team. We also evaluate a 3-agent UAV reconnaissance problem involving a UAV tasked with intercepting two fugitives in a 3x3 grid before they both reach the safe house [8]. It has 162 states for the UAV, 5 actions, 4 observations for each agent, and 200,400 models for the two fugitives. Finally, the recent policing protest problem is used in which police must maintain order in 3 designated protest sites populated by 4 groups of protesters who may be peaceful or disruptive [15]. It exhibits 27 states, 9 policing and 4 protesting actions, 8 observations, and 600 models per protesting group. The latter two domains are historically the largest test problems for self-interested planning. Comparative performance of all methods In Fig. 2-I(a-d), we compare the variants on all problems. Each method starts with a random seed, and the converged value is significantly better than a random FSC for all methods and problems. Increasing the sizes of FSCs gives better values in general but also increases time; using FSCs of sizes 5, 3, 9 and 5, for the 4 domains respectively demonstrated a good balance. We explored various coordinate block configurations eventually settling on 3 equal-sized blocks for both the tiger and ML, 5 blocks for UAV and 2 for policing protest. I-EM and the Greedy and BCD variants clearly exhibit an anytime property on the tiger, UAV and policing problems. The noncooperative ML shows delayed increases because we show the value of agent i's controller and initial improvements in the other agent's controller may maintain or decrease the value of i's controller. This is not surprising due to competition in the problem. Nevertheless, after a small delay the values improve steadily which is desirable. I-EM-BCD consistently improves on I-EM and is often the fastest: the corresponding value improves by large steps initially (fast non-asymptotic rate of convergence). In the context of ML and UAV, I-EM-BCD-Greedy shows substantive improvements leading to controllers with much improved values compared to other approaches. Despite a low sample size of about 1,000 for the problems, I-EM-FFBS obtains FSCs whose values improve in general for tiger and ML, though slowly and not always to the level of others. This is because the EM gets caught in a worse local optima due
7

to sampling approximation - this strongly impacts the UAV problem; more samples did not escape these optima. However, forward filtering only (as used in Wu et al. [6]) required a much larger sample size to reach these levels. FFBS did not improve the controller in the fourth domain. Characterization of local optima While an exact solution for the smaller tiger problem with 5 agents (or the larger problems) could not be obtained for comparison, I-EM climbs to the optimal value of 8.51 for the downscaled 2-agent version (not shown in Fig. 2). In comparison, BPI does not get past the local optima of -10 using an identical-sized controller - corresponding controller predominantly contains listening actions - relying on adding nodes to eventually reach optimum. While we are unaware of any general technique to escape local convergence in EM, I-EM can reach the global optimum given an appropriate seed. This may not be a coincidence: the I-POMDP value function space exhibits a single fixed point - the global optimum - which in the context of Proposition 1 makes the likelihood function, Q(i,l|i,l), unimodal (if i,l is appropriately sized as we do not have a principled way of adding nodes). If Q(i,l|i,l) is continuously differentiable for the domain on hand, Corollary 1 in Wu [16] indicates that i,l will converge to the unique maximizer. Improvement on I-BPI We compare the quickest of the I-EM variants with previous best algorithm, I-BPI (Figs. 2-II(a-d)), allowing the latter to escape local optima as well by adding nodes. Observe that FSCs improved using I-EM-BCD converge to values similar to those of I-BPI almost two orders of magnitude faster. Beginning with 5 nodes, I-BPI adds 4 more nodes to obtain the same level of value as EM for the tiger problem. For money laundering, I-EM-BCD-Greedy converges to controllers whose value is at least 1.5 times better than I-BPI's given the same amount of allocated time and less nodes. I-BPI failed to improve the seed controller and could not escape for the UAV and policing protest problems. To summarize, this makes I-EM variants with emphasis on BCD the fastest iterative approaches for infinite-horizon I-POMDPs currently.
6 Concluding Remarks
The EM formulation of Section 3 builds on the EM for POMDP and differs drastically from the Eand M-steps for the cooperative DEC-POMDP [9]. The differences reflect how I-POMDPs build on POMDPs and differ from DEC-POMDPs. These begin with the structure of the DBNs where the DBN for I-POMDPi,1 in Fig. 1 adds to the DBN for POMDP hexagonal model nodes that contain candidate models; chance nodes for action; and model update edges for each other agent at each time step. This differs from the DBN for DEC-POMDP, which adds controller nodes for all agents and a joint observation chance node. The I-POMDP DBN contains controller nodes for the subject agent only, and each model node collapses into an efficient distribution on running EM at level 0. In domains where the joint reward function may be decomposed into factors encompassing subsets of agents, ND-POMDPs allow the value function to be factorized as well. Kumar et al. [17] exploit this structure by simply decomposing the whole DBN mixture into a mixture for each factor and iterating over the factors. Interestingly, the M-step may be performed individually for each agent and this approach scales beyond two agents. We exploit both graphical and problem structures to speed up and scale in a way that is contextual to I-POMDPs. BCD also decomposes the DBN mixture into equal blocks of horizons. While it has been applied in other areas [18, 19], these applications do not transfer to planning. Additionally, problem structure is considered by using FFBS that exploits information in the transition and observation distributions of the subject agent. FFBS could be viewed as a tenuous example of Monte Carlo EM, which is a broad category and also includes the forward sampling utilized by Wu et al. [6] for DEC-POMDPs. However, fundamental differences exist between the two: forward sampling may be run in simulation and does not require the transition and observation functions. Indeed, Wu et al. utilize it in a model free setting. FFBS is model based utilizing exact forward messages in the backward sampling phase. This reduces the accumulation of sampling errors over many time steps in extended DBNs, which otherwise afflicts forward sampling. The advance in this paper for self-interested multiagent planning has wider relevance to areas such as game play and ad hoc teams where agents model other agents. Developments in online EM for hidden Markov models [20] provide an interesting avenue to utilize inference for online planning.
Acknowledgments This research is supported in part by a NSF CAREER grant, IIS-0845036, and a grant from ONR, N000141310870. We thank Akshat Kumar for feedback that led to improvements in the paper.
8

References
[1] Ekhlas Sonu and Prashant Doshi. Scalable solutions of interactive POMDPs using generalized and bounded policy iteration. Journal of Autonomous Agents and Multi-Agent Systems, pages DOI: 10.1007/s10458-014-9261-5, in press, 2014.
[2] Hagai Attias. Planning by probabilistic inference. In Ninth International Workshop on AI and Statistics (AISTATS), 2003.
[3] Marc Toussaint and Amos J. Storkey. Probabilistic inference for solving discrete and continuous state markov decision processes. In International Conference on Machine Learning (ICML), pages 945-952, 2006.
[4] Jeffrey A. Fessler and Alfred O. Hero. Space-alternating generalized expectationmaximization algorithm. IEEE Transactions on Signal Processing, 42:2664-2677, 1994.
[5] P. Tseng. Convergence of block coordinate descent method for nondifferentiable minimization. Journal of Optimization Theory and Applications, 109:475-494, 2001.
[6] Feng Wu, Shlomo Zilberstein, and Nicholas R. Jennings. Monte-carlo expectation maximization for decentralized POMDPs. In Twenty-Third International Joint Conference on Artificial Intelligence (IJCAI), pages 397-403, 2013.
[7] Piotr J. Gmytrasiewicz and Prashant Doshi. A framework for sequential planning in multiagent settings. Journal of Artificial Intelligence Research, 24:49-79, 2005.
[8] Yifeng Zeng and Prashant Doshi. Exploiting model equivalences for solving interactive dynamic influence diagrams. Journal of Artificial Intelligence Research, 43:211-255, 2012.
[9] Akshat Kumar and Shlomo Zilberstein. Anytime planning for decentralized pomdps using expectation maximization. In Conference on Uncertainty in AI (UAI), pages 294-301, 2010.
[10] Ankan Saha and Ambuj Tewari. On the nonasymptotic convergence of cyclic coordinate descent methods. SIAM Journal on Optimization, 23(1):576-601, 2013.
[11] C. K. Carter and R. Kohn. Markov chainmonte carlo in conditionally gaussian state space models. Biometrika, 83:589-601, 1996.
[12] Marc Toussaint, Laurent Charlin, and Pascal Poupart. Hierarchical POMDP controller optimization by likelihood maximization. In Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI), pages 562-570, 2008.
[13] Prashant Doshi and Piotr J. Gmytrasiewicz. Monte Carlo sampling methods for approximating interactive POMDPs. Journal of Artificial Intelligence Research, 34:297-337, 2009.
[14] Brenda Ng, Carol Meyers, Kofi Boakye, and John Nitao. Towards applying interactive POMDPs to real-world adversary modeling. In Innovative Applications in Artificial Intelligence (IAAI), pages 1814-1820, 2010.
[15] Ekhlas Sonu, Yingke Chen, and Prashant Doshi. Individual planning in agent populations: Anonymity and frame-action hypergraphs. In International Conference on Automated Planning and Scheduling (ICAPS), pages 202-211, 2015.
[16] C. F. Jeff Wu. On the convergence properties of the em algorithm. Annals of Statistics, 11(1):95-103, 1983.
[17] Akshat Kumar, Shlomo Zilberstein, and Marc Toussaint. Scalable multiagent planning using probabilistic inference. In International Joint Conference on Artificial Intelligence (IJCAI), pages 2140-2146, 2011.
[18] S. Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless channels. IEEE Transactions on Information Theory, 18(1):14-20, 1972.
[19] Jeffrey A. Fessler and Donghwan Kim. Axial block coordinate descent (ABCD) algorithm for X-ray CT image reconstruction. In International Meeting on Fully Three-dimensional Image Reconstruction in Radiology and Nuclear Medicine, volume 11, pages 262-265, 2011.
[20] Olivier Cappe and Eric Moulines. Online expectation-maximization algorithm for latent data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):593-613, 2009.
9

