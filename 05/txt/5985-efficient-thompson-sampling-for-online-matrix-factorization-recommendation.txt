Efficient Thompson Sampling for Online Matrix-Factorization Recommendation

Jaya Kawale, Hung Bui, Branislav Kveton
Adobe Research
San Jose, CA {kawale, hubui, kveton}@adobe.com

Long Tran Thanh University of Southampton
Southampton, UK ltt08r@ecs.soton.ac.uk

Sanjay Chawla Qatar Computing Research Institute, Qatar
University of Sydney, Australia sanjay.chawla@sydney.edu.au

Abstract
Matrix factorization (MF) collaborative filtering is an effective and widely used method in recommendation systems. However, the problem of finding an optimal trade-off between exploration and exploitation (otherwise known as the bandit problem), a crucial problem in collaborative filtering from cold-start, has not been previously addressed. In this paper, we present a novel algorithm for online MF recommendation that automatically combines finding the most relevant items with exploring new or less-recommended items. Our approach, called Particle Thompson sampling for MF (PTS), is based on the general Thompson sampling framework, but augmented with a novel efficient online Bayesian probabilistic matrix factorization method based on the Rao-Blackwellized particle filter. Extensive experiments in collaborative filtering using several real-world datasets demonstrate that PTS significantly outperforms the current state-of-the-arts.
1 Introduction
Matrix factorization (MF) techniques have emerged as a powerful tool to perform collaborative filtering in large datasets [1]. These algorithms decompose a partially-observed matrix R  RNxM into a product of two smaller matrices, U  RNxK and V  RMxK , such that R  U V T . A variety of MF-based methods have been proposed in the literature and have been successfully applied to various domains. Despite their promise, one of the challenges faced by these methods is recommending when a new user/item arrives in the system, also known as the problem of coldstart. Another challenge is recommending items in an online setting and quickly adapting to the user feedback as required by many real world applications including online advertising, serving personalized content, link prediction and product recommendations.
In this paper, we address these two challenges in the problem of online low-rank matrix completion by combining matrix completion with bandit algorithms. This setting was introduced in the previous work [2] but our work is the first satisfactory solution to this problem. In a bandit setting, we can model the problem as a repeated game where the environment chooses row i of R and the learning agent chooses column j. The Rij value is revealed and the goal (of the learning agent) is to minimize the cumulative regret with respect to the optimal solution, the highest entry in each row of R. The key design principle in a bandit setting is to balance between exploration and exploitation which solves the problem of cold start naturally. For example, in online advertising, exploration implies presenting new ads, about which little is known and observing subsequent feedback, while exploitation entails serving ads which are known to attract high click through rate.
1

While many solutions have been proposed for bandit problems, in the last five years or so, there
has been a renewed interest in the use of Thompson sampling (TS) which was originally proposed
in 1933 [3, 4]. In addition to having competitive empirical performance, TS is attractive due to its conceptual simplicity. An agent has to choose an action a (column) from a set of available actions so as to maximize the reward r, but it does not know with certainty which action is optimal. Following TS, the agent will select a with the probability that a is the best action. Let  denotes the unknown parameter governing reward structure, and O1:t the history of observations currently available to the agent. The agent chooses a = a with probability

I

E [r|a, ] = max E [r|a , ]
a

P (|O1:t)d

which can be implemented by simply sampling  from the posterior P (|O1:t) and let a = arg maxa E [r|a , ]. However for many realistic scenarios (including for matrix completion), sampling from P (|O1:t) is not computationally efficient and thus recourse to approximate methods is required to make TS practical.

We propose a computationally-efficient algorithm for solving our problem, which we call Particle Thompson sampling for matrix factorization (PTS). PTS is a combination of particle filtering for online Bayesian parameter estimation and TS in the non-conjugate case when the posterior does not have a closed form. Particle filtering uses a set of weighted samples (particles) to estimate the posterior density. In order to overcome the problem of the huge parameter space, we utilize Rao-Blackwellization and design a suitable Monte Carlo kernel to come up with a computationally and statistically efficient way to update the set of particles as new data arrives in an online fashion. Unlike the prior work [2] which approximates the posterior of the latent item features by a single point estimate, our approach can maintain a much better approximation of the posterior of the latent features by a diverse set of particles. Our results on five different real datasets show a substantial improvement in the cumulative regret vis-a-vis other online methods.

2 Probabilistic Matrix Factorization

We first review the probabilistic matrix factorization approach to
the low-rank matrix completion problem. In matrix completion, a portion Ro of the N x M matrix R = (rij) is observed, and the goal is to infer the unobserved entries of R. In probabilistic matrix factorization (PMF) [5], R is assumed to be a noisy perturbation of a rank-K matrix R = U V where UNxK and VMxK are termed the user and item latent features (K is typically small). The full generative model of PMF is

,

Ui

v
Vj M Rij

Ui i.i.d.  Vj i.i.d. 

N (0, u2IK ) N (0, v2IK )

u
(1)

N M

rij|U, V i.i.d.  N (Ui Vj, 2)

Figure 1: Graphical model of probabilistic matrix factoriza-

where the variances (2, U2 , V2 ) are the parameters of the model. tion model

We also consider a full Bayesian treatment where the variances

U2 and V2 are drawn from an inverse Gamma prior (while 2 is held fixed), i.e., U = U-2  (, ); V = V-2  (, ) (this is a special case of the

Bayesian PMF [6] where we only consider isotropic Gaussians)1. Given this generative model,

from the observed ratings Ro, we would like to estimate the parameters U and V which will al-

low us to "complete" the matrix R. PMF is a MAP point-estimate which finds U, V to maximize Pr(U, V |Ro, , U , V ) via (stochastic) gradient ascend (alternate least square can also be used [1]). Bayesian PMF [6] attempts to approximate the full posterior Pr(U, V |Ro, , , ). The joint pos-

terior of U and V are intractable; however, the structure of the graphical model (Fig. 1) can be

exploited to derive an efficient Gibbs sampler.

We now provide the expressions for the conditional probabilities of interest. Supposed that V and U are known. Then the vectors Ui are independent for each user i. Let rts(i) = {j|rij  Ro} be the set of items rated by user i, observe that the ratings {Rioj|j  rts(i)} are generated i.i.d. from Ui

1[6] considers the full covariance structure, but they also noted that isotropic Gaussians are effective enough.

2

following a simple conditional linear Gaussian model. Thus, the posterior of Ui has the closed form

Pr(Ui|V, Ro, , U ) = Pr(Ui|Vrts(i), Rio,rts(i), U , ) = N (Ui|ui , (ui )-1)

where

ui

=

1 2

(ui )-1iu

;

ui

=

1 2

Vj Vj

jrts(i)

+

1 u2

IK ;

iu =

rioj Vj .

jrts(i)

(2) (3)

The conditional posterior of V , Pr(V |U, Ro, V , ) is similarly factorized into

M j=1

N

(Vj

|vj

,

(vj

)-1)

where

the

mean

and

precision

are

similarly

defined.

The posterior

of the precision U = U-2 given U (and simiarly for V ) is obtained from the conjugacy of the

Gamma prior and the isotropic Gaussian

Pr(U |U, , )

=

(U

|

NK 2

+

,

1 2

U

2 F

+

).

(4)

Although not required for Bayesian PMF, we give the likelihood expression

Pr(Rij = r|V, Ro, U , ) = N (r|Vj

ui ,

1 2

+ Vj

V,iVj ).

(5)

The advantage of the Bayesian approach is that uncertainty of the estimate of U and V are available which is crucial for exploration in a bandit setting. However, the bandit setting requires maitaining online estimates of the posterior as the ratings arrive over time which makes it rather awkward for MCMC. In this paper, we instead employ a sequential Monte-Carlo (SMC) method for online Bayesian inference [7, 8]. Similar to the Gibbs sampler [6], we exploit the above closed form updates to design an efficient Rao-Blackwellized particle filter [9] for maintaining the posterior over time.

3 Matrix-Factorization Recommendation Bandit
In a typical deployed recommendation system, users and observed ratings (also called rewards) arrive over time, and the task of the system is to recommend item for each user so as to maximize the accumulated expected rewards. The bandit setting arises from the fact that the system needs to learn over time what items have the best ratings (for a given user) to recommend, and at the same time sufficiently explore all the items.

We formulate the matrix factorization bandit as follows. We assume that ratings are generated following Eq. (1) with a fixed but unknown latent features (U , V ). At time t, the environment chooses user it and the system (learning agent) needs to recommend an item jt. The user then rathastaeatstrtaehkweeasrredicn.otomWmaececanobdubenrdet vittiheaemtehwtihsiittsohrayrsatorifntogth=reitor,jibtts,jetr.vNeTd(hUreaittsiynsVgtsejtm,prir2oe)rcoatmnodmtitmehneedastg,eitrne1otm:tr-ejc1te, iuvwseihsnegtrheaisrpr1oao:ttliinc=gy {(ik, jk, rko)}tk=1. The highest expected reward the system can earn at time t is maxj Ui Vj, and this is achieved if the optimal item j(i) = arg maxj Ui Vj is recommended. Since (U , V ) are unknown, the optimal item j(i) is also not known a priori. The quality of the recommendation
system is measured by its expected cumulative regret:

nn

CR = E

[rto - rit,j(it)] = E

[rto

-

max
j

Uit

Vj]

t=1 t=1

(6)

where the expectation is taken with respect to the choice of the user at time t and also the randomness in the choice of the recommended items by the algorithm.

3.1 Particle Thompson Sampling for Matrix Factorization Bandit
While it is difficult to optimize the cumulative regret directly, TS has been shown to work well in practice for contextual linear bandit [3]. To use TS for matrix factorization bandit, the main difficulty is to incrementally update the posterior of the latent features (U, V ) which control the reward structure. In this subsection, we describe an efficient Rao-Blackwellized particle filter (RBPF) designed to exploit the specific structure of the probabilistic matrix factorization model. Let  = (, , ) be the control parameters and let posterior at time t be pt = Pr(U, V, U , V , |r1o:t, ). The standard

3

Algorithm 1 Particle Thompson Sampling for Matrix Factorization (PTS)

Global control params: , U , V ; for Bayesian version (PTS-B): , , 

1: p0  InitializeParticles() 2: Ro = 

3: for t = 1, 2 . . . do

4: i  current user

5: Sample d  pt-1.w 6: V  pt-1.V (d)
7: [If PTS-B] U  pt-1.U(d) 8: Sample Ui  Pr(Ui|V , U , , r1o:t-1) 9: j  arg maxj Ui Vj 10: Recommend j for user i and observe rating r.

sample new Ui due to Rao-Blackwellization

11: rto  (i, j, r) 12: pt  UpdatePosterior(pt-1, r1o:t)

13: end for

14: procedure UPDATEPOSTERIOR(p, r1o:t)

15: 16: (i, j, r)  rto

p has the structure (w, particles) where particles[d] = (U (d), V (d), U(d), V(d)).

17: d, ui (d)  ui (V (d), r1o:t-1), iu(d)  iu(V (d), r1o:t-1)

see Eq. (3)

18: d, wd  Pr(Rij = r|V (d), U(d), , r1o:t-1), see Eq.(5), wd = 1

19:

d,

i



p.w; p .particles[d]



p.particles[i];

d, p .wd



1 D

20: for all d do

Reweighting; see Eq.(5)
Resampling Move

21:

ui (d)



ui (d)

+

1 2

Vj

Vj

;

iu(d)  iu(d) + rVj

22: p .Ui(d)  Pr(Ui|p .V (d), p .U(d), , r1o:t)

see Eq. (2)

23: [If PTS-B] Update the norm of p .U (d)

24: vj (d)  vj (V (d), r1o:t), jv(d)  iu(V (d), r1o:t) 25: p .Vj(d)  Pr(Vj |p .U (d), p .V(d), , r1o:t) 26: [If PTS-B] p .U(d)  Pr(U |p .U (d), , ) 27: end for

see Eq.(4)

28: return p

29: end procedure

particle filter would sample all of the parameters (U, V, U , V ). Unfortunately, in our experi-

ments, degeneracy is highly problematic for such a vanilla particle filter (PF) even when U , V

are assumed known (see Fig. 4(b)). Our RBPF algorithm maintains the posterior distribution pt as

follows. Each of the particle conceptually represents a point-mass at V, U (U and V are integrated

out

analytically

whenever possible)2.

Thus,

pt(V, U )

is

approximated

by

pt

=

1 D

D
d=1 (V (d),U(d))

where D is the number of particles.

Crucially, since the particle filter needs to estimate a set of non-time-vayring parameters, having

an effective and efficient MCMC-kernel move Kt(V , U ; V, U ) stationary w.r.t. pt is essential. Our design of the move kernel Kt are based on two observations. First, we can make use of U and V as auxiliary variables, effectively sampling U, V |V, U  pt(U, V |V, U ), and then V , U |U, V  pt(V , U |U, V ). However, this move would be highly inefficient due to the number of variables that need to be sampled at each update. Our second observation is the key to an

efficient implementation. Note that latent features for all users dependent of the current observed rating rto: pt(U-it |V, U ) =

except the pt-1(U-it

current user U-it |V, U ), therefore

are inat time

t we only have to resample Uit as there is no need to resample U-it . Furthermore, it suffices to

resample the latent feature of the current item Vjt . This leads to an efficient implementation of the

RBPF where each particle in fact stores3 U, V, U , V , where (U, V ) are auxiliary variables, and

for the kernel move Kt, we sample Uit |V, U then Vjt |U, V and U |U, , .

The PTS algorithm is given in Algo. 1. At each time t, the complexity is O(((N + M )K2 + K3)D) where N and M are the maximum number of users who have rated the same item and the maximum

2When there are fewer users than items, a similar strategy can be derived to integrate out U and V instead. 3This is not inconsistent with our previous statement that conceptually a particle represents only a point-
mass distribution V,U .

4

number of items rated by the same user, respectively. The dependency on K3 arises from having to invert the precision matrix, but this is not a concern since the rank K is typically small. Line 24 can be replaced by an incremental update with caching: after line 22, we can incrementally update vj and jv for all item j previously rated by the current user i. This reduces the complexity to O((M K2 + K3)D), a potentially significant improvement in a real recommendation systems where each user tends to rate a small number of items.
4 Analysis
We believe that the regret of PTS can be bounded. However, the existing work on TS and bandits does not provide sufficient tools for proper analysis of our algorithm. In particular, while existing techniques can provide O(log T ) (or O( T ) for gap-independent) regret bounds for our problem, these bounds are typically linear in the number of entries of the observation matrix R (or at least linear in the number of users), which is typically very large, compared to T . Thus, an ideal regret bound in our setting is the one that has sub-linear dependency (or no dependency at all) on the number of users. A key obstacle of achieving this is that, while the conditional posteriors of U and V are Gaussians, neither their marginal and joint posteriors belong to well behaved classes (e.g., conjugate posteriors, or having closed forms). Thus, novel tools, that can handle generic posteriors, are needed for efficient analysis. Moreover, in the general setting, the correlation between Ro and the latent features U and V are non-linear (see, e.g., [10, 11, 12] for more details). As existing techniques are typically designed for efficiently learning linear regressions, they are not suitable for our problem. Nevertheless, we show how to bound the regret of TS in a very specific case of n x m rank-1 matrices, and we leave the generalization of these results for future work.

In particular, we analyze the regret of PTS in the setting of Gopalan et al. [13]. We model our

problem as follows. The parameter space is u x v, where u = {d, 2d, . . . , 1}Nx1 and v = {d, 2d, . . . , 1}Mx1 are discretizations of the parameter spaces of rank-1 factors u and v for some

integer 1/d. For the sake of theoretical analysis, we assume that PTS can sample from the full

posterior. We also assume that ri,j  N (ui vj, 2) for some u  u and v  u. Note that

in this setting, the highest-rated item in expectation is the same for all users. We denote this item

by j = arg max 1jM vj and assume that it is uniquely optimal, uj > uj for any j = j. We

leverage these properties in our analysis. The random variable Xt at time t is a pair of a random rating matrix Rt = {ri,j}Ni=,1M,j=1 and a random row 1  it  N . The action At at time t is a

column 1  jt  M . The observation is Yt = (it, rit,jt ). We bound the regret of PTS as follows.

Theorem 1. For any   (0, 1) and  (0, 1), there exists T  such that PTS on u x v recom-

mends items j

= j in T



T

steps

at

most

(2M

1+ 1-

2 d4

log

T

+ B) times with probability of at

least 1 - , where B is a constant independent of T .

Proof. By Theorem 1 of Gopalan et al. [13], the number of recommendations j = j is bounded by C(log T ) + B, where B is a constant independent of T . Now we bound C(log T ) by counting the number of times that PTS selects models that cannot be distinguished from (u, v) after observing Yt under the optimal action j. Let:
j = (u, v)  u x v : i : uivj = ui vj , vj  maxk=j vk

be the set of such models where action j is optimal. Suppose that our algorithm chooses model (u, v)  j. Then the KL divergence between the distributions of ratings ri,j under models (u, v) and (u, v) is bounded from below as:

DKL(uivj

ui vj) =

(uivj - ui vj)2 22



d4 22

.

for any i. The last inequality follows from the fact that uivj  uivj = ui vj > ui vj, be-

cause j is uniquely optimal in (u, v). We know that uivj - ui vj  d2 because the gran-

ularity of our discretization is d. Let i1, . . . , in be any n row indices. Then the KL divergence

between the distributions of ratings in positions (i1, j), . . . , (in, j) under models (u, v) and (u, v)

is are

n t=1

DKL

(uit

unlikely to be

vj uit chosen

vj)



n

d4 22

.

by PTS in T

By Theorem steps when

1 of
n
t=1

Gopalan DKL(uit

et vj

al. [13], uit vj)

the 

models (u, log T . This

v)  j happens

after

at

most

n



2

1+ 1-

2 d4

log

T

selections of (u, v)  j. Now we apply the same argument to all

j, M - 1 in total, and sum up the corresponding regrets.

5

Remarks:

Note

that

Theorem

1

implies

at

O(2M

1+ 1-

2 d4

log

T)

regret

bound

that

holds

with

high

probability. Here, d2 plays the role of a gap , the smallest possible difference between the expected

ratings of item j = j in any row i. In this sense, our result is O((1/2) log T ) and is of a

similar magnitude as the results in Gopalan et al. [13]. While we restrict u, v  (0, 1]Kx1 in

the proof, this does not affect the algorithm. In fact, the proof only focuses on high probability

events where the samples from the posterior are concentrated around the true parameters, and thus, are within (0, 1]Kx1 as well. Extending our proof to the general setting is not trivial. In particular,

moving from discretized parameters to continuous space introduces the abovementioned ill behaved

posteriors. While increasing the value of K will violate the fact that the best item will be the same

for all users, which allowed us to eliminate N from the regret bound.

5 Experiments and Results

The goal of our experimental evaluation is twofold: (i) evaluate the PTS algorithm for making online recommendations with respect to various baseline algorithms on several real-world datasets and (ii) understand the qualitative performance and intuition of PTS.

5.1 Dataset description
We use a synthetic dataset and five real world datasets to evaluate our approach. The synthetic dataset is generated as follows - At first we generate the user and item latent features (U and V ) of rank K by drawing from a Gaussian distribution N (0, u2) and N (0, v2) respectively. The true rating matrix is then R = U V T . We generate the observed rating matrix R from R by adding Gaussian noise N (0, 2) to the true ratings. We use five real world datasets as follows: Movielens 100k, Movielens 1M, Yahoo Music4, Book crossing5 and EachMovie as shown in Table 1.

# users # items # ratings

Movielens 100k Movielens 1M Yahoo Music Book crossing

943

6040

15400

6841

1682

3900

1000

5644

100k

1M 311,704 90k

Table 1: Characteristics of the datasets used in our study

EachMovie 36656 1621 2.58M

5.2 Baseline measures

There are no current approaches available that simultaneously learn both the user and item factors by sampling from the posterior in a bandit setting. From the currently available algorithms, we choose two kinds of baseline methods - one that sequentially updates the the posterior of the user features only while fixing the item features to a point estimate (ICF) and another that updates the MAP estimates of user and item features via stochastic gradient descent (SGD-Eps). A key challenge in online algorithms is unbiased offline evaluation. One problem in the offline setting is the partial information available about user feedback, i.e., we only have information about the items that the user rated. In our experiment, we restrict the recommendation space of all the algorithms to recommend among the items that the user rated in the entire dataset which makes it possible to empirically measure regret at every interaction. The baseline measures are as follows: 1) Random : At each iteration, we recommend a random movie to the user. 2) Most Popular : At each iteration, we recommend the most popular movie restricted to the movies rated by the user on the dataset. Note that this is an unrealistically optimistic baseline for an online algorithm as it is not possible to know the global popularity of the items beforehand. 3) ICF: The ICF algorithm [2] proceeds by first estimating the user and item latent factors (U and V ) on a initial training period and then for every interaction thereafter only updates the user features (U ) assuming the item features (V ) as fixed. We run two scenarios for the ICF algorithm one in which we use 20% (icf-20) and 50% (icf-50) of the data as the training period respectively. During this period of training, we randomly recommend a movie to the user to compute the regret. We use the PMF implementation by [5] for estimating the U and V . 4) SGD-Eps: We learn the latent factors using an online variant of the PMF algorithm [5]. We use the stochastic gradient descent to update the latent factors with a mini-batch size of 50. In order to make a recommendation, we use the -greedy strategy and recommend the highest UiV T with a probability and make a random recommendations otherwise. ( is set as 0.95 in our experiments.)
4http://webscope.sandbox.yahoo.com/ 5http://www.bookcrossing.com

6

5.3 Results on Synthetic Dataset
We generated the synthetic dataset as mentioned earlier and run the PTS algorithm with 100 particles for recommendations. We simulate the setting as mentioned in Section 3 and assume that at time t, a random user it arrives and the system recommends an item jt. The user rates the recommended item rit,jt and we evaluate the performance of the model by computing the expected cumulative regret defined in Eq(6). Fig. 2 shows the cumulative regret of the algorithm on the synthetic data averaged over 100 runs using different size of the matrix and latent features K. The cumulative regret increases sub-linearly with the number of interactions and this gives us confidence that our approach works well on the synthetic dataset.

Cummulative Regret Cummulative Regret Cummulative Regret
Cummulative Regret Cummulative Regret

60 50 40 30 20 10
00 20 40 60 80 100 Iterations
(a) N, M=10,K=1

200 180 160 140 120 100
80 60 40 20
0 0

100 200 300 400 500 Iterations

(b) N, M=20,K=1

450 400 350 300 250 200 150 100
50 0 0

200 400 600 800 1000 Iterations

(c) N, M=30,K=1

120 100
80 60 40 20
00 20 40 60 80 100 Iterations
(d) N, M=10,K=2

140 120 100
80 60 40 20
0 0 20 40 60 80 100
Iterations
(e) N, M=10,K=3

Figure 2: Cumulative regret on different sizes of the synthetic data and K averaged over 100 runs.

5.4 Results on Real Datasets

Cummulative Regret Cummulative Regret Cummulative Regret

15 x 104 PTS random popular icf-20 icf-50
10 sgd-eps PTS-B

15 x 105 PTS random popular icf-20 icf-50
10 sgd-eps PTS-B

6 x 105 PTS random
5 popular icf-20 icf-50
4 sgd-eps PTS-B
3

5 52

1

0 0 2 4 6 8 10

Iterations

x 104

(a) Movielens 100k

0 0 2 4 6 8 10 12

Iterations

x 105

(b) Movielens 1M

0 0 0.5 1 1.5 2 2.5 3 3.5

Iterations

x 105

(c) Yahoo Music

Cummulative Regret Cummulative Regret

18 x 104 PTS
16 random popular
14 icf-50 sgd-eps
12 PTS-B
10
8
6
4
2
0 02468
Iterations
(d) Book Crossing

10 x 104

6 x 106 PTS random
5 popular icf-20 icf-50
4 sgd-eps PTS-B
3

2

1

0 0 0.5 1 1.5 2 2.5 3

Iterations

x 106

(e) EachMovie

Figure 3: Comparison with baseline methods on five datasets.
Next, we evaluate our algorithms on five real datasets and compare them to the various baseline algorithms. We subtract the mean ratings from the data to centre it at zero. To simulate an extreme cold-start scenario we start from an empty set of user and rating. We then iterate over the datasets and assume that a random user it has arrived at time t and the system recommends an item jt constrained to the items rated by this user in the dataset. We use K = 2 for all the algorithms and use 30 particles for our approach. For PTS we set the value of 2 = 0.5 and u2 = 1, v2 = 1. For PTS-B (Bayesian version, see Algo. 1 for more details), we set 2 = 0.5 and the initial shape parameters of the Gamma distribution as  = 2 and  = 0.5. For both ICF-20 and ICF-50, we set 2 = 0.5 and u2 = 1. Fig. 3 shows the cumulative regret of all the algorithms on the five datasets6. Our approach performs significantly better as compared to the baseline algorithms on this diverse set of datasets. PTS-B with no parameter tuning performs slightly better than PTS and achieves the best regret. It is important to note that both PTS and PTS-B performs comparable to or even better than the "most popular" baseline despite not knowing the global popularity in advance. Note that ICF is very sensitive to the length of the initial training period; it is not clear how to set this apriori.

6ICF-20 fails to run on the Bookcrossing dataset as the 20% data is too sparse for the PMF implementation.

7

MSE MSE

4 test error pmf
3.5
3
2.5
2
1.5
1
0.5 -200 0 200 400 600 800 1000
Iterations x 1000
(a) Movielens 1M

4

RB 0.6

No RB

ICF-20

3 PTS-20 0.4 PTS-100

2 0.2

10

0
-1
-2 0 20 40 60 80 Iterations
(b) RB particle filter

-0.2 -0.4 -0.6 100 -0.8
-0.4 -0.2 0 0.2 0.4 0.6
(c) Movie feature vector

Figure 4: a) shows MSE on movielens 1M dataset, the red line is the MSE using the PMF algorithm b) shows performance of a RBPF (blue line) as compared to vanilla PF (red line) on a synthetic dataset N,M=10 and c) shows movie feature vectors for a movie with 384 ratings, the red dot is the feature vector from the ICF-20 algorithm (using 73 ratings). PTS-20 is the feature vector at 20% of the data (green dots) and PTS-100 at 100% (blue dots).

We also evaluate the performance of our model in an offline setting as follows: We divide the datasets into training and test set and iterate over the training data triplets (it, jt, rt) by pretending that jt is the movie recommended by our approach and update the latent factors according to RBPF. We compute the recovered matrix R as the average prediction U V T from the particles at each time step and compute the mean squared error (MSE) on the test dataset at each iteration. Unlike the batch method such as PMF which takes multiple passes over the data, our method was designed to have bounded update complexity at each iteration. We ran the algorithm using 80% data for training and the rest for testing and computed the MSE by averaging the results over 5 runs. Fig. 4(a) shows the average MSE on the movielens 1M dataset. Our MSE (0.7925) is comparable to the PMF MSE (0.7718) as shown by the red line. This demonstrates that the RBPF is performing reasonably well for matrix factorization. In addition, Fig. 4(b) shows that on the synthetic dataset, the vanilla PF suffers from degeneration as seen by the high variance. To understand the intuition why fixing the latent item features V as done in the ICF does not work, we perform an experiment as follows: We run the ICF algorithm on the movielens 100k dataset in which we use 20% of the data for training. At this point the ICF algorithm fixes the item features V and only updates the user features U . Next, we run our algorithm and obtain the latent features. We examined the features for one selected movie from the particles at two time intervals - one when the ICF algorithm fixes them at 20% and another one in the end as shown in the Fig. 4(c). It shows that movie features have evolved into a different location and hence fixing them early is not a good idea.
6 Related Work
Probabilistic matrix completion in a bandit setting setting was introduced in the previous work by Zhao et al. [2]. The ICF algorithm in [2] approximates the posterior of the latent item features by a single point estimate. Several other bandit algorithms for recommendations have been proposed. Valko et al. [14] proposed a bandit algorithm for content-based recommendations. In this approach, the features of the items are extracted from a similarity graph over the items, which is known in advance. The preferences of each user for the features are learned independently by regressing the ratings of the items from their features. The key difference in our approach is that we also learn the features of the items. In other words, we learn both the user and item factors, U and V , while [14] learn only U . Kocak et al. [15] combine the spectral bandit algorithm in [14] with TS. Gentile et al. [16] propose a bandit algorithm for recommendations that clusters users in an online fashion based on the similarity of their preferences. The preferences are learned by regressing the ratings of the items from their features. The features of the items are the input of the learning algorithm and they only learn U . Maillard et al. [17] study a bandit problem where the arms are partitioned into unknown clusters unlike our work which is more general.
7 Conclusion
We have proposed an efficient method for carrying out matrix factorization (M  U V T ) in a bandit setting. The key novelty of our approach is the combined use of Rao-Blackwellized particle filtering and Thompson sampling (PTS) in matrix factorization recommendation. This allows us to simultaneously update the posterior probability of U and V in an online manner while minimizing the cumulative regret. The state of the art, till now, was to either use point estimates of U and V or use a point estimate of one of the factor (e.g., U ) and update the posterior probability of the other (V ). PTS results in substantially better performance on a wide variety of real world data sets.

8

References
[1] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30-37, 2009.
[2] Xiaoxue Zhao, Weinan Zhang, and Jun Wang. Interactive collaborative filtering. In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pages 1411-1420. ACM, 2013.
[3] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In NIPS, pages 2249-2257, 2011.
[4] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In ICML (3), pages 127-135, 2013.
[5] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. In NIPS, volume 1, pages 2-1, 2007.
[6] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo. In ICML, pages 880-887, 2008.
[7] Nicolas Chopin. A sequential particle filter method for static models. Biometrika, 89(3):539- 552, 2002.
[8] Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential monte carlo samplers. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411-436, 2006.
[9] Arnaud Doucet, Nando De Freitas, Kevin Murphy, and Stuart Russell. Rao-blackwellised particle filtering for dynamic bayesian networks. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 176-183. Morgan Kaufmann Publishers Inc., 2000.
[10] A. Gelman and X. L Meng. A note on bivariate distributions that are conditionally normal. Amer. Statist., 45:125-126, 1991.
[11] B. C. Arnold, E. Castillo, J. M. Sarabia, and L. Gonzalez-Vega. Multiple modes in densities with normal conditionals. Statist. Probab. Lett., 49:355-363, 2000.
[12] B. C. Arnold, E. Castillo, and J. M. Sarabia. Conditionally specified distributions: An introduction. Statistical Science, 16(3):249-274, 2001.
[13] Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online problems. In Proceedings of The 31st International Conference on Machine Learning, pages 100-108, 2014.
[14] Michal Valko, Remi Munos, Branislav Kveton, and Tomas Kocak. Spectral bandits for smooth graph functions. In 31th International Conference on Machine Learning, 2014.
[15] Tomas Kocak, Michal Valko, Remi Munos, and Shipra Agrawal. Spectral thompson sampling. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.
[16] Claudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. arXiv preprint arXiv:1401.8257, 2014.
[17] Odalric-Ambrym Maillard and Shie Mannor. Latent bandits. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 136-144, 2014.
9

