Adaptive Primal-Dual Splitting Methods for Statistical Learning and Image Processing

Thomas Goldstein Department of Computer Science
University of Maryland College Park, MD

Min Li School of Economics and Management
Southeast University Nanjing, China

Xiaoming Yuan Department of Mathematics Hong Kong Baptist University Kowloon Tong, Hong Kong

Abstract
The alternating direction method of multipliers (ADMM) is an important tool for solving complex optimization problems, but it involves minimization sub-steps that are often difficult to solve efficiently. The Primal-Dual Hybrid Gradient (PDHG) method is a powerful alternative that often has simpler sub-steps than ADMM, thus producing lower complexity solvers. Despite the flexibility of this method, PDHG is often impractical because it requires the careful choice of multiple stepsize parameters. There is often no intuitive way to choose these parameters to maximize efficiency, or even achieve convergence. We propose self-adaptive stepsize rules that automatically tune PDHG parameters for optimal convergence. We rigorously analyze our methods, and identify convergence rates. Numerical experiments show that adaptive PDHG has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user.
1 Introduction
Splitting methods such as ADMM [1, 2, 3] have recently become popular for solving problems in distributed computing, statistical regression, and image processing. ADMM allows complex problems to be broken down into sequences of simpler sub-steps, usually involving large-scale least squares minimizations. However, in many cases these least squares minimizations are difficult to directly compute. In such situations, the Primal-Dual Hybrid Gradient method (PDHG) [4, 5], also called the linearized ADMM [4, 6], enables the solution of complex problems with a simpler sequence of sub-steps that can often be computed in closed form. This flexibility comes at a cost - the PDHG method requires the user to choose multiple stepsize parameters that jointly determine the convergence of the method. Without having extensive analytical knowledge about the problem being solved (such as eigenvalues of linear operators), there is no intuitive way to select stepsize parameters to obtain fast convergence, or even guarantee convergence at all. In this article we introduce and analyze self-adaptive variants of PDHG - variants that automatically tune stepsize parameters to attain (and guarantee) fast convergence without user input. Applying adaptivity to splitting methods is a difficult problem. It is known that naive adaptive variants of
tomg@cs.umd.edu limin@seu.edu.cn xmyuan@hkbu.edu.hk
1

ADMM are non-convergent, however recent results prove convergence when specific mathematical requirements are enforced on the stepsizes [7]. Despite this progress, the requirements for convergence of adaptive PDHG have been unexplored. This is surprising, given that stepsize selection is a much bigger issue for PDHG than for ADMM because it requires multiple stepsize parameters. The contributions of this paper are as follows. First, we describe applications of PDHG and its advantages over ADMM. We then introduce a new adaptive variant of PDHG. The new algorithm not only tunes parameters for fast convergence, but contains a line search that guarantees convergence when stepsize restrictions are unknown to the user. We analyze the convergence of adaptive PDHG, and rigorously prove convergence rate guarantees. Finally, we use numerical experiments to show the advantages of adaptivity on both convergence speed and ease of use.

2 The Primal-Dual Hybrid Gradient Method

The PDHG scheme has its roots in the Arrow-Hurwicz method, which was studied by Popov [8]. Research in this direction was reinvigorated by the introduction of PDHG, which converges rapidly for a wider range of stepsizes than Arrow-Hurwicz. PDHG was first presented in [9] and analyzed for convergence in [4, 5]. It was later studied extensively for image segmentation [10]. An extensive technical study of the method and its variants is given by He and Yuan [11]. Several extensions of PDHG, including simplified iterations for the case that f or g is differentiable, are presented by Condat [12]. Several authors have also derived PDHG as a preconditioned form of ADMM [4, 6].

PDHG solves saddle-point problems of the form

min max f (x) + yT Ax g(y).
x2X y2Y

(1)

for convex f and g. We will see later that an incredibly wide range of problems can be cast as (1).

The steps of PDHG are given by

8 >>>>>>><

xk+1 xk+1

= =

xk kAT yk
arg min f (x) +
x2X

1 2k

kx

>>>>>>>:

yk+1 yk+1

= =

yk + kA(2xk+1

arg min
y2Y

g(y)

+

1 2k

xk ) ky

xk+1k2 yk+1k2

(2) (3)
(4) (5)

where {k} and { k} are stepsize parameters. Steps (2) and (3) of the method update x, decreasing the energy (1) by first taking a gradient descent step with respect to the inner product term in (1) and then taking a "backward" or proximal step involving f . In steps (4) and (5), the energy (1) is increased by first marching up the gradient of the inner product term with respect to y, and then a backward step is taken with respect to g.

PDHG has been analyzed in the case of constant stepsizes, k =  and k = . In particular, it is known to converge as long as  < 1/(AT A) [4, 5, 11]. However, PDHG typically does not converge when non-constant stepsizes are used, even in the case that kk < 1/(AT A) [13]. Furthermore, it is unclear how to select stepsizes when the spectral properties of A are unknown. In this article, we identify the specific stepsize conditions that guarantee convergence in the presence of adaptivity, and propose a backtracking scheme that can be used when the spectral radius of A is unknown.

3 Applications

Linear Inverse Problems Many inverse problems and statistical regressions have the form

minimize h(Sx) + f (Ax b)

(6)

where f (the data term) is some convex function, h is a (convex) regularizer (such as the 1-norm), A and S are linear operators, and b is a vector of data. Recently, the alternating direction method

2

of multipliers (ADMM) has become a popular method for solving such problems. The ADMM relies on the change of variables y Sx, and generates the following sequence of iterates for some stepsize 

8<xk+1 :ykk++11

= arg minx f (Ax

b) + (Sx

y k )T

k

+

 2

kSx

= arg miny h(y) + (Sxk+1

y)T

k

+

 2

kS

xk+1

= k +  (Sxk+1 yk+1).

y k k2 yk2

(7)

The x-update in (7) requires the solution of a (potentially large) least-square problem involving both A and S. Common formulations such as the consensus ADMM [14] solve these large sub-problems with direct matrix factorizations, however this is often impractical when either the data matrices are extremely large or fast transforms (such as FFT, DCT, or Hadamard) cannot be used.

The problem (6) can be put into the form (1) using the Fenchel conjugate of the convex function h, denoted h, which satisfies the important identity

h(z) = max yT z h(y)
y

for all z in the domain of h. Replacing h in (6) with this expression involving its conjugate yields

min max f (Ax b) + yT Sx h(y)
xy

which is of the form (1). The forward (gradient) steps of PDHG handle the matrix A explicitly, allowing linear inverse problems to be solved without any difficult least-squares sub-steps. We will see several examples of this below.

Scaled Lasso The square-root lasso [15] or scaled lasso [16] is a variable selection regression that

obtains sparse solutions to systems of linear equations. Scaled lasso has several advantages over

classical lasso - it is more robust to noise and it enables setting penalty parameters without cross

validation [15, 16]. Given a data matrix D and a vector b, the scaled lasso finds a sparse solution to

the system Dx = b by solving

min
x

kxk1

+

kDx

bk2

(8)

for some scaling parameter . Note the 2 term in (8) is not squared as in classical lasso. If we write

kxk1

=

max
ky1 k1 

y1T x,

and

kDx

bk2

=

max
ky2 k2 1

y2T (Dx

b)

we can put (8) in the form (1)

min max
x ky1k1,ky2k21

y1T x + y2T (Dx

b).

Unlike ADMM, PDHG does not require the solution of least-squares problems involving D.

(9)

Total-Variation Minimization Total variation [17] is commonly used to solve problems of the

form

min
x

krxk1

+

1 2

kAx

f k22

(10)

where x is a 2D array (image), r is the discrete gradient operator, A is a linear operator, and f

contains data. If we add a dual variable y and write krxk1 = maxkyk1 yT rx, we obtain

max min
kyk1 x

1 2

kAx

f k2 + yT rx

(11)

which is clearly of the form (1).

The PDHG solver using formulation (11) avoids the inversion of the gradient operator that is required by ADMM. This is useful in many applications. For example, in compressive sensing the matrix A may be a sub-sampled orthogonal Hadamard [18], wavelet, or Fourier transform [19, 20]. In this case, the proximal sub-steps of PDHG are solvable in closed form using fast transforms because they do not involve the gradient operator r. The sub-steps of ADMM involve both the gradient operator and the matrix A simultaneously, and thus require inner loops with expensive iterative solvers.

3

4 Adaptive Formulation

The convergence of PDHG can be measured by the size of the residuals, or gradients of (1) with respect to the primal and dual variables x and y. These primal and dual gradients are simply

pk+1 = @f (xk+1) + AT yk+1, and dk+1 = @g(yk+1) + Axk+1

(12)

where @f and @g denote the sub-differential of f and g. The sub-differential can be directly evalu-

ated from the sequence of PDHG iterates using the optimality condition for (3): 0 2 @f (xk+1) +

1 k

(xk+1

xk+1).

Rearranging

this

yields

1 k

(xk+1

xk+1) 2 @f (xk+1). The same method can be

applied to (5) to obtain @g(yk+1). Applying these results to (12) yields the closed form residuals

pk+1

=

1 k

(xk

xk+1)

AT (yk

yk+1),

dk+1 = 1 (yk
k

yk+1)

A(xk

xk+1). (13)

When choosing the stepsize for PDHG, there is a tradeoff between the primal and dual residuals. Choosing a large k and a small k drives down the primal residuals at the cost of large dual residuals. Choosing a small k and large k results in small dual residuals but large primal errors. One would like to choose stepsizes so that the larger of pk+1 and dk+1 is as small as possible. If we assume the residuals on step k+1 change monotonically with k, then max{pk+1, dk+1} is minimized when pk+1 = dk+1. This suggests that we tune k to "balance" the primal and dual residuals. To achieve residual balancing, we first select a parameter 0 < 1 that controls the aggressiveness of adaptivity. On each iteration, we check whether the primal residual is at least twice the dual. If so, we increase the primal stepsize to k+1 = k/(1 k) and decrease the dual to k+1 = k(1 k). If the dual residual is at least twice the primal, we do the opposite. When we modify the stepsize, we shrink the adaptivity level to k+1 = k, for  2 (0, 1). We will see in Section 5 that this adaptivity level decay is necessary to guarantee convergence. In our implementation we use 0 =  = .95. In addition to residual balancing, we check the following backtracking condition after each iteration

c 2k

kxk+1

xk k2

2(yk+1

yk)T A(xk+1

xk )

+

c 2k

kyk+1

ykk2 > 0

(14)

where c 2 (0, 1) is a constant (we use c = 0.9) is our experiments. If condition (14) fails, then we shrink k and k before the next iteration. We will see in Section 5 that the backtracking condition (14) is sufficient to guarantee convergence. The complete scheme is listed in Algorithm 1.

Algorithm 1 Adaptive PDHG

1: 2:

Cwhhoiloeskepxk0k,,yk0d,klkar>geto0learnadnc0e,

and do

set

0

=

= 0.95.

3: Compute (xk+1, yk+1) from (xk, yk) using the PDHG updates (2-5)

4: Check the backtracking condition (14) and if it fails set k k/2, k k/2

5: Compute the residuals (13), and use them for the following two adaptive updates

6: 7:

If If

2kpk+1k < kdk+1k, kpk+1k > 2kdk+1k,

then then

set set

k+1 k+1

= =

k (1 k /(1

k), k+1 = k/(1 k), k+1 = k(1

k), and k+1 = k k), and k+1 = k

8: 9:

If end

no adaptive while

updates

were

triggered,

then

k+1

= k,

k+1 =

k, and k+1 = k

5 Convergence Theory

In this section, we analyze Algorithm 1 and its rate of convergence. In our analysis, we consider adaptive variants of PDHG that satisfy the following assumptions. We will see later that these assumptions guarantee convergence of PDHG with rate O(1/k).

Algorithm 1 trivially satisfies Assumption A. The sequence { k} measures the adaptive aggressive-

ness on ensures

iteration k, and that Assumption

Bserhvoelsdst.heThseambaecrkotrleacaksingkruinle

Algorithm 1. The geometric decay explicitly guarantees Assumption C.

of

k

4

Assumptions for Adaptive PDHG

A The sequences {k} and { k} are positive and bounded.

no

B The sequence { k} is summable, where

k = max

,k k+1
k

k

k+1 , 0
k

.

C Either X or Y is bounded, and there is a constant c 2 (0, 1) such that for all k > 0

c 2k

kxk+1

xk k2

2(yk+1

yk)T A(xk+1

xk )

+

c 2k

kyk+1

ykk2 > 0.

5.1 Variational Inequality Formulation

For notational simplicity, we define the composite vector uk = (xk, yk) and the matrices

Mk

=

 k

1I A

AT k 1I



,

Hk

=

 k

1I

0

 

0 k 1I

, and Q(u) =

AT y Ax

.

(15)

This notation allows us to formulate the optimality conditions for (1) as a variational inequality (VI). If u? = (x?, y?) is a solution to (1), then x? is a minimizer of (1). More formally,

f (x) f (x?) + (x x?)T AT y? 0 8 x 2 X. Likewise, (1) is maximized by y?, and so
g(y) + g(y?) + (y y?)T Ax?  0 8 y 2 Y. Subtracting (17) from (16) and letting h(u) = f (x) + g(y) yields the VI formulation

(16) (17)

h(u) h(u?) + (u u?)T Q(u?) 0 8u 2 , where  = X  Y. We say u is an approximate solution to (1) with VI accuracy  if

(18)

h(u) h(u) + (u u)T Q(u)  8u 2 B1(u) \ ,

(19)

where B1(u) is a unit ball centered at u. In Theorem 1, we prove O(1/k) ergodic convergence of adaptive PDHG using the VI notion of convergence.

5.2 Preliminary Results

We now prove several results about the PDHG iterates that are needed to obtain a convergence rate. Lemma 1. The iterates generated by PDHG (2-5) satisfy
kuk u?k2Mk kuk+1 ukk2Mk + kuk+1 u?k2Mk .

The proof of this lemma follows standard techniques, and is presented in the supplementary material. This next lemma bounds iterates generated by PDHG. Lemma 2. Suppose the stepsizes for PDHG satisfy Assumptions A, B and C. Then

for some upper bound CU > 0.

kuk u?k2Hk  CU

The proof of this lemma is given in the supplementary material.

Lemma 3. Under Assumptions A, B, and C, we have

where C

Xn 



kuk uk2Mk kuk uk2Mk 1  2C CU + 2C CH ku

k=1

=

P1
k=0

k and CH is a constant such that ku

u?k2Hk  CH ku

u?k2 u?k2.

5

Proof. Using the definition of Mk we obtain

Xn 



kuk uk2Mk kuk uk2Mk 1

k=1

= 

Xn
k=1
Xn
k=1



(

1 k

k1

1 k

1

)kxk



1 k

kxk

xk2

xk2 + ( 1
k
+ 1 kyk
k

1 )kyk
k1

yk2

Xn = k 1kuk
k=1

uk2Hk

Xn  2 k 1 kuk
k=1

u?k2Hk + ku

u?k2Hk

Xn  2 k 1 CU + CH ku u?k2

k=1

 2C CU + 2C CH ku u?k2,

where we have used the bound kuk u?k2Hk  CU from Lemma 2 and C

yk2

=

P1
k=0

k.

(20)

This final lemma provides a VI interpretation of the PDHG iteration. Lemma 4. The iterates uk = (xk, yk) generated by PDHG satisfy
h(u) h(uk+1) + (u uk+1)T [Quk+1 + Mk(uk+1 uk)]

0

8u 2 .

(21)

Proof. Let uk = (xk, yk) be a pair of PDHG iterates. The minimizers in (3) and (5) of PDHG satisfy the following for all x 2 X

f (x) f (xk+1) + (x xk+1)T [AT yk+1 AT (yk+1 yk) + 1 (xk+1 xk)] 0, k
and also for all y 2 Y

(22)

g(y) g(yk+1) + (y yk+1)T [ Axk+1 A(xk+1 xk) + 1 (yk+1 yk)] 0.
k
Adding these two inequalities and using the notation (15) yields the result.

(23)

5.3 Convergence Rate

We now combine the above lemmas into our final convergence result.

Theorem 1. Suppose that the stepsizes in PDHG satisfy Assumptions A, B, and C. Consider the sequence defined by

ut

=

1 t

Xt uk .

k=1

This sequence satisfies the convergence bound

h(u) h(ut) + (u ut)T Q(ut)

ku utk2Mt

ku u0k2M0 2C CU 2t

Thus ut converges to a solution of (1) with rate O(1/k) in the VI sense (19).

2C CH ku

u?k2 .

6

Proof. We begin with the following identity (a special case of the polar identity for vector spaces):

(u

uk+1)T Mk(uk

uk+1)

=

1 2

(ku

uk+1k2Mk

ku

ukk2Mk )

+

1 2

kuk

We apply this to the VI formulation of the PDHG iteration (18) to get

uk+1k2Mk .

h(u) h(uk+1) + (u uk+1)T Q(uk+1)

1 2

ku

uk+1k2Mk

ku

uk k2Mk

+

1 2

kuk

uk+1k2Mk . (24)

Note that

(u uk+1)T Q(u uk+1) = (x xk+1)AT (y yk+1) (y yk+1)A(x xk+1) = 0, (25)

and so (u uk+1)T Q(u) = (u uk+1)T Q(uk+1). Also, Assumption C guarantees that kuk uk+1k2Mk 0. These observations reduce (24) to

h(u) h(uk+1) + (u uk+1)T Q(u)

1 2

ku

uk+1k2Mk

ku ukk2Mk .

(26)

We now sum (26) for k = 0 to t 1, and invoke Lemma 3,

Xt 1 2 [h(u) h(uk+1) + (u uk+1)T Q(u)]

k=0

Xt h

i

ku utk2Mt ku u0k2M0 +

ku ukk2Mk 1 ku ukk2Mk

k=1

ku utk2Mt ku u0k2M0 2C CU 2C CH ku u?k2.

Because h is convex,

Xt 1 Xt h(uk+1) = h(uk)

k=0

k=1

!

th

1 Xt uk t

= th(ut).

k=1

(27)

The left side of (27) therefore satisfies

2t h(u) h(ut) + (u ut)T Q(u)

Xt 1  2 h(u)

k=0

Combining (27) and (28) yields the tasty bound

h(uk+1) + (u

uk+1

)T

 Q(u)

.

(28)

h(u) h(ut) + (u ut)T Q(u)

ku utk2Mt

ku u0k2M0 2C CU 2t

2C CH ku

u?k2 .

Applying (19) proves the theorem.

6 Numerical Results

We apply the original and adaptive PDHG to the test problems described in Section 3. We terminate

the algorithms when both the primal and dual residual norms (i.e. kpkk and kdkk) are smaller

than 0.05. We consider four variants of PDHG. The method "Adapt:Backtrack" denotes adaptive

PDHG with backtracking. The method "Adapt:  = L" refers to the adaptive method without

backtracking with 0 =

0 = 0.95(AT A)

.1
2

We ,

a=lsopcoLn"sirdeefrertshetontohne-acdoanpsttiavnet-PsDteHpsGizewimthetthwood dwififtehrebnotthstesptespizseizcehpoaicraems. eTtehres

meqeutahlodto"pCoLns=t:

(AT A)

1 2

.

The

method

"Const:



-final"

refers

to

the

constant-stepsize

method,

where

the

stepsizes

are chosen to be the final values of the stepsizes used by "Adapt:  = L." This final method is

meant to demonstrate the performance of PDHG with a stepsize that is customized to the problem

at hand, but still non-adaptive. The specifics of each test problem are described below:

7

107 ROF Convergence Curves,  = 0.05

12

Primal Stepsize (k)

Energy Gap k

106 105 104 103 102

Adapt:Backtrack Adapt:  = L
Con st:  = L Const: -final

10 8 6 4

Adapt:Backtrack Adapt:  = L

101 2

100 0

0 50 100 150 200 250 300

0 50 100 150 200 250 300

Iteration

Iteration

Figure 1: (left) Convergence curves for the TV denoising experiment with  = 0.05. The y-axis

displays the difference between the objective (10) at the kth iterate and the optimal objective value.

(right) Stepsize sequences, {k}, for both adaptive schemes.

Table 1: Iteration counts for each problem with runtime (sec) in parenthesis.

Problem

Adapt:Backtrack

Adapt:  = L

Const: ,

p =L

Const:  -final

Scaled Lasso (50%) Scaled Lasso (20%) Scaled Lasso (10%)
TV,  = .25 TV,  = .05 TV,  = .01 Compressive (20%) Compressive (10%) Compressive (5%)

212 (0.33) 349 (0.22) 360 (0.21) 16 (0.0475) 50 (0.122) 109 (0.262) 163 (4.08) 244 (5.63) 382 (9.54)

240 (0.38) 330 (0.21) 322 (0.18) 16 (0.041) 51 (0.122) 122 (0.288) 168 (4.12) 274 (6.21) 438 (10.7)

342 (0.60) 437 (0.25) 527 (0.28) 78 (0.184) 281 (0.669) 927 (2.17) 501 (12.54) 908 (20.6) 1505 (34.2)

156 (0.27) 197 (0.11) 277 (0.15) 48 (0.121) 97 (0.228) 152 (0.369) 246 (6.03) 437 (9.94) 435 (9.95)

Scaled Lasso We test our methods on (8) using the synthetic problem suggested in [21]. The test problem recovers a 1000 dimensional vector with 10 nonzero components using a Gaussian matrix.

Total Variation Minimization We apply the model (10) with A = I to the "Cameraman" image. The image is scaled to the range [0, 255], and noise contaminated with standard deviation 10. The image is denoised with  = 0.25, 0.05, and 0.01. See Table 1 for time trial results. Note the similar performance of Algorithm 1 with and without backtracking, indicating that there is no advantage to knowing the constant L = (AT A) 1. We plot convergence curves and show the evolution of k in Figure 1. Note that k is large for the first several iterates and then decays over time. Compressed Sensing We reconstruct a Shepp-Logan phantom from sub-sampled Hadamard measurements. Data is generated by applying the Hadamard transform to a 256  256 discretization of the Shepp-Logan phantom, and then sampling 5%, 10%, and 20% of the coefficients are random.

7 Discussion and Conclusion
Several interesting observations can be made from the results in Table 1. First, both the backtracking ("Adapt: Backtrack") and non-backtracking ("Adapt:  = L") methods have similar performance on average for the imaging problems, with neither algorithm showing consistently better performance. Thus there is no cost to using backtracking instead of knowing the ideal stepsize (AT A).
Finally, the method "Const:  -final" (using non-adaptive, "optimized" stepsizes) did not always outperform the constant, non-optimized stepsizes. This occurs because the true "best" stepsize choice depends on the active set of the problem and the structure of the remaining error and thus evolves over time. This is depicted in Figure 1, which shows the time dependence of k. This show that adaptive methods can achieve superior performance by evolving the stepsize over time.
8 Acknowledgments
This work was supported by the National Science Foundation ( #1535902), the Office of Naval Research (#N00014-15-1-2676), and the Hong Kong Research Grants Council's General Research Fund (HKBU 12300515). The second author was supported in part by the Program for New Century Excellent University Talents under Grant No. NCET-12-0111, and the Qing Lan Project.

8

References
[1] R. Glowinski and A. Marroco. Sur l'approximation, par elements finis d'ordre un, et la resolution, par penalisation-dualite d'une classe de problemes de Dirichlet non lineaires. Rev. Francaise d'Automat. Inf. Recherche Operationelle, 9(2):41-76, 1975.
[2] Roland Glowinski and Patrick Le Tallec. Augmented Lagrangian and Operator-Splitting Methods in Nonlinear Mechanics. Society for Industrial and Applied Mathematics, Philadephia, PA, 1989.
[3] Tom Goldstein and Stanley Osher. The Split Bregman method for 1 regularized problems. SIAM J. Img. Sci., 2(2):323-343, April 2009.
[4] Ernie Esser, Xiaoqun Zhang, and Tony F. Chan. A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science. SIAM Journal on Imaging Sciences, 3(4):1015- 1046, 2010.
[5] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Convergence, 40(1):1-49, 2010.
[6] Yuyuan Ouyang, Yunmei Chen, Guanghui Lan, and Eduardo Pasiliao Jr. An accelerated linearized alternating direction method of multipliers. arXiv preprint arXiv:1401.6607, 2014.
[7] B. He, H. Yang, and S.L. Wang. Alternating direction method with self-adaptive penalty parameters for monotone variational inequalities. Journal of Optimization Theory and Applications, 106(2):337-356, 2000.
[8] L.D. Popov. A modification of the arrow-hurwicz method for search of saddle points. Mathematical notes of the Academy of Sciences of the USSR, 28:845-848, 1980.
[9] Mingqiang Zhu and Tony Chan. An efficient primal-dual hybrid gradient algorithm for total variation image restoration. UCLA CAM technical report, 08-34, 2008.
[10] T. Pock, D. Cremers, H. Bischof, and A. Chambolle. An algorithm for minimizing the mumford-shah functional. In Computer Vision, 2009 IEEE 12th International Conference on, pages 1133-1140, 2009.
[11] Bingsheng He and Xiaoming Yuan. Convergence analysis of primal-dual algorithms for a saddle-point problem: From contraction perspective. SIAM J. Img. Sci., 5(1):119-149, January 2012.
[12] Laurent Condat. A primal-dual splitting method for convex optimization involving lipschitzian, proximable and linear composite terms. Journal of Optimization Theory and Applications, 158(2):460-479, 2013.
[13] Silvia Bonettini and Valeria Ruggiero. On the convergence of primal-dual hybrid gradient algorithms for total variation image restoration. Journal of Mathematical Imaging and Vision, 44(3):236-253, 2012.
[14] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Foundations and Trends in Machine Learning, 2010.
[15] A. Belloni, Victor Chernozhukov, and L. Wang. Square-root lasso: pivotal recovery of sparse signals via conic programming. Biometrika, 98(4):791-806, 2011.
[16] Tingni Sun and Cun-Hui Zhang. Scaled sparse linear regression. Biometrika, 99(4):879-898, 2012. [17] L Rudin, S Osher, and E Fatemi. Nonlinear total variation based noise removal algorithms. Physica. D.,
60:259-268, 1992. [18] Tom Goldstein, Lina Xu, Kevin Kelly, and Richard Baraniuk. The STONE transform: Multi-resolution
image enhancement and real-time compressive video. Preprint available at Arxiv.org (arXiv:1311.34056), 2013. [19] M. Lustig, D. Donoho, and J. Pauly. Sparse MRI: The application of compressed sensing for rapid MR imaging. Magnetic Resonance in Medicine, 58:1182-1195, 2007. [20] Xiaoqun Zhang and J. Froment. Total variation based fourier reconstruction and regularization for computer tomography. In Nuclear Science Symposium Conference Record, 2005 IEEE, volume 4, pages 2332-2336, Oct 2005. [21] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267-288, 1994.
9

