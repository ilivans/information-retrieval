Fighting Bandits with a New Kind of Smoothness

Jacob Abernethy University of Michigan jabernet@umich.edu

Chansoo Lee University of Michigan chansool@umich.edu

Ambuj Tewari University of Michigan tewaria@umich.edu

Abstract

We provide a new analysis framework for the adversarial multi-armed bandit

problem. Using the notion of convex smoothing, we define a novel family of

algorithms with minimax optimal regret guarantees. First, we show that regular-

itzhaetiOon(pviNa

the T)

Tsallis entropy, which minimax regret with a

includes EXP3 as a special case, matches smaller constant factor. Second, we show

tahsaOt a(pwNidTe

class of log N ),

perturbation methods achieve a near-optimal as long as the perturbation distribution has a

regret as bounded

low haz-

ard function. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma

distributions all satisfy this key property and lead to near-optimal algorithms.

1 Introduction

The classic multi-armed bandit (MAB) problem, generally attributed to the early work of Robbins (1952), poses a generic online decision scenario in which an agent must make a sequence of choices from a fixed set of options. After each decision is made, the agent receives some feedback in the form of a loss (or gain) associated with her choice, but no information is provided on the outcomes of alternative options. The agent's goal is to minimize the total loss over time, and the agent is thus faced with the balancing act of both experimenting with the menu of choices while also utilizing the data gathered in the process to improve her decisions. The MAB framework is not only mathematically elegant, but useful for a wide range of applications including medical experiments design (Gittins, 1996), automated poker playing strategies (Van den Broeck et al., 2009), and hyperparameter tuning (Pacula et al., 2012).

Early MAB results relied on stochastic assumptions (e.g., IID) on the loss sequence (Auer et al., 2002; Gittins et al., 2011; Lai and Robbins, 1985). As researchers began to establish non-stochastic, worst-case guarantees for sequential decision problems such as prediction with expert advice (Littlestone and Warmuth, 1994), a natural question arose as to whether similar guarantees were possible for the bandit setting. The pioneering work of Auer, Cesa-Bianchi, Freund, and Schapire (2003) answered this in the affirmative by showing that their algorithm EXP3 possesses nearly-optimal regret bounds with matching lower bounds. Attention later turned to the bandit version of online linear optimization, and several associated guarantees were published the following decade (Abernethy et al., 2012; Dani and Hayes, 2006; Dani et al., 2008; Flaxman et al., 2005; McMahan and Blum, 2004).

Nearly all proposed methods have relied on a particular algorithmic blueprint; they reduce the bandit problem to the full-information setting, while using randomization to make decisions and to estimate the losses. A well-studied family of algorithms for the full-information setting is Follow the Regularized Leader (FTRL), which optimizes the objective function of the following form:

arg min L>x + R(x)
x2K

(1)

where K is the decision set, L is (an estimate of) the cumulative loss vector, and R is a regularizer, a convex function with suitable curvature to stabilize the objective. The choice of regularizer R is

1

critical to the algorithm's performance. For example, the EXP3 algorithm (Auer, 2003) regularizes with the entropy function and achieves a nearly optimal regret bound when K is the probability simplex. For a general convex set, however, other regularizers such as self-concordant barrier functions (Abernethy et al., 2012) have tighter regret bounds.

Another class of algorithms for the full information setting is Follow the Perturbed Leader (FTPL) (Kalai and Vempala, 2005) whose foundations date back to the earliest work in adversarial online learning (Hannan, 1957). Here we choose a distribution D on RN , sample a random vector Z  D, and solve the following linear optimization problem

arg min(L + Z)>x.
x2K

(2)

FTPL is computationally simpler than FTRL due to the linearity of the objective, but it is analytically much more complex due to the randomness. For every different choice of D, an entirely new set of techniques had to be developed (Devroye et al., 2013; Van Erven et al., 2014). Rakhlin et al. (2012) and Abernethy et al. (2014) made some progress towards unifying the analysis framework. Their techniques, however, are limited to the full-information setting.

In this paper, we propose a new analysis framework for the multi-armed bandit problem that unifies the regularization and perturbation algorithms. The key element is a new kind of smoothness property, which we call differential consistency. It allows us to generate a wide class of both optimal and near-optimal algorithms for the adversarial multi-armed bandit problem. We summarize our main results:

1. We show that regularization via the Tsallis entropy leads to the state-of-the-art adversarial MAB algorithm, matching the minimax regret rate of Audibert and Bubeck (2009) with a tighter constant. Interestingly, our algorithm fully generalizes EXP3.

2. We show that a wide array of well-studied noise distributions lead to near-optimal regret bounds

(matching those of sufficient condition

fEoXr Pac3h).ieFviunrgthOer(mpoTre),roeugrreatn: athlyeshisazraevrdearlastea

strikingly simple and appealing function of the noise distribution

must be bounded by a constant. We conjecture that this requirement is in fact both necessary and

sufficient.

2 Gradient-Based Prediction Algorithms for the Multi-Armed Bandit

Let us now introduce the adversarial multi-armed bandit problem. On each round t = 1, . . . , T ,

a learner must choose a distribution pt 2 N over the set of N available actions. The adversary

(Nature) chooses a vector gt 2 [ 1, 0]N of losses, the learner samples it  pt, and plays action it.

After to the

svealelucetisnggt,tjhifsoarcjtio6=n,itth. eTlheiasrnliemr iotebdseirnvfeosrmonaltyiotnhefeveadlbuaecgkt,iist

, and what

receives no information as makes the bandit problem

much more challenging than the full-information setting in which the entire gt is observed.

The learner's goal is to minimize the regret. Regret is defined to be the difference in the realized loss and the loss of the best fixed action in hindsight:

XT

RegretT

:=

max (gt,i
i2[N ] t=1

gt,it ).

(3)

To be precise, we consider the expected regret, where the expectation is taken with respect to the learner's randomization.

Loss vs. Gain Note: We use the term "loss" to refer to g, although the maximization in (3) would imply that g should be thought of as a "gain" instead. We use the former term, however, as we impose the assumption that gt 2 [ 1, 0]N throughout the paper.

2.1 The Gradient-Based Algorithmic Template Our results focus on a particular algorithmic template described in Framework 1, which is a slight variation of the Gradient Based Prediction Algorithm (GBPA) of Abernethy et al. (2014). Note that

2

the algorithm (i) maintains an unbiased estimate of the cumulative losses Gt, (ii) updates Gt by adding a single round estimate gt that has only one non-zero coordinate, and (iii) uses the gradient of a convex function  as sampling distribution pt. The choice of  is flexible but  must be a differentiable convex function and its derivatives must always be a probability distribution. Framework 1 may appear restrictive but it has served as the basis for much of the published work on adversarial MAB algorithms (Auer et al., 2003; Kujala and Elomaa, 2005; Neu and Bartok, 2013). First, the GBPA framework essentially encompasses all FTRL and FTPL algorithms (Abernethy et al., 2014), which are the core techniques not only for the full information settings, but also for the bandit settings. Second, the estimation scheme ensures that Gt remains an unbiased estimate of Gt. Although there is some flexibility, any unbiased estimation scheme would require some kind of inverse-probability scaling--information theory tells us that the unbiased estimates of a quantity that is observed with only probabilty p must necessarily involve fluctuations that scale as O(1/p).

Framework 1: Gradient-Based Prediction Alg. (GBPA) Template for Multi-Armed Bandit

GBPA(  ):  is a differentiable convex function such that r  2 N and ri  > 0 for all i.

Initialize G0 = 0 for t = 1 to T do

Nature: A loss vector gt 2 [ 1, 0]N is chosen by the Adversary

Sampling: Learner chooses it according to the distribution p(Gt 1) = r t(Gt 1)

Cost: Learner "gains" loss gt,it Estimation: Learner "guesses"

gt

:=

gt,it pit (Gt

1) eit

Update: Gt = Gt 1 + gt

Lemma 2.1. Define (G)  maxi Gi so that we can write the expected regret of GBPA(  ) as ERegretT = (GT ) PTt=1hr  (Gt 1), gti.

Then, the expected regret of the GBPA(  ) can be written as:

 XT

ERegretT 

 |

(0)

{z

(0})

+Ei1,...,it 1

overestimation penalty

| (GT ) {z 
underestimation

(GT })
penalty

+

t=1

E| it

[D (Gt,{Gzt 1)|Gt
divergence penalty

1}]

,

(4)

where the expectations are over the sampling of it.

Proof. Let  be a valid convex function for the GBPA. Consider GBPA(  ) being run on the loss sequence g1, . . . , gT . The algorithm produces a sequence of estimated losses g1, . . . , gT . Now consider GBPA-NE(  ), which is GBPA(  ) run with the full information on the deterministic loss

sequence g1, . . . , gT (there is no estimation step, and the learner updates Gt directly). The regret of

this run can be written as

(GT ) PTt=1hr  (Gt 1), gti,

and (GT )  (GT ) by the convexity of . Hence, it suffices to show that the GBPA-NE(  ) has regret at most the righthand side of Equation 4, which is a fairly well-known result in online learning literature; see, for example, (Cesa-Bianchi and Lugosi, 2006, Theorem 11.6) or (Abernethy et al., 2014, Section 2). For completeness, we included the full proof in Appendix A.

2.2 A New Kind of Smoothness What has emerged as a guiding principle throughout machine learning is that enforcing stability of an algorithm can often lead immediately to performance guarantees--that is, small modifications of the input data should not dramatically alter the output. In the context of GBPA, algorithmic stability is guaranteed as long as the dervative r  (*) is Lipschitz. Abernethy et al. (2014) explored a set of conditions on r2  (*) that lead to optimal regret guarantees for the full-information setting. Indeed,

3

this work discussed different settings where the regret depends on an upper bound on either the nuclear norm or the operator norm of this hessian.
In short, regret in the full information setting relies on the smoothness of the choice of  . In the bandit setting, however, merely a uniform bound on the magnitude of r2  is insufficient to guarantee low regret; the regret (Lemma 2.1) involves terms of the form D (Gt 1 + gt, Gt 1), where the incremental quantity gt can scale as large as the inverse of the smallest probability of p(Gt 1). What is needed is a stronger notion of the smoothness that bounds r2  in correspondence with r  , and we propose the following definition: Definition 2.2 (Differential Consistency). For constants , C > 0, we say that a convex function  (*) is ( , C)-differentially-consistent if for all G 2 ( 1, 0]N ,
r2ii  (G)  C(ri  (G)) .

We now prove a useful bound that emerges from differential consistency, and in the following two sections we shall show how this leads to regret guarantees. Theorem 2.3. Suppose  is ( , C)-differentially-consistent for constants C, > 0. Then divergence penalty at time t in Lemma 2.1 can be upper bounded as:

XN 



Eit [D (Gt, Gt 1)|Gt 1]  C

ri  (Gt 1)

i=1

1
.

Proof. For the sake of clarity, we drop the subscripts; we use G to denote the cumulative estimate Gt 1, g to denote the marginal estimate gt = Gt Gt 1, and g to denote the true loss gt.

Note that by definition of Algorithm 1, g is a sparse vector with one non-zero and non-positive coordinate git = gt,i/ri  (G). Plus, it is conditionally independent given G. For a fixed it, Let

h(r) := D (G + rg/kgk, G) = D (G + reit , G),







so that h00(r) = (g/kgk)>r2  G + tg/kgk (g/kgk) = e>it r2  G teit eit . Now we can write

Eit [D 

(G

+

g,

G)|G]

= =

PN
i=1
PN
i=1

P[it ri 

= i] (G)

R kgk

R

0 kgk

0

Rs

R

0 s

0

h00(r) e>i r2

dr ds 
 G

 rei ei dr ds



PN
i=1

ri

 (G)

R kgk
0

Rs
0

C

 ri

 (G

 rei) dr ds



PN
i=1

ri



(G)

R kgk
0

Rs
0

C

 ri



 (G)

dr ds

=

C

PN
i=1

 ri



(G)1+

R kgk R s
00

dr ds

=

C 2

PN
i=1

 ri  (G)

1

gi2



C

PN
i=1

 ri

  (G)

1
.

The first inequality is by the supposition and the second inequality is due to the convexity of  which guarantees that ri is an increasing function in the i-th coordinate. Interestingly, this part of the proof critically depends on the fact that the we are in the "loss" setting where g is always non-positive.

3 A Minimax Bandit Algorithm via Tsallis Smoothing

The design of a multi-armed bandit algorithm in the adversarial setting proved to be a challenging

task. Ignoring the dependence on N for the moment, we note that the initial published work on

EXP3 provided only an O(T 2/3) of this work (Auer et al., 2003)

guarantee (Auer et al., 1995), that the authors obtained the

oapntdimitawl aOs(npoTt u)nrtailteth. eFfionratlhveemrsioorne

4

general setting of online Hayes, 2006; Flaxman et

alli.n, e2a0r05o;ptMimciMzaathioann,asnedveBrlaulms,u2b0-o0p4t)imbeaflorraettehsewdeesreireadchpieTvewda(sDoabntai iannedd

(Abernethy et al., 2012; Dani et al., 2008).

We can view EXP3 as an instance of GBPA where the potential function  (*) is the Fenchel con-

jHfuacg(tap,t)ew:oe=fhtahPveeiSaphiacllnoongsoepndi-,efoannrtrmdopietyxs.pFrFeeosnsrcihoaennlyfcoporn2tjhuegastuNep,irsethmHeu?(mn(G:eg)Hati=?v(eG)s)uSph=pa2nn1oNlno{ghep(nP,trGoiipeyxpis(HdGe(fiip)n))e}.d.BaInys inspecting the gradient of the above expression, it is easy to see that EXP3 chooses the distribution

pt = rH?(G) every round.

The tighter EXP3 bound given by authors provided a matching lower

Auer et al. (2003) bound of the form

sca(lpedTaNcc)o. rIdtirnegmtaoinOe(dpaTn

N log N ) and open question

the for

some time whether there exists a minimax optimal algorithm that does not contain the log term un-

til Audibert and Bubeck (2009) proposed the Implicitly Normalized Forecaster (INF). The INF is

implicitly defined via a specially-designed potential function with certain properties. It was not im-

mediately clear from this result how to define a minimax-optimal algorithm using the now-standard

tools of regularization and Bregman divergence.

More recently, Audibert et al. (2011) improved upon Audibert and Bubeck (2009), extending the

results to the combinatorial setting, and they also discovered that INF can be interpreted in terms

of Bregman divergences. We give here a reformulation of INF that leads to a very simple analysis

in terms of our notion of differential consistency. Our reformulation can be viewed as a variation

of EXP3, where the key modification is to replace the Shannon entropy function with the Tsallis

entropy1 for parameter 0 <  < 1:

S(p)

=

1

1



 1

X pi .

This particular function, proposed by Tsallis (1988), possesses a number of natural properties. The

Tsallis entropy is in fact a generalization of the Shannon entropy, as one obtains the latter as a special

case of the former asymptotically. That is, it is easy to prove the following uniform convergence:

S(*) ! H(*) as  ! 1. We emphasize again that one can easily show that Tsallis-smoothing bandit algorithm is indeed identical to INF using the appropriate parameter mapping, although our analysis is simpler due to the notion of differential consistency (Definition 2.2).

Theorem 3.1. Let  (G) = maxp2 N {hp, Gi S(p)}. Then the GBPA(  ) has regret at most

ERegret





N

1
1




1

+

NT 

.

(5)

Before proving the theorem, we note that it immediately recovers the EXP3 upper bound as a special

case



!

1.

An

easy

applicationpof

L'Ho pital's

rule

shows

that

as



!

1,

N1 1




1

!

log N

and

Np/ 2 TN

! log

N. N.

Choosing  However the

= (N log N )/T , we see that the right-hand side of choice  ! 1 is clearly not the optimal choice, as we

(5) tends to show in the

following statement, which directly follows from the tqheorem once we see that N 1  1 < N 1 .

Corollary 3.2. For any  2 (0, 1), if we choose  =

N 1 2 (1 )T

then we have

q

ERegret  2

N (1

T )

.

In

particular,

the

choice

of



=

1 2

gives

a

regret

of

no

more

than

p 4N

T

.

Proof of Theorem 3.1. We will bound each penalty term in Lemma 2.1. Since S is non-positive, the underestimation penalty is upper bounded by 0 and the overestimation penalty is at most

(

min

S).

The minimum of S occurs (overestimation penalty) 

at

(1/N,
 1

.

.

., 1

1/N ). Hence,

XN

1 N

!



(N 1



i=1

1).

(6)

1More precisely, the function we give here is the negative Tsallis entropy according to its original definition.

5

Now it remains to upper bound the divergence penalty with () 1N T . We observe that straight-

forward calculus gives r2S(p) = diag(p1 2, . . . , pN 2). Let I N (*) be the indicator function

of N ; the dual S(*) +

toIhfatNthi(es*,)fuIantcNpti((oGxn))S,=fo(l*0l)of+woirInxgNt2h(*e),saeNntudapnmodofIrPeeoNnvo(extr )(w1=e99o41b)s.efrTovarekxitnhga2/tardv2NaSn.ta(Ipgt )eisioscfalPesaruorbpt-hohaseitstisoia(n*n)3o.i2sf

in the latter reference, we conclude that r 2S(p(G)) is a super-hessian of  = S at G. Hence,

r2  (G) () 1diag(p21 (G), . . . , p2N (G))

for any G. What we have stated, indeed, is that  is (2 , () 1)-differentially-consistent, and thus applying Theorem 2.3 gives

D (Gt, Gt

1)  ()

1

XN

 pi(Gt

1 1)


.

i=1

Ntooatninygptrhoabtatbhielit1y-dniosrtmribauntidotnhpe11, .1..-,npoNrmtoaroebdtauianl to each other, we can apply Holder's inequality

XN XN

p1i  =

p1i  * 1 

XN

!1 
1

pi1 

XN

1

1 

!

=

(1)1

N = N.

i=1 i=1 i=1 i=1

So, the divergence penalty is at most () 1N , which completes the proof.

4 Near-Optimal Bandit Algorithms via Stochastic Smoothing

Let D be a continuous distribution over an unbounded support with probability density function f and cumulative density function F . Consider the GBPA(  (G; D)) where

 (G; D) = EZ1,...,ZN iidD miax{Gi + Zi}

which is a stochastic smoothing of (maxi Gi) function. Since the max function is convex,  is also convex. By Bertsekas (1973), we can swap the order of differentiation and expectation:

 (G; D)

=

EZ1,...,ZN iidD ei ,

where i

=

arg max{Gi
i=1,...,N

+ Zi}.

(7)

Even if the function is not differentiable everywhere, the swapping is still possible with any subgradient as long as they are bounded. Hence, the ties between coordinates (which happen with probability zero anyways) can be resolved in an arbitrary manner. It is clear that r  is in the probability simplex, and note that

@ @Gi

=

EZ1,...,ZN 1{Gi + Zi

>

Gj

+ Zj, 8j

6=

i}

= EGj [PZi [Zi > Gj Gi]] = EGj [1 F (Gj

Gi)]

(8)

where Gj = maxj6=i Gj + Zj. The unbounded support condition guarantees that this partial derivative is non-zero for all i given any G. So,  (G; D) satisfies the requirements of Algorithm 1.

4.1 Connection to Follow the Perturbed Leader
There is a straightforward way to efficiently implement the sampling step of the bandit GBPA (Algorithm 1) with a stochastically smoothed function. Instead of evaluating the expectation of Equation 7, we simply take a random sample. In fact, this is equivalent to Follow the Perturbed Leader Algorithm (FTPL) (Kalai and Vempala, 2005) for bandit settings. On the other hand, implementing the estimation step is hard because generally there is no closed-form expression for r  . To address this issue, Neu and Bartok (2013) proposed Geometric Resampling (GR). GR uses an iterative resampling process to estimate ri  . This process gives an unbiased estimate when allowed

6

to run for an unbounded number of iterations. Even when we truncate the resampling process after

Mlowietrerbaotiuonnds,fothretheextmraulrteig-arremt deduebatondthitepersotbimleamtioisnOb(iapsNisTa)t,manoystcNehMoTic(eaodfdiMtive=teOrm(p). NSTin)cedothees not affect the asymptotic regret of the algorithm. In summary, all our GBPA regret bounds in this

section hold for the corresponding FTPL algorithm with an

extra additive

NT eM

term in the bound.

Despite the fact that perturbation-based algorithms provide a natural randomized decision strategy,

they have seen little applications mostly because they are hard to analyze. But one should expect

general results to be within reach: the EXP3 algorithm, for example, can be viewed through the

lens of perturbations, where the noise is distributed according to the Gumbel distribution. Indeed,

an early result of Kujala and Elomaa (2005) showed that a near-optimal MAB strategy comes about

through the use of exponentially-distributed noise, and the same perturbation strategy has more

recently been utilized in the work of Neu and Bartok (2013) and Kocak et al. (2014). However,

a more general understanding of perturbation methods has remained elusive. For example, would

Gaussian noise be sufficient for a guarantee? What about, say, the Weibull distribution?

4.2 Hazard Rate analysis

In this section, we show that the performance of the GBPA(  (G; D)) can be characterized by the hazard function of the smoothing distribution D. The hazard rate is a standard tool in survival analysis to describe failures due to aging; for example, an increasing hazard rate models units that deteriorate with age while a decreasing hazard rate models units that improve with age (a counter intuitive but not illogical possibility). To the best of our knowledge, the connection between hazard rates and design of adversarial bandit algorithms has not been made before. Definition 4.1 (Hazard rate function). Hazard rate function of a distribution D is

hD (x)

:=

1

f (x) F (x)

For the rest of the section, we assume that D is unbounded in the direction of +1, so that the hazard

function is well-defined everywhere. This assumption is for the clarity of presentation and can be

easily removed (Appendix B).

Theorem 4.2. The regret of the GBPA on  (L) = EZ1,...,ZnD maxi{Gi + Zi} is at most:

N

(sup 

hD

)

T

+

 EZ1 ,...,Zn D

hi

max
i

Zi

Proof. We analyze each penalty term in Lemma 2.1. Due to the convexity of , the underestimation

penalty Lemma

is non-positive. The overestimation penalty is 4.3 proves the N (sup hD) upper bound on the

dcilveearrglyenactempeonstalEtyZ. 1

,...,Zn

D

[maxi

Zi],

and

It remains to provide the tuning parameter . Suppose we scale the perturbation Z by  > 0, i.e., we

add Zi to each coordinate. It is easy to see that E[maxi=1,...,n Xi] = E[maxi=1,...,n Xi]. For the

divergence penalty, let F be the CDF of the scaled random variable. Observe that F(t) = F (t/)

and

thus

f (t)

=

1 

f

(t/).

Hence,

the

hazard

rate

scales

by

1/,

which

completes

the

proof.

Lemma 4.3. The divergence penalty of the GBPA with  (G) = EZD maxi{Gi + Zi} is at most N (sup hD) each round.

Proof. Recall the gradient expression in Equation 8. The i-th diagonal entry of the Hessian is:



r2ii  (G)

=

@ @Gi

EG j 

[1

F (Gj

Gi)] = EGj

@ @Gi

(1

F (Gj

Gi)) = EGj f (Gj

= EGj [h(Gj Gi)(1 F (Gj Gi))]

 (sup h)EGj [1 F (Gj Gi)]

= (sup h)ri(G)

Gi) (9)

where Gj = maxj6=i{Gj + Zj} which is a random variable independent of Zi. We now apply Theorem 2.3 with = 1 and C = (sup h) to complete the proof.

7

Distribution
Gumbel( = 1, = 1) Frechet ( > 1)
Weibull*( = 1, k  1) Pareto*(xm = 1, ) Gamma( 1, )

supx hD(x) 1 as x ! 0 at most 2
k at x = 0  at x = 0
as x ! 1

E[maxNi=1 Zi]

log N + 0 N 1/ (1 1/)

O(

1 k

!(log

N

)

1 k

)

N 1//( 1)

log N +( 1) log log N

log () +

1 0

p O( T

N

log

N

)

Param.

N/A

 = log N

k = 1 (Exponential)

 = log N =  = 1 (Exponential)

Table

1:

Distributions

that

give

p O( T N

log N )

regret

FTPL

algorithm.

The

parameterization

fol-

lows Wikipedia pages for easy lookup. We denote the Euler constant ( 0.58) by 0. Distributions marked with (*) need to be slightly modified using the conditioning trick explained in Appendix B.2.

The maximum of Frechet hazard function has to be computed numerically (Elsayed, 2012, p. 47)

but elementary calculations show that it is bounded by 2 (Appendix D).

Corollary 4.4. Follow the Perturbed Leader Algorithm

certain range of has an expected

preagrraemt eotfeorrsd),ecroOm(bpinTedNwliotgh

Geometric N ).

wReitshamdipstlrinibgu(tSioencstioinnT4a.1b)lew1it(hreMstr=ictpedNtoTa,

Table 1 provides the two terms we need to bound. We derive the third column of the table in Appendix C using Extreme Value Theory (Embrechts et al., 1997). Note that our analysis in the proof of Lemma 4.3 is quite tight; the only place we have an inequality is when we upper bound the hazard rate. It is thus reasonable to pose the following conjecture: Conjecture 4.5. If a distribution D has a monotonically increasing hazard rate hD(x) that does not converge as x ! +1 (e.g., Gaussian), then there is a sequence of losses that will incur at least a linear regret.

The intuition is that if adversary keeps incurring a high loss for the i-th arm, then with high probability Gj Gi will be large. So, the expectation in Equation 9 will be dominated by the hazard function evaluated at large values of Gj Gi.

Acknowledgments. J. Abernethy acknowledges the support of NSF under CAREER grant IIS1453304. A. Tewari acknowledges the support of NSF under CAREER grant IIS-1452099.

References
J. Abernethy, E. Hazan, and A. Rakhlin. Interior-point methods for full-information and bandit online learning. IEEE Transactions on Information Theory, 58(7):4164-4175, 2012.
J. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. In COLT, pages 807-823, 2014.
J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In COLT, pages 217-226, 2009.
J.-Y. Audibert, S. Bubeck, and G. Lugosi. Minimax policies for combinatorial prediction games. In COLT, 2011.
P. Auer. Using confidence bounds for exploitation-exploration trade-offs. The Journal of Machine Learning Research, 3:397-422, 2003.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. Gambling in a rigged casino: The adversarial multi-arm bandit problem. In FOCS, 1995.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235-256, 2002.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal of Computuataion, 32(1):48-77, 2003. ISSN 0097-5397.
D. P. Bertsekas. Stochastic optimization problems with nondifferentiable cost functionals. Journal of Optimization Theory and Applications, 12(2):218-231, 1973. ISSN 0022-3239.

8

N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.
V. Dani and T. P. Hayes. Robbing the bandit: less regret in online geometric optimization against an adaptive adversary. In SODA, pages 937-943, 2006.
V. Dani, T. Hayes, and S. Kakade. The price of bandit information for online optimization. In NIPS, 2008.
L. Devroye, G. Lugosi, and G. Neu. Prediction by random-walk perturbation. In Conference on Learning Theory, pages 460-473, 2013.
E. Elsayed. Reliability Engineering. Wiley Series in Systems Engineering and Management. Wiley, 2012. ISBN 9781118309544. URL https://books.google.com/books?id= NdjF5G6tfLQC.
P. Embrechts, C. Kluppelberg, and T. Mikosch. Modelling Extremal Events: For Insurance and Finance. Applications of mathematics. Springer, 1997. ISBN 9783540609315. URL https: //books.google.com/books?id=BXOI2pICfJUC.
A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In SODA, pages 385-394, 2005. ISBN 0-89871-585-7.
J. Gittins. Quantitative methods in the planning of pharmaceutical research. Drug Information Journal, 30(2):479-487, 1996.
J. Gittins, K. Glazebrook, and R. Weber. Multi-armed bandit allocation indices. John Wiley & Sons, 2011.
J. Hannan. Approximation to bayes risk in repeated play. In M. Dresher, A. W. Tucker, and P. Wolfe, editors, Contributions to the Theory of Games, volume III, pages 97-139, 1957.
A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 71(3):291-307, 2005.
T. Kocak, G. Neu, M. Valko, and R. Munos. Efficient learning by implicit exploration in bandit problems with side observations. In NIPS, pages 613-621. Curran Associates, Inc., 2014.
J. Kujala and T. Elomaa. On following the perturbed leader in the bandit setting. In Algorithmic Learning Theory, pages 371-385. Springer, 2005.
T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4-22, 1985.
N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation, 108(2):212-261, 1994. ISSN 0890-5401.
H. B. McMahan and A. Blum. Online geometric optimization in the bandit setting against an adaptive adversary. In COLT, pages 109-123, 2004.
G. Neu and G. Bartok. An efficient algorithm for learning with semi-bandit feedback. In Algorithmic Learning Theory, pages 234-248. Springer, 2013.
M. Pacula, J. Ansel, S. Amarasinghe, and U.-M. OReilly. Hyperparameter tuning in bandit-based adaptive operator selection. In Applications of Evolutionary Computation, pages 73-82. Springer, 2012.
J.-P. Penot. Sub-hessians, super-hessians and conjugation. Nonlinear Analysis: Theory, Methods & Applications, 23(6):689-702, 1994. URL http://www.sciencedirect.com/ science/article/pii/0362546X94902127.
S. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize: From value to algorithms. In Advances in Neural Information Processing Systems, pages 2141-2149, 2012.
H. Robbins. Some aspects of the sequential design of experiments. Bull. Amer. Math. Soc., 58(5): 527-535, 1952.
C. Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of Statistical Physics, 52 (1-2):479-487, 1988.
G. Van den Broeck, K. Driessens, and J. Ramon. Monte-carlo tree search in poker using expected reward distributions. In Advances in Machine Learning, pages 367-381. Springer, 2009.
T. Van Erven, W. Kotlowski, and M. K. Warmuth. Follow the leader with dropout perturbations. In COLT, 2014.
9

