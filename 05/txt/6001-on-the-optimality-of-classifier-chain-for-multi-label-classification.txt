On the Optimality of Classifier Chain for Multi-label Classification

Weiwei Liu

Ivor W. Tsang

Centre for Quantum Computation and Intelligent Systems

University of Technology, Sydney

liuweiwei863@gmail.com, ivor.tsang@uts.edu.au

Abstract
To capture the interdependencies between labels in multi-label classification problems, classifier chain (CC) tries to take the multiple labels of each instance into account under a deterministic high-order Markov Chain model. Since its performance is sensitive to the choice of label order, the key issue is how to determine the optimal label order for CC. In this work, we first generalize the CC model over a random label order. Then, we present a theoretical analysis of the generalization error for the proposed generalized model. Based on our results, we propose a dynamic programming based classifier chain (CC-DP) algorithm to search the globally optimal label order for CC and a greedy classifier chain (CC-Greedy) algorithm to find a locally optimal CC. Comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed CC-DP algorithm outperforms state-of-the-art approaches and the CCGreedy algorithm achieves comparable prediction performance with CC-DP.
1 Introduction
Multi-label classification, where each instance can belong to multiple labels simultaneously, has significantly attracted the attention of researchers as a result of its various applications, ranging from document classification and gene function prediction, to automatic image annotation. For example, a document can be associated with a range of topics, such as Sports, Finance and Education [1]; a gene belongs to the functions of protein synthesis, metabolism and transcription [2]; an image may have both beach and tree tags [3].
One popular strategy for multi-label classification is to reduce the original problem into many binary classification problems. Many works have followed this strategy. For example, binary relevance (BR) [4] is a simple approach for multi-label learning which independently trains a binary classifier for each label. Recently, Dembczynski et al. [5] have shown that methods of multi-label learning which explicitly capture label dependency will usually achieve better prediction performance. Therefore, modeling the label dependency is one of the major challenges in multi-label classification problems. Many multi-label learning models [5, 6, 7, 8, 9, 10, 11, 12] have been developed to capture label dependency. Amongst them, the classifier chain (CC) model is one of the most popular methods due to its simplicity and promising experimental results [6].
CC works as follows: One classifier is trained for each label. For the (i + 1)th label, each instance is augmented with the 1st, 2nd, * * * , ith label as the input to train the (i + 1)th classifier. Given a new instance to be classified, CC firstly predicts the value of the first label, then takes this instance together with the predicted value as the input to predict the value of the next label. CC proceeds in this way until the last label is predicted. However, here is the question: Does the label order affect the performance of CC? Apparently yes, because different classifier chains involve different
Corresponding author
1

classifiers trained on different training sets. Thus, to reduce the influence of the label order, Read et al. [6] proposed the ensembled classifier chain (ECC) to average the multi-label predictions of CC over a set of random chain ordering. Since the performance of CC is sensitive to the choice of label order, there is another important question: Is there any globally optimal classifier chain which can achieve the optimal prediction performance for CC? If yes, how can the globally optimal classifier chain be found?
To answer the last two questions, we first generalize the CC model over a random label order. We then present a theoretical analysis of the generalization error for the proposed generalized model. Our results show that the upper bound of the generalization error depends on the sum of reciprocal of square of the margin over the labels. Thus, we can answer the second question: the globally optimal CC exists only when the minimization of the upper bound is achieved over this CC. To find the globally optimal CC, we can search over q! different label orders1, where q denotes the number of labels, which is computationally infeasible for a large q. In this paper, we propose the dynamic programming based classifier chain (CC-DP) algorithm to simplify the search algorithm, which requires O(q3nd) time complexity. Furthermore, to speed up the training process, a greedy classifier chain (CC-Greedy) algorithm is proposed to find a locally optimal CC, where the time complexity of the CC-Greedy algorithm is O(q2nd).
Notations: Assume xt  Rd is a real vector representing an input or instance (feature) for t  {1, * * * , n}. n denotes the number of training samples. Yt  {1, 2, * * * , q} is the corresponding output (label). yt  {0, 1}q is used to represent the label set Yt, where yt(j) = 1 if and only if j  Yt.

2 Related work and preliminaries
To capture label dependency, Hsu et al. [13] first use compressed sensing technique to handle the multi-label classification problem. They project the original label space into a low dimensional label space. A regression model is trained on each transformed label. Recovering multi-labels from the regression output usually involves solving a quadratic programming problem [13], and many works have been developed in this way [7, 14, 15]. Such methods mainly aim to use different projection methods to transform the original label space into another effective label space.
Another important approach attempts to exploit the different orders (first-order, second-order and high-order) of label correlations [16]. Following this way, some works also try to provide a probabilistic interpretation for label correlations. For example, Guo and Gu [8] model the label correlations using a conditional dependency network; PCC [5] exploits a high-order Markov Chain model to capture the correlations between the labels and provide an accurate probabilistic interpretation of CC. Other works [6, 9, 10] focus on modeling the label correlations in a deterministic way, and CC is one of the most popular methods among them. This work will mainly focus on the deterministic high-order classifier chain.

2.1 Classifier chain
Similar to BR, the classifier chain (CC) model [6] trains q binary classifiers hj (j  {1, * * * , q}). Classifiers are linked along a chain where each classifier hj deals with the binary classification problem for label j. The augmented vector {xt, yt(1), * * * , yt(j)}nt=1 is used as the input for training classifier hj+1. Given a new testing instance x, classifier h1 in the chain is responsible for predicting the value of y(1) using input x. Then, h2 predicts the value of y(2) taking x plus the predicted value of y(1) as an input. Following in this way, hj+1 predicts y(j + 1) using the predicted value of y(1), * * * , y(j) as additional input information. CC passes label information between classifiers, allowing CC to exploit the label dependence and thus overcome the label independence problem of BR. Essentially, it builds a deterministic high-order Markov Chain model to capture the label correlations.

1! represents the factorial notation.

2

2.2 Ensembled classifier chain
Different classifier chains involve different classifiers learned on different training sets and thus the order of the chain itself clearly affects the prediction performance. To solve the issue of selecting a chain order for CC, Read et al. [6] proposed the extension of CC, called ensembled classifier chain (ECC), to average the multi-label predictions of CC over a set of random chain ordering. ECC first randomly reorders the labels {1, 2, * * * , q} many times. Then, CC is applied to the reordered labels for each time and the performance of CC is averaged over those times to obtain the final prediction performance.

3 Proposed model and generalization error analysis

3.1 Generalized classifier chain
We generalize the CC model over a random label order, called generalized classifier chain (GCC) model. Assume the labels {1, 2, * * * , q} are randomly reordered as {1, 2, * * * , q}, where j = k means label k moves to position j from k. In the GCC model, classifiers are also linked along a chain where each classifier hj deals with the binary classification problem for label j (k). GCC follows the same training and testing procedures as CC, while the only difference is the label order. In the GCC model, for input xt, yt(j) = 1 if and only if j  Yt.

3.2 Generalization error analysis

In this section, we analyze the generalization error bound of the multi-label classification problem using GCC based on the techniques developed for the generalization performance of classifiers with a large margin [17] and perceptron decision tree [18].

Let X represent the input space. Both s and s are m samples drawn independently according to

an unknown distribution D. We denote logarithms to base 2 by log. If S is a set, |S| denotes its

cardinality.  *  means the l2 norm. We train a support vector machine(SVM) for each label j.

Let {xt}nt=1 as the feature and {yt(j)}nt=1 as the label, the output parameter of SVM is defined as

[wj, bj] = SV M ({xt, yt(1), * * * , yt(j-1)}nt=1, {yt(j)}nt=1). The margin for label j is defined

as:

j

=

1 ||wj ||2

(1)

We begin with the definition of the fat shattering dimension.

Definition 1 ([19]). Let H be a set of real valued functions. We say that a set of points P is -

shattered by H relative to r = (rp)pP if there are real numbers rp indexed by p  P such that for

all binary vectors b indexed by P , there is a function fb  H satisfying

{

fb(p) =

 rp +   rp - 

if bp = 1 otherwise

The fat shattering dimension f at() of the set H is a function from the positive real numbers to the integers which maps a value  to the size of the largest -shattered set, if this is finite, or infinity otherwise.

Assume H is the real valued function class and h  H. l(y, h(x)) denotes the loss function. The

expected error of h is defined as erD[h] = E(x,y)D[l(y, h(x))], where (x, y) drawn from the

unknown distribution D. Here we select 0-1 loss function. So, erD[h] = P(x,y)D(h(x) = y).

ers[h]

is

defined

as

ers[h]

=

1 n

n [yt

=

h(xt)].2

t=1

Suppose N (, H, s) is the -covering number of H with respect to the l pseudo-metric measuring the maximum discrepancy on the sample s. The notion of the covering number can be referred to
the Supplementary Materials. We introduce the following general corollary regarding the bound of
the covering number:

2The expression [yt = h(xt)] evaluates to 1 if yt = h(xt) is true and to 0 otherwise.

3

Corollary 1 ([17]). Let H be a class of functions X  [a, b] and D a distribution over X. Choose

0 <  < 1 and let d = f at(/4)  em. Then

E(N

(,

H,

s))



( 4m(b - 2 2

a)2

)d log(2em(b-a)/(d))

(2)

where the expectation E is over samples s  Xm drawn according to Dm.

We study the generalization error bound of the specified GCC with the specified number of labels and margins. Let G be the set of classifiers of GCC, G = {h1, h2, * * * , hq}. ers[G] denotes the fraction of the number of errors that GCC makes on s. Define x  X x {0, 1}, hj(x) = hj(x)(1 - y(j)) - hj(x)y(j). If an instance x  X is correctly classified by hj, then hj(x) < 0. Moreover, we introduce the following proposition:

Proposition 1. If an instance x  X is misclassified by a GCC model, then hj  G, hj(x)  0.

Lemma 1. Given a specified GCC model with q labels and with margins 1, 2, * * * , q for each

label satisfying ki = f at(i/8), where f at is continuous from the right. If GCC has correctly

classified m multi-labeled examples s generated independently according to the unknown (but fixed)

distribution D and s is a set of another m multi-labeled examples, then we can bound the following

probability to sified > (m,

be less than : P 2m{ss :  q, )} < , where (m, q, 

a GCC model, it correctly

)

=

1 m

(Q

log(32m)

+

log

2cqla)sasnifideQs s,=fractqii=o1n

of ki

s misclas-

log(

8em ki

).

Proof. (of Lemma 1). Suppose G is a GCC model with q labels and with margins 1, 2, * * * , q, the probability event in Lemma 1 can be described as

A = {ss : G, ki = f at(i/8), ers[G] = 0, ers[G] > }.

Let s and s denote two different set of m examples, which are drawn i.i.d. from the distribution

D x {0, 1}. Applying the definition of x, h and Proposition 1, the event can also be written as

A = {ss : G, i = i/2, ki = f at(i/4), ers[G] = 0, ri = maxthi(xt), 2i = -ri, |{y  s :

hi  G, hi(y)  2i + ri}| > m}. Here, -maxthi(xt) means the minimal value of |hi(x)|

which represents the margin for label i, so 2i = -ri. Let ki = min{ : f at(/4)  ki}, so

ki



i,

we

define

the

following

function: 

0 if h  0

(h) = -h 2ki

if h  -2ki otherwise

so (h)  [-2ki , 0]. Let (G) = {(h) : h  G}.
Let Bsksi represent the minimal ki -cover set of (G) in the pseudo-metric dss. We have that for any hi  G, there exists f  Bsksi , |(hi(z)) - (f(z))| < ki , for all z  ss. For all x  s, by the definition of ri, hi(x)  ri = -2i, and ki  i, hi(x)  -2ki , (hi(x)) = -2ki , so (f(x)) < -2ki + ki = -ki . However, there are at least m points y  s such that hi(y)  0, so (f(y)) > -ki > maxt(f(xt)). Since  only reduces separation between output values, we conclude that the inequality f(y) > maxtf(xt) holds. Moreover, the m points in s with the largest f values must remain for the inequality to hold. By the permutation argument, at most 2-m of the sequences obtained by swapping corresponding points satisfy the conditions for fixed f.

As for any hi  G, there exists f  Bsksi , so there are |Bsksi | possibilities of f that satisfy the inequality for ki. Note that |Bsksi | is a positive integer which is usually bigger than 1 and by the union bound, we get the following inequality:

P (A)  (E(|Bsks1 |) + * * * + E(|Bsksq |))2-m  (E(|Bsks1 |) x * * * x E(|Bsksq |))2-m

Since every set of points -shattered by (G) can be -shattered by G, so f at(G)()  f atG(), where G = {h : h  G}. Hence, by Corollary 1 (setting [a, b] to [-2ki , 0],  to ki and m to 2m),

E(|Bsksi |)

=

E(N

(ki ,

(G),

ss))



2(32m)d

log(

8em d

)

4

where d

=

f at(G)(ki /4)



f atG (ki /4)



ki.

Thus E(|Bsksi |)



2(32m)ki

log(

8em ki

),

and we

obtain

P (A)



(E(|Bsks1 |)

x

**

*

x

E(|Bsksq |))2-m



q

2(32m)ki

log(

8em ki

)

=

2q (32m)Q

i=1

where

Q

=

q
i=1

ki

log(

8em ki

).

And

so

(E(|Bsks1 |)

x

*

*

*

x

E(|Bsksq |))2-m

<



provided

(m, q, ) 

1 ( 2q ) Q log(32m) + log

m

as required.

Lemma 1 applies to a particular GCC model with a specified number of labels and a specified margin for each label. In practice, we will observe the margins after running the GCC model. Thus, we must bound the probabilities uniformly over all of the possible margins that can arise to obtain a practical bound. The generalization error bound of the multi-label classification problem using GCC is shown as follows:

Theorem 1. Suppose a random m multi-labeled sample can be correctly classified using a GCC model, and suppose this GCC model contains q classifiers with margins 1, 2, * * * , q for each

label. Then we can bound the generalization error with probability greater than 1 -  to be less than

130R2

( Q

log(8em)

log(32m)

+

log

2(2m)q

)

m

where

Q

=

q
i=1

1 ( i )2

and

R

is

the

radius

of

a

ball

containing

the

support

of

the

distribution.

Before proving Theorem 1, we state one key Symmetrization lemma and Theorem 2.
Lemma 2 (Symmetrization). Let H be the real valued function class. s and s are m samples both drawn independently according to the unknown distribution D. If m2  2, then

Ps(sup |erD[h] - ers[h]|  )  2Pss(sup |ers[h] - ers[h]|  /2)

hH

hH

(3)

The proof details of this lemma can be found in the Supplementary Material.

Theorem 2 ([20]). Let H be restricted to points in a ball of M dimensions of radius R about the

origin, then

f atH()



min

{

R2 2

,

M

+

} 1

(4)

Proof. (of Theorem 1). We must bound the probabilities over different margins. We first use Lemma 2 to bound the probability of error in terms of the probability of the discrepancy between the performance on two halves of a double sample. Then we combine this result with Lemma 1. We must consider all possible patterns of ki's for label i. The largest value of ki is m. Thus, for fixed q, we can bound the number of possibilities by mq. Hence, there are mq of applications of Lemma 1.

Let ci = {1, 2, * * * , q} denote the i-th combination of margins varied in {1, * * * , m}q. G denotes
a set of GCC models. The generalization error of G can be represented as erD[G] and ers[G] is 0, where G  G. The uniform convergence bound of the generalization error is

Ps(sup |erD[G] - ers[G]|  )
GG

Applying Lemma 2,

Ps(sup |erD[G]-ers[G]|  )  2Pss(sup |ers[G] - ers[G]|  /2)

GG

GG

Let Jci = {ss :  a GCC model G with q labels and with margins ci : ki = f at(i/8), ers[G] = 0, ers[G]  /2}. Clearly,

Pss ( sup

|ers[G]

-

ers[G]|



/2)



P

(
mq

mq

) Jci

GG

i=1

5

As ki still satisfies ki = f at(i/8), Lemma 1 can still be applied to each case of P mq (Jci ). Let k = /mq. Applying Lemma 1 (replacing  by k/2), we get:

P bwohuenred,i(tmsu,fkfi,ceks/t2o)show2/tmha(tQPlmogq ((32mim=q1)

mq (Jci ) < k/2 J+cil)og2xk2miq=)q1aPndmqQ(J=ci

q
i=1
) < k

ki /2

log(

4em ki

).

x mq = 

By /2.

the union Applying

Lemma 2,

Ps(sup |erD[G] - ers[G]|  )  2Pss(sup |ers[G] - ers[G]|  /2)

GG

GG



2P

mq

(

mq

) Jci

<



i=1

Thus, Ps(supGG |erD[G] - ers[G]|  )  1 - . Let R be the radius of a ball containing the support of the distribution. Applying Theorem 2, we get ki = f at(i/8)  65R2/(i)2. Note that we have replaced the constant 82 = 64 by 65 in order to ensure the continuity from the right

required for the application of Lemma 1. We have upperbounded log(8em/ki) by log(8em). Thus,

( 2(2m)q )

erD[G]  2/m Q log(32m) + log 



130R2

( Q

log(8em)

log(32m)

+ log

2(2m)q )

where

Q

=

q
i=1

1 ( i )2

.

m



Given the training data size and the number of labels, Theorem 1 reveals one important factor in reducing the generalization error bound for the GCC model: the minimization of the sum of reciprocal of square of the margin over the labels. Thus, we obtain the following Corollary:
Corollary 2 (Globally Optimal Classifier Chain). Suppose a random m multi-labeled sample with q labels can be correctly classified using a GCC model, this GCC model is the globally optimal classifier chain if and only if the minimization of Q in Theorem 1 is achieved over this classifier chain.
Given the number of labels q, there are q! different label orders. It is very expensive to find the globally optimal CC, which can minimize Q, by searching over all of the label orders. Next, we discuss two simple algorithms.

4 Optimal classifier chain algorithm

In this section, we propose two simple algorithms for finding the optimal CC based on our result
in Section 3. To clearly state the algorithms, we redefine the margins with label order information. Given label set M = {1, 2, * * * , q}, suppose a GCC model contains q classifiers. Let oi(1  oi  q) denote the order of i in the GCC model, ioi represents the margin for label i, with pwrietvhioouutsaouig-me1nltaebdeilnspaust.thTehaeungQmeinstereddienfipnuet.dIafsoQi = =1,thqie=n1(i1ior1ie)p2r.esents the margin for label i,

4.1 Dynamic programming algorithm

To simplify the search algorithm globally optimal CC. Note that Q

m=entioqi=n1ed(bio1ei )f2or=e,

we propose

1 (qoq )2

+***

the[ +

CC-DP
1 (ko+k+11 )2

a+lgorikjth=m1

(tojo1jfi)2n]d,

the we

explore the idea of DP to iteratively optimize Q over a subset of M with the length of 1, 2, * * * , q.

Finally, we can obtain the optimal Q over M. Assume i  {1, * * * , q}. Let V (i, ) be the optimal

Q over a subset of M with the length of (1    q), where the label order is ending by label i.

QSupopvoesreMM,iwrheeprreestehnet

the corresponding label label order is ending by

set for V labe{l i.

(i, ). When  = q, V (i, q) be The DP equation}is written as:

the

optimal

1

V (i,  + 1) = min
j =i,i Mj

(i+1)2 + V (j, )

(5)

6

where i+1 is the margin for label i, with Mj as the augmented input. The initial condition of

DP

is:

V (i, 1)

=

1 (i1 )2

and

Mi1

=

{i}.

Then,

the optimal

Q

over

M

can

be

obtained

by solving

mini{1,*** ,q} V (i, q). Assume the training of linear SVM takes O(nd). The CC-DP algorithm is

shown as the following bottom-up procedure:

from the bottom, we first compute V (i, 1)

=

,1
(i1 )2

which

takes

O(nd).

Then

we

compute

V (i, 2)

=

minj=i,i

Mj1

{

1 (i2 )2

+

V (j, 1)},

which

requires

at most O(qnd), and set Mi2 = Mj1  {i}. Similarly, it takes at most O(q2nd) time complexity to

calculate V (i, q). Last, we iteratively solve this DP Equation, and use mini{1,*** ,q} V (i, q) to get

the optimal solution, which requires at most O(q3nd) time complexity.

Theorem 3 (Correctness of CC-DP). Q can be minimized by CC-DP, which means this Algorithm can find the globally optimal CC.

The proof can be referred to in the Supplementary Materials.

4.2 Greedy algorithm
We propose a CC-Greedy algorithm to find a locally optimal CC to speed up the CC-DP algorithm. To save time, we construct only one classifier chain with the locally optimal label order. Based on the training instances, we select the label from {1, 2, * * * , q} as the first label, if the maximum margin can be achieved over this label, without augmented input. The first label is denoted by 1. Then we select the label from the remainder as the second label, if the maximum margin can be achieved over this label with 1 as the augmented input. We continue in this way until the last label is selected. Finally, this algorithm will converge to the locally optimal CC. We present the details of the CC-Greedy algorithm in the Supplementary Materials, where the time complexity of this algorithm is O(q2nd).

5 Experiment
In this section, we perform experimental studies on a number of benchmark data sets from different domains to evaluate the performance of our proposed algorithms for multi-label classification. All the methods are implemented in Matlab and all experiments are conducted on a workstation with a 3.2GHZ Intel CPU and 4GB main memory running 64-bit Windows platform.
5.1 Data sets and baselines
We conduct experiments on eight real-world data sets with various domains from three websites.345 Following the experimental settings in [5] and [7], we preprocess the LLog, yahoo art, eurlex sm and eurlex ed data sets. Their statistics are presented in the Supplementary Materials. We compare our algorithms with some baseline methods: BR, CC, ECC, CCA [14] and MMOC [7]. To perform a fair comparison, we use the same linear classification/regression package LIBLINEAR [21] with L2-regularized square hinge loss (primal) to train the classifiers for all the methods. ECC is averaged over several CC predictions with random order and the ensemble size in ECC is set to 10 according to [5, 6]. In our experiment, the running time of PCC and EPCC [5] on most data sets, like slashdot and yahoo art, takes more than one week. From the results in [5], ECC is comparable with EPCC and outperforms PCC, so we do not consider PCC and EPCC here. CCA and MMOC are two state-of-the-art encoding-decoding [13] methods. We cannot get the results of CCA and MMOC on yahoo art 10, eurlex sm 10 and eurlex ed 10 data sets in one week. Following [22], we consider the Example-F1, Macro-F1 and Micro-F1 measures to evaluate the prediction performance of all methods. We perform 5-fold cross-validation on each data set and report the mean and standard error of each evaluation measurement. The running time complexity comparison is reported in the Supplementary Materials.
3http://mulan.sourceforge.net 4http://meka.sourceforge.net/#datasets 5http://cse.seu.edu.cn/people/zhangml/Resources.htm#data

7

Table 1: Results of Example-F1 on the various data sets (mean  standard deviation). The best results are in bold. Numbers in square brackets indicate the rank.

Data set

BR

CC

ECC

CCA

MMOC

CC-Greedy

CC-DP

yeast

0.6076  0.019[6]

image

0.5247  0.025[7]

slashdot

0.4898  0.024[6]

enron

0.4792  0.017[7]

LLog 10 0.3138  0.022[6]

yahoo art 10 0.4840  0.023[5]

eurlex sm 10 0.8594  0.003[5]

eurlex ed 10 0.7170  0.012[5]

Average Rank 5.88

0.5850 0.033[7] 0.5991 0.021[1] 0.5246 0.028[4] 0.4799 0.011[6] 0.3219 0.028[4] 0.5013 0.022[4] 0.8609 0.004[1] 0.7176 0.012[4]
3.88

0.6096 0.018[5] 0.5947 0.015[4] 0.5123 0.027[5] 0.4848 0.014[4] 0.3223 0.030[3] 0.5070 0.020[3] 0.8606 0.003[3] 0.7183 0.013[2]
3.63

0.6109  0.024[4] 0.5947  0.009[4] 0.5260  0.021[3] 0.4812  0.024[5] 0.2978  0.026[7] -
4.60

0.6132  0.021 [3] 0.5960  0.012[3] 0.4895  0.022[7] 0.4940  0.016[1] 0.3153  0.026[5] -
3.80

0.6144 0.021[1] 0.5939 0.021[6] 0.5266 0.022[2] 0.4894  0.016[2] 0.3269 0.023[2] 0.5131 0.015[2] 0.8600 0.004[4] 0.7183 0.013[2]
2.63

0.6135 0.015[2] 0.5976 0.015[2] 0.5268 0.022[1] 0.4880 0.015[3] 0.3298 0.025[1] 0.5135 0.020[1] 0.8609 0.004[1] 0.7190 0.013[1]
1.50

5.2 Prediction performance
Example-F1 results for our method and baseline approaches in respect of the different data sets are reported in Table 1. Other measure results are reported in the Supplementary Materials. From the results, we can see that: 1) BR is much inferior to other methods in terms of Example-F1. Our experiment provides empirical evidence that the label correlations exist in many real word data sets and because BR ignores the information about the correlations between the labels, BR achieves poor performance on most data sets. 2) CC improves the performance of BR, however, it underperforms ECC. This result verifies the answer to our first question stated in Section 1: the label order does affect the performance of CC; ECC, which averages over several CC predictions with random order, improves the performance of CC. 3) CC-DP and CC-Greedy outperforms CCA and MMOC. This studies verify that optimal CC achieve competitive results compared with stateof-the-art encoding-decoding approaches. 4) Our proposed CC-DP and CC-Greedy algorithms are successful on most data sets. This empirical result also verifies the answers to the last two questions stated in Section 1: the globally optimal CC exists and CC-DP can find the globally optimal CC which achieves the best prediction performance; the CC-Greedy algorithm achieves comparable prediction performance with CC-DP, while it requires lower time complexity than CC-DP. In the experiment, our proposed algorithms are much faster than CCA and MMOC in terms of both training and testing time, and achieve the same testing time with CC. Through the training time for our algorithms is slower than BR, CC and ECC. Our extensive empirical studies show that our algorithms achieve superior performance than those baselines.
6 Conclusion
To improve the performance of multi-label classification, a plethora of models have been developed to capture label correlations. Amongst them, classifier chain is one of the most popular approaches due to its simplicity and good prediction performance. Instead of proposing a new learning model, we discuss three important questions in this work regarding the optimal classifier chain stated in Section 1. To answer these questions, we first propose a generalized CC model. We then provide a theoretical analysis of the generalization error for the proposed generalized model. Based on our results, we obtain the answer to the second question: the globally optimal CC exists only if the minimization of the upper bound is achieved over this CC. It is very expensive to search over q! different label orders to find the globally optimal CC. Thus, we propose the CC-DP algorithm to simplify the search algorithm, which requires O(q3nd) complexity. To speed up the CC-DP algorithm, we propose a CC-Greedy algorithm to find a locally optimal CC, where the time complexity of the CCGreedy algorithm is O(q2nd). Comprehensive experiments on eight real-world multi-label data sets from different domains verify our theoretical studies and the effectiveness of proposed algorithms.
Acknowledgments
This research was supported by the Australian Research Council Future Fellowship FT130100746.
References
[1] Robert E. Schapire and Yoram Singer. BoosTexter: A Boosting-based System for Text Categorization. Machine Learning, 39(2-3):135-168, 2000.

8

[2] Zafer Barutcuoglu and Robert E. Schapire and Olga G. Troyanskaya. Hierarchical multi-label prediction of gene function. Bioinformatics, 22(7):22-7, 2006.
[3] Matthew R. Boutell and Jiebo Luo and Xipeng Shen and Christopher M. Brown. Learning Multi-Label Scene Classification. Pattern Recognition, 37(9):1757-1771, 2004.
[4] Grigorios Tsoumakas and Ioannis Katakis and Ioannis P. Vlahavas. Mining Multi-label Data. In Data Mining and Knowledge Discovery Handbook, pages 667-685, 2010. Springer US.
[5] Krzysztof Dembczynski and Weiwei Cheng and Eyke Hullermeier. Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains. Proceedings of the 27th International Conference on Machine Learning, pages 279-286, Haifa, Israel, 2010. Omnipress.
[6] Jesse Read and Bernhard Pfahringer and Geoffrey Holmes and Eibe Frank. Classifier Chains for Multilabel Classification. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, pages 254-269, Berlin, Heidelberg, 2009. Springer-Verlag.
[7] Yi Zhang and Jeff G. Schneider. Maximum Margin Output Coding. Proceedings of the 29th International Conference on Machine Learning, pages 1575-1582, New York, NY, 2012. Omnipress.
[8] Yuhong Guo and Suicheng Gu. Multi-Label Classification Using Conditional Dependency Networks. Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, pages 1300- 1305, Barcelona, Catalonia, Spain, 2011. AAAI Press.
[9] Sheng-Jun Huang and Zhi-Hua Zhou. Multi-Label Learning by Exploiting Label Correlations Locally. Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, Toronto, Ontario, Canada, 2012. AAAI Press.
[10] Feng Kang and Rong Jin and Rahul Sukthankar. Correlated Label Propagation with Application to Multilabel Learning. 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1719-1726, New York, NY, 2006. IEEE Computer Society.
[11] Weiwei Liu and Ivor W. Tsang. Large Margin Metric Learning for Multi-Label Prediction. Proceedings of the Twenty-Ninth Conference on Artificial Intelligence, pages 2800-2806, Texas, USA, 2015. AAAI Press.
[12] Mingkui Tan and Qinfeng Shi and Anton van den Hengel and Chunhua Shen and Junbin Gao and Fuyuan Hu and Zhen Zhang. Learning Graph Structure for Multi-Label Image Classification via Clique Generation. The IEEE Conference on Computer Vision and Pattern Recognition, 2015.
[13] Daniel Hsu and Sham Kakade and John Langford and Tong Zhang. Multi-Label Prediction via Compressed Sensing. Advances in Neural Information Processing Systems, pages 772-780, 2009. Curran Associates, Inc.
[14] Yi Zhang and Jeff G. Schneider. Multi-Label Output Codes using Canonical Correlation Analysis. Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 873- 882, Fort Lauderdale, USA, 2011. JMLR.org.
[15] Farbound Tai and Hsuan-Tien Lin. Multilabel Classification with Principal Label Space Transformation. Neural Computation, 24(9):2508-2542, 2012.
[16] Min-Ling Zhang and Kun Zhang. Multi-label learning by exploiting label dependency. Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 999-1008, QWashington, DC, USA, 2010. ACM.
[17] John Shawe-Taylor and Peter L. Bartlett and Robert C. Williamson and Martin Anthony. Structural Risk Minimization Over Data-Dependent Hierarchies. IEEE Transactions on Information Theory, 44(5):1926- 1940, 1998.
[18] Kristin P. Bennett and Nello Cristianini and John Shawe-Taylor and Donghui Wu. Enlarging the Margins in Perceptron Decision Trees. Machine Learning, 41(3):295-313, 2000.
[19] Michael J. Kearns and Robert E. Schapire. Efficient Distribution-free Learning of Probabilistic Concepts. Proceedings of the 31st Symposium on the Foundations of Computer Science, pages 382-391, Los Alamitos, CA, 1990. IEEE Computer Society Press.
[20] Peter L. Bartlett and John Shawe-Taylor. Generalization Performance of Support Vector Machines and Other Pattern Classifiers. Advances in Kernel Methods - Support Vector Learning, pages 43-54, Cambridge, MA, USA, 1998. MIT Press.
[21] Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang and Chih-Jen Lin. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871-1874, 2008.
[22] Qi Mao and Ivor Wai-Hung Tsang and Shenghua Gao. Objective-Guided Image Annotation. IEEE Transactions on Image Processing, 22(4):1585-1597, 2013.
9

