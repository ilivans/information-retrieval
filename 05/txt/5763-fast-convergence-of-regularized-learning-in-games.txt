Fast Convergence of Regularized Learning in Games

Vasilis Syrgkanis Microsoft Research
New York, NY vasy@microsoft.com
Haipeng Luo Princeton University
Princeton, NJ haipengl@cs.princeton.edu

Alekh Agarwal Microsoft Research
New York, NY alekha@microsoft.com
Robert E. Schapire Microsoft Research
New York, NY schapire@microsoft.com

Abstract
We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at O(T 3/4), while the sum of utilities converges to an approximate optimum at O(T 1)-an improvement upon the worst case O(T 1/2) rates. We show a blackbox reduction for any algorithm in the class to achieve O(T 1/2) rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of Rakhlin and Shridharan [17] and Daskalakis et al. [4], who only analyzed two-player zero-sum games for specific algorithms.

1 Introduction

What happens when players in a game interact with one another, all of them acting independently and selfishly to maximize their own utilities? If they are smart, we intuitively expect their utilities -- both individually and as a group -- to grow, perhaps even to approach the best possible. We also expect the dynamics of their behavior to eventually reach some kind of equilibrium. Understanding these dynamics is central to game theory as well as its various application areas, including economics, network routing, auction design, and evolutionary biology.

It is natural in this setting for the players to each make use of a no-regret learning algorithm for mak-

ing their decisions, an approach known as decentralized no-regret dynamics. No-regret algorithms

are a strong match for playing games because their regret bounds hold even in adversarial environ-

ments. As a benefit, these bounds ensure that each player's utility approaches optimality. When

played against one another, it can also be shown that the sum of utilities approaches an approximate

optimum [2, 18], and the player strategies converge to an equilibrium under appropriate condi-

tions [6, 1, 8], at rates governed by the regret bounds. Well-known families of no-regret algorithms

include multiplicative-weights [13, 7], Mirror Descent [14], and Follow the Regularized/Perturbed

Leader [12]. (See [3, the worst-case rate of

O19(]1/fopr Tex),cewllheincthoisveurnviimewpsro.)vaFbolre

all in

of these, the average regret fully adversarial scenarios.

vanishes

at

However, the players in our setting are facing other similar, predictable no-regret learning algorithms, a chink that hints at the possibility of improved convergence rates for such dynamics. This was first observed and exploited by Daskalakis et al. [4]. For two-player zero-sum games, they developed a decentralized variant of Nesterov's accelerated saddle point algorithm [15] and showed that each player's average regret converges at the remarkable rate of O(1/T ). Although the resulting

1

dynamics are somewhat unnatural, in later work, Rakhlin and Sridharan [17] showed surprisingly that the same convergence rate holds for a simple variant of Mirror Descent with the seemingly minor modification that the last utility observation is counted twice. Although major steps forward, both these works are limited to two-player zero-sum games, the very simplest case. As such, they do not cover many practically important settings, such as auctions or routing games, which are decidedly not zero-sum, and which involve many independent actors. In this paper, we vastly generalize these techniques to the practically important but far more challenging case of arbitrary multi-player normal-form games, giving natural no-regret dynamics whose convergence rates are much faster than previously possible for this general setting.

Contributions. We show that the average welfare of the game, that is, the sum of player utilities, craotnevoefrgOes(1t/opapTp)r.oCxiomnactreeltyeloy,ptwime aslhowwelfaanreatautrathl eclraastseoOf (r1eg/uTl)a,rirzaetdhenrot-hraengrtehtealpgroerviitohumsslywkinthowrencency bias that achieve welfare at least ( /(1 + ))OPT O(1/T ), where and  are parameters in a smoothness condition on the game introduced by Roughgarden [18]. For the same class of algorithms, we show that each individual player's average regret converges to zero at the rate O T 3/4 . Thus, our results entail an algorithm for computing coarse correlated equilibria in a decentralized manner with significantly faster convergence than existing methods.

WwheilaedrdoibtiuosntalyllymgaiivnetaainbinlagcOk-(b1o/xpreTd)urcetgiornettahgaat ipnrsetsaenryveosptphoenfeanstt

rates in favorable in the worst case.

environments,

Even for two-person zero-sum games, our results for general games expose a hidden generality and

modularity underlying the previous results [4, 17]. First, our analysis identifies stability and recency

bias as key structural ingredients of an algorithm with fast rates. This covers the Optimistic Mirror

Descent of Rakhlin and Sridharan [17] as an example, but also applies to optimistic variants of Fol-

low the Regularized Leader (FTRL), including dependence on arbitrary weighted windows in the

history as opposed to just the utility from the last round. Recency bias is a behavioral pattern com-

monly observed in game-theoretic environments [9]; as such, our results can be viewed as a partial

theoretical justification. Second, gence against similar algorithms

wphreilveioautsthaeppsaromaechtiems einO[(41,/1p7]To)nreagcrheitervaitnegs

both faster converagainst adversaries

were shown via ad-hoc modifications of specific algorithms. We give a black-box modification

which is not algorithm specific and works for all these optimistic algorithms.

Finally, we simulate a 4-bidder simultaneous auction game, and compare our optimistic algorithms against Hedge [7] in terms of utilities, regrets and convergence to equilibria.

2 Repeated Game Model and Dynamics

Consider a static game G among a set N of n players. Each player i has a strategy space Si and a utility function ui : S1  . . .  Sn ! [0, 1] that maps a strategy profile s = (s1, . . . , sn) to a utility ui(s). We assume that the strategy space of each player is finite and has cardinality d, i.e. |Si| = d. We denote with w = (w1, . . . , wn) a profile of mixed strategies, where wi 2 (Si) and wi,x is the probability of strategy x 2 Si. Finally let Ui(w) = Esw[ui(s)], the expected utility of player i.

We consider the setting where the game G is played repeatedly for T time steps. At each time

sotbespertveeascthhepleaxypeercitepdicuktsiliatymhiexewdosutrldatehgayvewreit c2eived(Shia)d.

At he

the end of the iteration each played any possible strategy

player i x 2 Si.

More formally, let uti,x = Es iwt i [ui(x, s i)], where s i is the set of strategies of all but the ith

player, and let uti = (uti,x)x2Si . At the end of each iteration each player i observes uti. Observe that

the expected utility of a player at iteration t is simply the inner product hwit, utii.

No-regret dynamics. ishing regret algorithm.

We assume Formally,

that the for each

players player

each i, the

decide regret

their after

Tstratitmegeyswtepit sbaisseedquoanl

a vanto the

maximum gain he could have achieved by switching to any other fixed strategy:

ri(T ) = sup XT wi
wi2 (Si) t=1

wit, uti .

2

The algorithm has vanishing regret if ri(T ) = o(T ).

Approximate Efficiency of No-Regret Dynamics. We are interested in analyzing the average

welfare defined W (w)

of as =

Etshsuecwhsu[vmWano(isfs)ht]ih.negWpreleagwyreaernt tustteiolqiutlioeewns:ceerWsb. o(suF)nodr=haoPgwivi2feaNnr

strategy profile s the social welfare is ui(s). We overload notation to denote the average welfare of the sequence is,

with respect to the optimal welfare of the static game:

OPT = max W (s).
s2S1 ...Sn

This is the optimal welfare achievable in the absence of player incentives and if a central coordinator could dictate each player's strategy. We next define a class of games first identified by Roughgarden [18] on which we can approximate the optimal welfare using decoupled no-regret dynamics.

Definition 1 (Smooth game that for any strategy profile

s[:18P])i.2AN

game is ui(si , s

(, i)

)-smooth OPT

if there exists W (s).

a

strategy

profile

s

such

In words, any player using his optimal strategy continues to do well irrespective of other players' strategies. This condition directly implies near-optimality of no-regret dynamics as we show below. Proposition 2. In a ( , )-smooth game, if each player i suffers regret at most ri(T ), then:

1 T

XT W (wt)
t=1

1 +  OPT

1 1+

1 T

X ri(T )

=

1 

O

PT

i2N

11 1+T

X ri(T ),

i2N

where the factor  = (1 + )/ is called the price of anarchy (POA).

This proposition is essentially a more explicit version of Roughgarden's result [18]; we provide a pbrviery(orTgothef)esi=ntqoutOPhae(nOptAaitplyaoptge1an(+1ddr)iaxTtT1e)fPoo, firniOc2woN(mhnirppcilh(elTtoceg)an.s(eedsT)ts/h.hTeeTrl)aeh.teAtaerrsreetwshmueeloatwnrseiyhlmloawswlhgsooowturhil,tadhftomimtrhsspeowlymchoteinhcvnahetarttaguhcerehanialcevceveleartoasasgePersOewogAefrelnfitasorr-dearretciegvoreonenf-t algorithms the average welfare converges at the much faster rate of O(n2 log(d)/T ).

3 Fast Convergence to Approximate Efficiency

In this section, we present our main theoretical results characterizing a class of no-regret dynamics which lead to faster convergence in smooth games. We begin by describing this class. Definition 3 (RVU property). We say that a vanishing regret algorithm satisfies the Regret bounded by Variation in Utilities (RVU) property with parameters  > 0 and 0 <  and a pair of dual norms (k * k, k * k)1 if its regret on any sequence of utilities u1, u2, . . . , uT is bounded as

XT w
t=1

wt, ut   +

XT kut

t=1

ut 1k2

XT kwt
t=1

wt 1k2.

(1)

Typical online in their vanilla and Sridharan

learning algorithms such form, as the middle term [16] give a modification

aogsfroMMwiisrrrraoosrrPDDeeTts=scc1eenkntutatwnkdi2thFfoTtrhRitshLepdsreoopmneoerttthsya,otdaisnsf.dyHwthoeewwRevVileUlr,ppRrreoaskpehenrlttiyna

similar variant of FTRL in the sequel.

We now present two sets of results when each player uses an algorithm with this property. The first discusses the convergence of social welfare, while the second governs the convergence of the individual players' utilities at a fast rate.

1The dual to a norm k * k is defined as kvk = supkuk1 hu, vi.

3

3.1 Fast Convergence of Social Welfare

GPseicvTtt=eion1nrPib(rooTup)nodsinsititoohrnids e2sru, mtwo weohboetnanliyneancceohendpvleatroygeeurnnucdeseerrsasdttaeynsndaomfthitcehseewvsoiotlchuiattihloenwReoVlffUatrhpeer.opsOueumrtry.omfaipnlaryeesrusl't

regrets in this

Theorem , and

4. Suppose such that

that 

the algorithm of each player i /(n 1)2 and k * k = k * k1.

sTahteisnfiPes it2hNe

property ri(T ) 

RVU n.

with

parameters

Proof. Since ui(s)  1, definitions imply: kuti

uti

1k



P
s

i

Q
j6=i

wjt,sj

Q
j6=i

wjt,sj1

. The

latter is the total variation distance of two product distributions. By known properties of total varia-

tion (see e.g. [11]), this is bounded by the sum of the total variations of each marginal distribution:

XY wjt,sj

YX

wjt,sj1 

kwjt

wjt 1k

(2)

s i j6=i

j6=i

j6=i

By

Jensen's

inequality,

P
j6=i

kwjt

wjt

1

2 k



(n

1)

P
j6=i

kwjt

wjt 1k2, so that

X XX

X

kuti uti 1k2  (n 1)

kwjt wjt 1k2 = (n 1)2

kwit wit 1k2.

i2N

i2N j6=i

i2N

The theorem follows by summing up the RVU property (1) for each player i and observing that the summation of the second terms is smaller than that of the third terms and thereby can be dropped.

Remark: The rates from the theorem depend on , which will be O(1) in the sequel. The above theorem extends to the case where k * k is any norm equivalent to the 1 norm. The resulting requirement on in terms of can however be more stringent. Also, the theorem does not require that all players use the same no-regret algorithm unlike previous results [4, 17], as long as each player's algorithm satisfies the RVU property with a common bound on the constants. We now instantiate the result with examples that satisfy the RVU property with different constants.

3.1.1 Optimistic Mirror Descent

The optimistic mirror descent (OMD) algorithm of Rakhlin and Sridharan [16] is parameterized by

an to

adaptive a norm k

predictor sequence * k. Let DR denote

tMheti

and a regularizer2 R which is Bregman divergence associated

1-strongly convex3 with respect with R. Then the update rule is

defined as follows: let gi0 = argming2 (Si) R(g) and

then:

(u, g) = argmax  * hw, ui DR(w, g),
w2 (Si)

wit = (Mti, git 1), and git = (uti, git 1)

Then the following proposition can be obtained for this method.

Proposition 5. The OMD with constants  = R/,

algorithm = , =

using stepsize 1/(8), where

 R

and Mti = maxi

s=upfutiDR1s(aft,isgfii0e)s.

the

RVU

property

The proposition follows by further crystallizing the arguments of Rakhlin and Sridaran [17], and we

provide a proof in the appendix for completeness. The above proposition, along with Theorem 4,

immediately yields the following corollary, which had been proved by Rakhlin and Sridharan [17]

for two-person zero-sum games, and which we here extend to

ChaovreoPllair2yN6.riI(fTe)ach

player runs OMD wpith Mti nR/  n(n 1) 8R =

= uti O(1).

1 and

general stepsize

games. p  = 1/( 8(n

1)), then we

The corollary follows by noting that the condition  /(n 1)2 is met with our choice of .

2Here and in the sequel, we can use a different regularizer Ri for each player i, without qualitatively affecting any of the results.

3R is 1-strongly convex if R

u+v 2



R(u)+R(v) 2

ku

vk2 8

,

8u,

v.

4

3.1.2 Optimistic Follow the Regularized Leader

We next consider a different class of algorithms denoted as optimistic follow the regularized leader

(OFTRL). This algorithm is similar but not equivalent to OMD, and is an analogous extension of

standard FTRL [12]. This algorithm takes the same parameters as for OMD and is defined as follows:

Let wi0 = argminw2 (Si) R(w) and:

*+

TX1

wiT = argmax w, uti + MTi

w2 (Si)

t=1

R(w) 

.

We consider recency bias

three variants of OFTRL in different forms.

with

different

choices

of

the

sequence

Mti ,

incorporating

the

One-step recency bias:  The simplest form of OFTRL uses Mti = uti 1 and obtains the following result, where R = maxi supf2 (Si) R(f ) inff2 (Si) R(f ) .

Proposition 7. The OFTRL algorithm using stepsize with constants  = R/, =  and = 1/(4).



and

Mti

=

uti

1 satisfies the RVU property

Combined with Theorem 4, this yields the following constant bound on the total regret of all players: CPorollary 8. If each player runs OFTRL with Mti = uti 1 and  = 1/(2(n 1)), then we have
i2N ri(T )  nR/  2n(n 1)R = O(1).
Rakhlin and Sridharan [16] also analyze an FTRL variant, but require a self-concordant barrier for the constraint set as opposed to an arbitrary strongly convex regularizer, and their bound is missing the crucial negative terms of the RVU property which are essential for obtaining Theorem 4.

HP-tste1p
 =t

recency H ui /H.

bias: More generally, given a We have the following proposition.

window

Proposition 9. The OFTRL algorithm using stepsize  and

size H, one

Mti

=

Pt 1
 =t

can define Mti H ui /H satisfies

= the

RVU property with constants  = R/, = H2 and = 1/(4).

Setting  = 1/(2H(n 1)), we obtain the analogue of Corollary 8, with an extra factor of H.

Geometrically discounted recency bias: The next proposition considers an alternative form of

recency bias which includes all the previous utilities, but with a geometric discounting.

Proposition 10.

The OFTRL algorithm using stepsize  and Mti

=

Pt

1
1

Pt 1
  =0

the RVU property with constants  = R/, = /(1

)3 and

 =0
= 1/(8).

 ui satisfies

Note that these choices for Mti can also be used in OMD with qualitatively similar results.

3.2 Fast Convergence of Individual Utilities

The previous section shows implications of the RVU property on the social welfare. This section complements these with a similar result for each player's individual utility.

Theorem 11. Suppose that the players use algorithms satisfying the RVU property with parameters

pla>yer0,PTt=>1

0, hwi

0. If we further have the stability property kwit wit, utii   + 2(n 1)2T.

wit+1k  , then for any

Similar reasoning as in Theorem 4 yields: kuti

uti 1k2  (n

1)

P
j6=i

kwjt

wjt 1k2  (n

1)22,

and summing the terms gives the theorem.

Noting that OFTRL satisfies the RVU property with constants given in Proposition 7 and stability property with  = 2 (see Lemma 20 in the appendix), we have the following corollary.

Cthoernowllearhyav1e2.PIfTt=al1l

players use the hwi wit, utii

OF(TRR+L a4l)gpornithm1w*iTth1/M4.ti

=

uti

1 and  = (n

1) 1/2T

1/4,

5

Similar results hold for the other forms of recency bias, as well as for OMD. Corollary 12 gives a fgaasmt ceo. nTvheirsgeimncperoravteesotfhethperpelvaiyoeurssl'ysktrnaotewgniecsotnovtehregesentcoefrcaotearpseTco(er.rge.la[t1e0d])eqtouiCliCbrEiau(sCinCgEn)aotuf rtahle, decoupled no-regret dynamics defined in [4].

4 Robustness to Adversarial Opponent

So far we have shown simple dynamics with rapid convergence properties in favorable environments

when each player in the game uses an algorithm with the RVU property. It is natural to wonder if

this comes at the cost of worst-case guarantees when some players do not use algorithms with this

property. Rakhlin and Sridharan [17] address this concern by modifying the OMD algorithm with

additional while still

sgmuaoroatnhtieneginagndO(a1d/apptiTve)

step-sizes regret for

so as to preserve the fast rates each player, no matter how the

in the favorable opponents play.

case It is

not so obvious how this modification might extend to other procedures, and it seems undesirable

to abandon the black-box regret transformations we used to obtain Theorem 4. In this section, we

present the fast

acognevneerrgicenwcaeyinoffatrvaonrsafbolremsientgtinagnsa,lbguotriatlhwmaywshgicuharsaanttiesfiesesatwheorRsVt-UcapsreorpeegrrteytsooftOha(t1i/t pretTai)n. s

In order to present our modification, we need a parametric form of the RVU property which will also involve a tunable parameter of the algorithm. For most online learning algorithms, this will correspond to the step-size parameter used by the algorithm. Definition 13 (RVU() property). We say that a parametric algorithm A() satisfies the Regret bounded by Variation in Utilities() (RVU()) property with parameters , , > 0 and a pair of dual norms (k * k, k * k) if its regret on any sequence of utilities u1, u2, . . . , uT is bounded as

XT w
t=1

wt, ut



 

+



XT kut
t=1

ut 1k2

XT  t=1 kwt

wt 1k2.

(3)

In both OMD and OFTRL algorithms from Section 3, the parameter  is precisely the stepsize . We now show an adaptive choice of  according to an epoch-based doubling schedule.

Black-box reduction. Given a parametric algorithm A() as a black-box we construct a wrapper

Athe0 abpnaldasyeBder1oi=nhta1hs,eaadnnoudupfbpoleirnrgbo=truinc1kd, :2o,Tf.hB.e.r,aoTlgnortehrpietehqamut:aonftietyacPh pTt=la1ykeur tiprouceti ed1sk2in.

epochs. We start

At each epoch r with a parameter

1. 2.

Play according to

If

P
t=1

|uti

uti

A(r 1k2

)

and Br

receive :

ui

.

n

o

(a) Update r

r + 1, Br

2Br, r = min

p Br

,



, with  as in Equation (3).

(b) Start a new run of A with parameter r.

Theorem 14. Algorithm A0 achieves regret at most the minimum of the following two terms:

XT wi
t=1
XT wi
t=1

!

wit, uti  log(T )

2

+

 

+

(2

+



*

0

wit, uti

 log(T ) @1 +

 

+ (1 +  *

XT ) kuti uti ) *tvuut=12 XT kuti
t=1

1 k2 uti

XT



kwit
t=1

1

1 k2 A

wit 1k2; (4) (5)

That is, the algorithm satisfies the RVU property, and also has regret that can never exceed O(pT ).

The theorem thus yields the following corollary, which illustrates the stated robustness of A0. Corollary 15. Algorithm A0, with  = (2+ )(n 1)2 log(T ) , achieves regret O(pT ) against any

adversarial sequence, while at players use such an algorithm,

tthheens:amPei2tiNmeris(aTti)sfyinnglothge(Tco)n(di/tion+s o2f)T=heOor(e1m).4.

Thereby,

if

all

6

Cumulative regret Cumulative regret

Sum of regrets

1500

Hedge

Optimistic Hedge

1000

500

0 0 2000 4000 6000 8000 10000
Number of rounds

Max of regrets
400 Hedge
350 Optimistic Hedge
300
250
200
150
100
50
0 0 2000 4000 6000 8000 10000
Number of rounds

Figure 1: Maximum and sum of individual regrets over time under the Hedge (blue) and Optimistic Hedge (red) dynamics.

Proof. Observe that for such , we have that: (2 +  * ) log(T )  (2 + ) log(T )  (n 1)2 . Therefore, algorithm A0, satisfies the sufficient conditions of Theorem 4.

If A() is the OFTRL algorithm, then we know by Proposition 7 that the above result applies with

 = R = maxw R(w), resulting algorithm A0 will all players use algorithm A0

= 1, =

have then

rPegret
i2N

1 4

and



at most:

ri(T ) =

= . O(n2

pSeTtt)inaggains=t an(2a+rbi)t(rnary1)2ad=vers1a2(rny1,

O(n3 log(T )).

1)2 , the while if

An analogue of Theorem 11 can also be established for this algorithm:

Cor=olTlar1y/416a.chIifevAessraetgisrfietesOt(hTe1R/4V)Uif(p)lapyreodpaegrtayi,nastnidtsaellfs,oankdwOit(pwT )it

1k  against

, any

then A0 with opponent.

Once again, OFTRL satisfies the above conditions with  = 2, implying robust convergence.

5 Experimental Evaluation

We analyzed the performance of optimistic follow the regularized leader with the entropy regularizer,

which corresponds to the Hedge algorithm [7] modified so that the last iteration's utility for each

strategy is double counted;

player i playing strategy j at

 than exp



*

PT 1
t=1

 utij

we refer to it as Optimistic iteration T is proportional to
as is standard for Hedge.

Hedge. exp

M* orPe fTto=r1m2 ualtiljy,+th2euTipjro1bab,ilriatythoefr

We studied a simple auction where n players are bidding for m items. Each player has a value v for getting at least one item and no extra value for more items. The utility of a player is the value for the allocation he derived minus the payment he has to make. The game is defined as follows: simultaneously each player picks one of the m items and submits a bid on that item (we assume bids to be discretized). For each item, the highest bidder wins and pays his bid. We let players play this game repeatedly with each player invoking either Hedge or optimistic Hedge. This game, and generalizations of it, are known to be (1 1/e, 0)-smooth [20], if we also view the auctioneer as a player whose utility is the revenue. The welfare of the game is the value of the resulting allocation, hence not a constant-sum game. The welfare maximization problem corresponds to the unweighted bipartite matching problem. The POA captures how far from the optimal matching is the average allocation of the dynamics. By smoothness we know it converges to at least 1 1/e of the optimal.

Fast convergence of individual and average regret. We run the game for n = 4 bidders and m = 4 items and valuation v = 20. The bids are discretized to be any integer in [1, 20]. We find that the sum of the regrets and the maximum individual regret of each player are remarkably lower under Optimistic Hedge as opposed to Hedge. In Figure 1 we plot the maximum individual regret as well as the sum of the regrets under the two algorithms, using  = 0.1 for both methods. Thus convergence to the set of coarse correlated equilibria is substantially faster under Optimistic Hedge,

7

Expected bid Utility

Expected bids of a player
3 Hedge Optimistic Hedge
2.5
2
1.5
1
0.5 0 2000 4000 6000 8000 10000
Number of rounds

Utility of a player
18 Hedge
16 Optimistic Hedge
14
12
10
8
6
4 0 2000 4000 6000 8000 10000
Number of rounds

Figure 2: Expected bid and per-iteration utility of a player on one of the four items over time, under Hedge (blue) and Optimistic Hedge (red) dynamics.

confirming our results in Section 3.2. We also observe similar behavior when each player only has value on a randomly picked player-specific subset of items, or uses other step sizes.

More stable dynamics. We observe that the behavior under Optimistic Hedge is more stable than under Hedge. In Figure 2, we plot the expected bid of a player on one of the items and his expected utility under the two dynamics. Hedge exhibits the sawtooth behavior that was observed in generalized first price auction run by Overture (see [5, p. 21]). In stunning contrast, Optimistic Hedge leads to more stable expected bids over time. This stability property of optimistic Hedge is one of the main intuitive reasons for the fast convergence of its regret.

Welfare. In this class of games, we did not observe any significant difference between the average

welfare of the methods. The key reason is the following: the proof that no-regret dynamics are

approximately efficient (Proposition 2) only relies on the fact that each player does not have regret

astgraaitnegstietsheissetrxapteegriymseintuaslleyd

in the definition of a smooth game. In this game, regret against these comparable under both algorithms, even though regret against the best

fixed strategy is remarkably different. This indicates a possibility for faster rates for Hedge in

terms of welfare. In Appendix H, we show fast convergence of the efficiency of Hedge for cost-

minimization games, though with a worse POA .

6 Discussion
This work extends and generalizes a growing body of work on decentralized no-regret dynamics in many ways. We demonstrate a class of no-regret algorithms which enjoy rapid convergence when played against each other, while being robust to adversarial opponents. This has implications in computation of correlated equilibria, as well as understanding the behavior of agents in complex multi-player games. There are a number of interesting questions and directions for future research which are suggested by our results, including the following: Convergence rates for vanilla Hedge: The fast rates of our paper do not apply to algorithms such as Hedge without modification. Is this modification to satisfy RVU only sufficient or also necessary? If not, are there counterexamples? In the supplement, we include a sketch hinting at such a counterexample, but also showing fast rates to a worse equilibrium than our optimistic algorithms. Convergence of players' strategies: The OFTRL algorithm often produces much more stable trajectories empirically, as the players converge to an equilibrium, as opposed to say Hedge. A precise quantification of this desirable behavior would be of great interest. Better rates with partial information: If the players do not observe the expected utility function, but only the moves of the other players at each round, can we still obtain faster rates?
8

References
[1] A. Blum and Y. Mansour. Learning, regret minimization, and equilibria. In Noam Nisan, Tim Roughgarden, E va Tardos, and Vijay Vazirani, editors, Algorithmic Game Theory, chapter 4, pages 4-30. Cambridge University Press, 2007.
[2] Avrim Blum, MohammadTaghi Hajiaghayi, Katrina Ligett, and Aaron Roth. Regret minimization and the price of total anarchy. In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC '08, pages 373-382, New York, NY, USA, 2008. ACM.
[3] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006.
[4] Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms for zero-sum games. Games and Economic Behavior, 92:327-348, 2014.
[5] Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz. Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords. Working Paper 11765, National Bureau of Economic Research, November 2005.
[6] Dean P. Foster and Rakesh V. Vohra. Calibrated learning and correlated equilibrium. Games and Economic Behavior, 21(12):40 - 55, 1997.
[7] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119 - 139, 1997.
[8] Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29(1):79-103, 1999.
[9] Drew Fudenberg and Alexander Peysakhovich. Recency, records and recaps: Learning and nonequilibrium behavior in a simple decision problem. In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC '14, pages 971-986, New York, NY, USA, 2014. ACM.
[10] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):1127-1150, 2000.
[11] Wassily Hoeffding and J. Wolfowitz. Distinguishability of sets of distributions. Ann. Math. Statist., 29(3):700-718, 1958.
[12] Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 71(3):291 - 307, 2005. Learning Theory 2003 Learning Theory 2003.
[13] Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and computation, 108(2):212-261, 1994.
[14] AS Nemirovsky and DB Yudin. Problem complexity and method efficiency in optimization. 1983. [15] Yu. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):127-
152, 2005. [16] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In COLT 2013,
pages 993-1019, 2013. [17] Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences.
In Advances in Neural Information Processing Systems, pages 3066-3074, 2013. [18] T. Roughgarden. Intrinsic robustness of the price of anarchy. In Proceedings of the 41st annual ACM
symposium on Theory of computing, pages 513-522, New York, NY, USA, 2009. ACM. [19] Shai Shalev-Shwartz. Online learning and online convex optimization. Found. Trends Mach. Learn.,
4(2):107-194, February 2012. [20] Vasilis Syrgkanis and E va Tardos. Composable and efficient mechanisms. In Proceedings of the Forty-
fifth Annual ACM Symposium on Theory of Computing, STOC '13, pages 211-220, New York, NY, USA, 2013. ACM.
9

