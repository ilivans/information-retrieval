Empirical Localization of Homogeneous Divergences on Discrete Sample Spaces
Takashi Takenouchi Department of Complex and Intelligent Systems
Future University Hakodate 116-2 Kamedanakano, Hakodate, Hokkaido, 040-8655, Japan
ttakashi@fun.ac.jp
Takafumi Kanamori Department of Computer Science and Mathematical Informatics
Nagoya University Furocho, Chikusaku, Nagoya 464-8601, Japan
kanamori@is.nagoya-u.ac.jp
Abstract
In this paper, we propose a novel parameter estimator for probabilistic models on discrete space. The proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant, which is frequently infeasible for models in the discrete space. We investigate statistical properties of the proposed estimator such as consistency and asymptotic normality, and reveal a relationship with the information geometry. Some experiments show that the proposed estimator attains comparable performance to the maximum likelihood estimator with drastically lower computational cost.
1 Introduction
Parameter estimation of probabilistic models on discrete space is a popular and important issue in the fields of machine learning and pattern recognition. For example, the Boltzmann machine (with hidden variables) [1] [2] [3] is a very popular probabilistic model to represent binary variables, and attracts increasing attention in the context of Deep learning [4]. A training of the Boltzmann machine, i.e., estimation of parameters is usually done by the maximum likelihood estimation (MLE). The MLE for the Boltzmann machine cannot be explicitly solved and the gradient-based optimization is frequently used. A difficulty of the gradient-based optimization is that the calculation of the gradient requires calculation of a normalization constant or a partition function in each step of the optimization and its computational cost is sometimes exponential order. The problem of computational cost is common to the other probabilistic models on discrete spaces and various kinds of approximation methods have been proposed to solve the difficulty. One approach tries to approximate the probabilistic model by a tractable model by the mean-field approximation, which considers a model assuming independence of variables [5]. Another approach such as the contrastive divergence [6] avoids the exponential time calculation by the Markov Chain Monte Carlo (MCMC) sampling.
In the literature of parameters estimation of probabilistic model for continuous variables, [7] employs a score function which is a gradient of log-density with respect to the data vector rather than parameters. This approach makes it possible to estimate parameters without calculating the normalization term by focusing on the shape of the density function. [8] extended the method to discrete variables, which defines information of "neighbor" by contrasting probability with that of a flipped
1

variable. [9] proposed a generalized local scoring rules on discrete sample spaces and [10] proposed an approximated estimator with the Bregman divergence.
In this paper, we propose a novel parameter estimator for models on discrete space, which does not require calculation of the normalization constant. The proposed estimator is defined by minimization of a risk function derived by an unnormalized model and the homogeneous divergence having a weak coincidence axiom. The derived risk function is convex for various kind of models including higher order Boltzmann machine. We investigate statistical properties of the proposed estimator such as the consistency and reveal a relationship between the proposed estimator and the -divergence [11].

2 Settings

Let X be a d-dimensional vector of random variables in a discrete spaceX (typically {+1, -1}d) and a bracket f  be summation of a function f (x) on X , i.e., f  = xX f (x). Let M and P be a space of all non-negative finite measures on X and a subspace consisting of all probability measures on X , respectively.

M = {f (x) |f  < , f (x)  0 } , P = {f (x) |f  = 1, f (x)  0 } .

In this paper, we focus on parameter estimation of a probabilistic model q(x) on X , written as

q (x)

=

q (x) Z

(1)

where  is an m-dimensional vector of parameters, q(x) is an unnormalized model in M and Z = q is a normalization constant. A computation of the normalization constant Z sometimes
requires calculation of exponential order and is sometimes difficult for models on the discrete space. Note that the unnormalized model q(x) is not normalized and q = xX q(x) = 1 does not necessarily hold. Let (x) be a function on X and throughout the paper, we assume without loss of generality that the unnormalized model q(x) can be written as

q(x) = exp((x)).

(2)

Remark 1. By setting (x) as (x) - log Z, the normalized model (1) can be written as (2).

Example 1. The Bernoulli distribution on X = {+1, -1} is a simplest example of the probabilistic model (1) with the function (x) = x.

Example 2. With a function ,k(x) = (x1, . . . , xd, x1x2, . . . , xd-1xd, x1x2x3, . . .), we can define a k-th order Boltzmann machine [1, 12].

Example 3. Let xo  {+( 1, -1}d)1 and xh  {+1, -1}d2 be an observed vector and hidden vector, respectively, and x = xTo , xTh  {+1, -1}d1+d2 where T indicates the transpose, be a con-

catenated vector. A function h,(xo) for the Boltzmann machine with hidden variables is written

as



h,(xo) = log exp(,2(x)),

(3)

xh
 where xh is the summation with respect to the hidden variable xh.

Let us assume that a dataset D = {xi}ni=1 generated by an underlying distribution p(x), is given and Z be a set of all patterns which appear in the dataset D. An empirical distribution p(x) associated

with the dataset D is defined as

{ nx p(x) = n

x  Z,

0 otherwise,

where

nx

=

n
i=1

I(xi

=

x)

is

a

number

of

pattern

x

appeared

in

the

dataset

D.

Definition 1. For the unnormalized model (2) and distributions p(x) and p(x) in P , probability

functions r,(x) and r,(x) on X are defined by

r,(x) = p(x)pqq1(-x)1- , r,(x) = p(x)pqq1(-x)1- .

2

The distribution r, (r,) is an e-mixture model of the unnormalized model (2) and p(x) (p(x)) with ratio  [11].
Remark 2. We observe that r0,(x) = r0,(x) = q(x), r1,(x) = p(x), r1,(x) = p(x). Also if p(x) = q0 (x), r,0 (x) = q0 (x) holds for an arbitrary .

fTroeqeusteinmtlayteetmheplpoayreadm, ewtehrereoLf p(ro)b=abilisnit=ic1

model q, the log q(xi) is

MLE defined by mle = the log-likelihood of the

argmax L() is parameter  with

the model q. Though the MLE is asymptotically consistent and efficient estimator, a main drawback

of the MLE is that computational cost for probabilistic models on the discrete space sometimes

becomes exponential. Unfortunately the MLE does not have an explicit solution in general, the

estimation of the parameter can be done by the gradient based optimization with a gradient p  -

q 

of

log-likelihood,

where



=

 

.

While the

first term

can

be

easily

calculated,

the

second

term includes calculation of the normalization term Z, which requires 2d times summation for

X = {+1, -1}d and is not feasible when d is large.

3 Homogeneous Divergences for Statistical Inference

Divergences are an extension of the squared distance and are often used in statistical inference. A formal definition of the divergence D(f, g) is a non-negative valued function on MxM or on P xP such that D(f, f ) = 0 holds for arbitrary f . Many popular divergences such as the Kullback-Leilber (KL) divergence defined on P x P enjoy the coincidence axiom, i.e., D(f, g) = 0 leads to f = g. The parameter in the statistical model q is estimated by minimizing the divergence D(p, q), with respect to .

In the statistical inference using unnormalized models, the coincidence axiom of the divergence is not suitable, since the probability and the unnormalized model do not exactly match in general. Our purpose is to estimate the underlying distribution up to a constant factor using unnormalized models. Hence, divergences having the property of the weak coincidence axiom, i.e., D(f, g) = 0 if and only if g = cf for some c > 0, are good candidate. As a class of divergences with the weak coincidence axiom, we focus on homogeneous divergences that satisfy the equality D(f, g) = D(f, cg) for any f, g  M and any c > 0.

A representative of homogeneous divergences is the pseudo-spherical (PS) divergence [13], or in

other words, -divergence [14], that is defined from the Holder inequality. Assume that  is a

positive constant. For all non-negative functions f, g in M, the Holder inequality

 f



+1



1  +1

g

+1





 +1

-

f g



0

holds. The inequality becomes an equality if and only if f and g are linearly dependent. The PS-

divergence D(f, g) for f, g  M is defined by

D(f, g)

=

1

1 +



log

 f



+1

+

1

 +



log

 g

+1

-

log f g ,

 > 0.

(4)

The PS divergence is homogeneous, and the Holder inequality ensures the non-negativity and the
weak coincidence axiom of the PS-divergence. One can confirm that the scaled PS-divergence, -1D, converges to the extended KL-divergence defined on MxM, as   0. The PS-divergence is used to obtain a robust estimator [14].

As shown in (4), the standard PS-divergence from the empirical distribution p to the unnormalized

model q requires the computation of q+1, that may be infeasible in our setup. To circumvent

such an expensive computation, we employ a trick and substitute a model pq localized by the

empirical distribution for q, which makes it possible to replace the total sum in q+1 with the

empirical mean.

More precisely, let us consider the PS-divergence from f

=

(p

q1-

)

1 1+

to

g

=

(p

q1-

)

1 1+

for the probability distribution p



P

and the unnormalized model q



M,

where ,  are two distinct real numbers. Then, the divergence vanishes if and only if pq1- 

p q1- , i.e., q  p. We define the localized PS-divergence S,,(p, q) by

S,, (p, q) = D ((pq1-)1/(1+), (p q1- )1/(1+))

= 1 log pq1- +  logp q1-  - log pq1- ,

1+

1+

(5)

3

where  = ( X is replaced

+ )/(1 + ). with a variant of

tShuebesmtitpuitriincgalthmeeeamn psiurcichalasdistpribqu1t-ion

p into p, the( = xZ

ntnoxt)alsqu1m-o(vxe)r

for a non-zero real number . Since S,,(p, q) = S,,1/(p, q) holds, we can assume  > 

without loss of generality. In summary, the conditions of the real parameters , ,  are given by

 > 0,  > ,  = 0,  = 0,  +  = 0,

where the last condition denotes  = 0.

Let us consider another aspect of the computational issue about the localized PS-divergence. For the probability distribution p and the unnormalized exponential model q, we show that the localized PS-divergence S,,(p, q) is convex in , when the parameters ,  and  are properly chosen.
Theorem 1. Let p  P be any probability distribution, and let q be the unnormalized exponential model q(x) = exp(T (x)), where (x) is any vector-valued function corresponding to the sufficient statistic in the (normalized) exponential model q. For a given , the localized PS-divergence S,,(p, q) is convex in  for any , ,  satisfying  = ( + )/(1 + ) if and only if  = 1.

Proof.

After some calculation,

we have

 2 log pq1-
T

=

(1 - )2Vr, [],

where Vr, []

is the

covariance matrix of (x) under the probability r,(x). Thus, the Hessian matrix of S,,(p, q)

is written as

2 T

S,, (p, q)

=

(1 - )2 1 +  Vr, [] +

(1 1

- +

)2 

Vr ,

[]

-

(1

-

 )2 Vr,

[].

The Hessian matrix is non-negative definite if  = 1. The converse direction is deferred to the supplementary material.

Up to a constant factor, the localized PS-divergence with  = 1 characterized by Theorem 1 is

denotes as S, (p, q) that is defined by

S, (p, q)

=



1 -1

log pq1-

+

1

1 - 

logp q1- 

for  > 1 >  = 0. The parameter  can be negative if p is positive on X . Clearly, S, (p, q) satisfies the homogeneity and the weak coincidence axiom as well as S,,(p, q).

4 Estimation with the localized pseudo-spherical divergence

Given the empirical distribution p and the unnormalized model q, we define a novel estimator with the localized PS-divergence S,, (or S, ). Though the localized PS-divergence plugged-in the empirical distribution is not well-defined when  < 0, we can formally define the following estimator by restricting the domain X to the observed set of examples Z, even for negative :

 = argmin S,, (p, q)



=

argmin


1

1 +



log


xZ

( nx n

)

q (x)1-

+

1

 +



log


xZ

( nx n

)

q (x)1-

-

log



(

nx n

)

q (x)1- .

xZ

(6)

Remark 3. The summation in (6) is defined on Z and then is computable even when , ,  < 0. Also the summation includes only Z( n) terms and its computational cost is O(n).

Proposition 1. For the unnormalized model (2), the estimator (6) is Fisher consistent.

Proof. We observe

  S,, (q0 , q)

=0

=

( 

-

 +  1+

)

 q0

 0



=

0

implying the Fisher consistency of .

4

Theorem 2. Let q(x) be the unnormalized model (2), and 0 be the true parameter of underlying

distribution

p(x)

=

q0 (x).

Then an asymptotic distribution of the n( - 0)  N (0, I(0)-1)

estimator

(6)

is

written

as

where I(0) = Vq0 [ 0 ] is the Fisher information matrix.

Proof. We shall sketch a proof and the detailed proof is given in supplementary material. Let us assume that the empirical distribution is written as

p(x) = q0 (x) + (x).

Note that  = 0 because p, q0  P. The asymptotic expansion of the equilibrium condition for the estimator (6) around  = 0 leads to

 0 =  S,, (p, q) =

=

  S,, (p, q)

=0

+

2 T S,, (p, q)

( - 0) + O(|| - 0||2)
=0

By the delta method [15], we have

  S,, (p, q)

=0

-

  S,, (p, q)

=0



-

(1

 +

)2

(

-

)2

 0

 

and from the central limit theorem, we observe that

 n

 0

 

=

1 n n

n

( 0 (xi)

-

 q0

 0

)

i=1

asymptotically follows the normal distribution with mean 0, and variance I(0) = Vq0 [ 0 ], which is known as the Fisher information matrix. Also from the law of large numbers, we have

2 T

S,, (p, q)

=0

( -

0)



(1

 + )2 (

- )2I(0),

in the limit of n  . Consequently, we observe that (2).

Remark 4. The asymptotic distribution of (6) is equal to that of the MLE, and its variance does not depend on , , .
Remark 5. As shown in Remark 1, the normalized model (1) is a special case of the unnormalized model (2) and then Theorem 2 holds for the normalized model.

5 Characterization of localized pseudo-spherical divergence S,

Throughout this section, we assume that  = 1 holds and investigate properties of the localized PSdivergence S, . We discuss influence of selection of ,  and characterization of the localized PS-divergence S, in the following subsections.

5.1 Influence of selection of , 

We investigate influence of selection of ,  for the localized PS-divergence S, with a view of

the estimating equation. The estimator  derived from S, satisfies

S, (p, q) 








=

r,

-

r,

= 0.

(7)

which is a moment matching with respect to two distributions r, and r, (,  = 0, 1). On the

other hand, the estimating equation of the MLE is written as

L() 

=mle



p mle 

-

qmle mle 

=

 r1,mle


mle



-

 r0,mle


mle



= 0,

(8)

which is a moment matching with respect to the empirical distribution p = r1,mle and the
normalized model q = r0,mle . While the localized PS-divergence S, is not defined with (, ) = (0, 1), comparison of (7) with (8) implies that behavior the estimator  becomes similar to that of the MLE in the limit of   1 and   0.

5

5.2 Relationship with the -divergence

The -divergence between two positive measures f, g  M is defined as

D(f, g)

=

1 (1 - )

 f

+ (1

- )g

- f g1- ,

where  is a real number. Note that D(f, g)  0 and 0 if and only if f = g, and the -divergence reduces to KL(f, g) and KL(g, f ) in the limit of   1 and 0, respectively.

Remark 6. An estimator defined by minimizing -divergence D(p, q) between the empirical distribution and normalized model, satisfies

D(p, q) 



pq1-

(

-

q



 )

=

0

and requires calculation proportional to |X | which is infeasible. Also the same hold for an estimator

defined model,

sbaytimsfiyniinmgiziDng(p,-qdi)verge(n1ce-D)(qp,q

)-beptwqe1e-nthe=em0.pirical

distribution

and

unnormalized

Here, we assume that ,  = 0, 1 and consider a trick to cancel out the term g by mixing two

-divergences as follows.

( - )

D, (f, g) =D(f, g) +  D (f, g)

=

( 1

1 -



-

 (1 -

) )

f

-

1 (1 -

f g1- )

+

(1

1 -

 ) f  g1- .

Remark 7. D, (f, g)  0 is divergence when  < 0 holds, i.e., D, (f, g)  0 and D, (f, g) = 0 if and only if f = g. Without loss of generality, we assume  > 0 >  for D, .

Firstly, we consider an estimator defined by the minmizer of

min




{ 1

1 - 

( nx ) n

q (x)1-

-

1

1 -



( nx n

)

} q (x)1-

.

xZ

(9)

Note that the summation in (9) includes only Z( n) terms. We remark the following.

Remark 8. Let q0 (x) be the underlying distribution and q(x) be the unnormalized model (2).

Then an estimator defined by minimizing D, (q0 , q) is not in general Fisher consistent, i.e.,

D, (q0 , q) 

=0



 q0 q1-0   0

-

 q0 q1-0  0

=

( q0

-

-

q0

) -

 q0

 0



= 0.

This remark shows that an estimator associated with D, (p, q) does not have suitable properties such as (asymptotic) unbiasedness and consistency while required computational cost is drastically reduced. Intuitively, this is because the (mixture of) -divergence satisfies the coincidence axiom.

To overcome this drawback, we consider the following minimization problem for estimation of the

parameter  of model q(x).

(, r) = argmin D, (p, rq)
,r

where r is a constant corresponding to an inverse of the normalization term Z = q.
Proposition 2. Let q(x) be the unnormalized model (2). For  > 1 and 0 > , the minimization of D, (p, rq) is equivalent to the minimization of

S, (p, q).

Proof. For a given , we observe that

r

=

argmin D, (p, rq)
r

=

(

 p  p

qq11--

)

1 -

.

(10)

6

Note that computation of (10) requires only sample order O(n) calculation. By plugging (10) into D, (p, rq), we observe

 = argmin D, (p, rq) = argmin S, (p, q).


(11)

If  > 1 and  < 0 hold, the estimator (11) is equivalent to the estimator associated with the localized PS-divergence S, , implying that S, is characterized by the mixture of -divergences.
Remark 9. From a viewpoint of the information geometry [11], a metric (information geometrical structure) induced by the -divergence is the Fisher metric induced by the KL-divergence. This implies that the estimation based on the (mixture of) -divergence is Fisher efficient and is an intuitive explanation of the Theorem 2. The localized PS divergence S,, and S, with  > 0 can be interpreted as an extension of the -divergence, which preserves Fisher efficiency.

6 Experiments

We especially focus on a setting of  = 1, i.e., convexity of the risk function with the unnormalized model exp(T (x)) holds (Theorem 1) and examined performance of the proposed estimator.

6.1 Fully visible Boltzmann machine

In the first experiment, we compared the proposed estimator with parameter settings (, ) = (1.01, 0.01), (1.01, -0.01), (2, -1), with the MLE and the ratio matching method [8]. Note that the ratio matching method also does not require calculation of the normalization constant, and the proposed method with (, ) = (1.01, 0.01) may behave like the MLE as discussed in section 5.1.

All methods were optimized with the optim function in R language [16]. The dimension d of input

was set to 10 and the synthetic dataset was randomly generated from the second order Boltzmann machine (Example 2) with a parameter   N (0, I). We repeated comparison 50 times and

observed averaged performance. Figure 1 (a) shows median of the root mean square errors (RMSEs)

between  and  of each method over 50 trials, against the number n of examples. We observe that

the proposed estimator works well and is superior to the ratio matching method. In this experiment,

the MLE outperforms the proposed method contrary to the prediction of Theorem 2. This is because

observed patterns were only a small portion of all possible patterns, as shown in Figure 1 (b). Even

in such a case, the MLE can take all possible patterns (210 = 1024) into account through the

normalization

term

log Z



C onst

+

1 2

||||2

that

works

like

a

regularizer.

On

the

other

hand,

the

proposed method genuinely uses only the observed examples, and the asymptotic analysis would not

be relevant in this case. Figure 1 (c) shows median of computational time of each method against

n. The computational time of the MLE does not vary against n because the computational cost is

dominated by the calculation of the normalization constant. Both the proposed estimator and the

ratio matching method are significantly faster than the MLE, and the ratio matching method is faster

than the proposed estimator while the RMSE of the proposed estimator is less than that of the ratio

matching.

6.2 Boltzmann machine with hidden variables
In this subsection, we applied the proposed estimator for the Boltzmann machine with hidden variables whose associated function is written as (3). The proposed estimator with parameter settings (, ) = (1.01, 0.01), (1.01, -0.01), (2, -1) was compared with the MLE. The dimension d1 of observed variables was fixed to 10 and d2 of hidden variables was set to 2, and the parameter  was generated as   N (0, I) including parameters corresponding to hidden variables. Note that the Boltzmann machine with hidden variables is not identifiable and different values of the parameter do not necessarily generate different probability distributions, implying that estimators are influenced by local minimums. Then we measured performance of each estimator by the averaged
7

50 100 200 500

Number |Z| of unique patterns 100 150 200 250 300

0.5 1.0

MLE Ratio matching a1=1.01,a2=0.01 a1=1.01,a2=-0.01 a1=2,a2=-1

Time[s]

MLE Ratio matching a1=1.01,a2=0.01 a1=1.01,a2=-0.01 a1=2,a2=-1

RMSE

5 10 20

0.1 0.2

50

2

0

0

5000

10000

15000

20000

25000

n

100 200 400 800 1600 3200 6400 12800 25600 n

0

5000

10000

15000

20000

25000

n

Figure 1: (a) Median of RMSEs of each method against n, in log scale. (b) Box-whisker plot of number |Z| of unique patterns in the dataset D against n. (c) Median of computational time of each method against n, in log scale.

log-likelihood

1 n

n
i=1

log

q(xi

)

rather

than

the

RMSE.

An

initial

value

of

the

parameter

was

set

by N (0, I) and commonly used by all methods. We repeated the comparison 50 times and ob-

served the averaged performance. Figure 2 (a) shows median of averaged log-likelihoods of each

method over 50 trials, against the number n of example. We observe that the proposed estimator is

comparable with the MLE when the number n of examples becomes large. Note that the averaged

log-likelihood of MLE once decreases when n is samll, and this is due to overfitting of the model.

Figure 2 (b) shows median of averaged log-likelihoods of each method for test dataset consists of

10000 examples, over 50 trials. Figure 2 (c) shows median of computational time of each method

against n, and we observe that the proposed estimator is significantly faster than the MLE.

500 1000

Averaged Log likelihood -15 -10 -5

Averaged Log likelihood -15 -10 -5

MLE a1=1.01,a2=0.01 a1=1.01,a2=-0.01 a1=2,a2=-1

MLE a1=1.01,a2=0.01 a1=1.01,a2=-0.01 a1=2,a2=-1

Time[s] 50 100 200

MLE a1=1.01,a2=0.01 a1=1.01,a2=-0.01 a1=2,a2=-1

5 10 20

0

5000

10000

15000

20000

25000

n

0

5000

10000

15000

20000

25000

n

0

5000

10000

15000

20000

25000

n

Figure 2: (a) Median of averaged log-likelihoods of each method against n. (b) Median of averaged log-likelihoods of each method calculated for test dataset against n. (c) Median of computational time of each method against n, in log scale.

7 Conclusions
We proposed a novel estimator for probabilistic model on discrete space, based on the unnormalized model and the localized PS-divergence which has the homogeneous property. The proposed estimator can be constructed without calculation of the normalization constant and is asymptotically efficient, which is the most important virtue of the proposed estimator. Numerical experiments show that the proposed estimator is comparable to the MLE and required computational cost is drastically reduced.

8

References
[1] Hinton, G. E. & Sejnowski, T. J. (1986) Learning and relearning in boltzmann machines. MIT Press, Cambridge, Mass, 1:282-317.
[2] Ackley, D. H., Hinton, G. E. & Sejnowski, T. J. (1985) A learning algorithm for boltzmann machines. Cognitive Science, 9(1):147-169.
[3] Amari, S., Kurata, K. & Nagaoka, H. (1992) Information geometry of Boltzmann machines. In IEEE Transactions on Neural Networks, 3: 260-271.
[4] Hinton, G. E. & Salakhutdinov, R. R. (2012) A better way to pretrain deep boltzmann machines. In Advances in Neural Information Processing Systems, pp. 2447-2455 Cambridge, MA: MIT Press.
[5] Opper, M. & Saad, D. (2001) Advanced Mean Field Methods: Theory and Practice. MIT Press, Cambridge, MA.
[6] Hinton, G.E. (2002) Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation, 14(8):1771-1800.
[7] Hyvarinen, A. (2005) Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6:695-708.
[8] Hyvarinen, A. (2007) Some extensions of score matching. Computational statistics & data analysis, 51(5):2499-2512.
[9] Dawid, A. P., Lauritzen, S. & Parry, M. (2012) Proper local scoring rules on discrete sample spaces. The Annals of Statistics, 40(1):593-608.
[10] Gutmann, M. & Hirayama, H. (2012) Bregman divergence as general framework to estimate unnormalized statistical models. arXiv preprint arXiv:1202.3727.
[11] Amari, S & Nagaoka, H. (2000) Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs. Oxford University Press.
[12] Sejnowski, T. J. (1986) Higher-order boltzmann machines. In American Institute of Physics Conference Series, 151:398-403.
[13] Good, I. J. (1971) Comment on "measuring information and uncertainty," by R. J. Buehler. In Godambe, V. P. & Sprott, D. A. editors, Foundations of Statistical Inference, pp. 337-339, Toronto: Holt, Rinehart and Winston.
[14] Fujisawa, H. & Eguchi, S. (2008) Robust parameter estimation with a small bias against heavy contamination. Journal of Multivariate Analysis, 99(9):2053-2081.
[15] Van der Vaart, A. W. (1998) Asymptotic Statistics. Cambridge University Press. [16] R Core Team. (2013) R: A Language and Environment for Statistical Computing. R Foundation
for Statistical Computing, Vienna, Austria.
9

