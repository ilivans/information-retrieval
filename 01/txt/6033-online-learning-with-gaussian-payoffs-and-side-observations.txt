Online Learning with Gaussian Payoffs and Side Observations

Yifan Wu1

Andras Gyorgy2

Csaba Szepesvari1

1Dept. of Computing Science
University of Alberta {ywu12,szepesva}@ualberta.ca

2Dept. of Electrical and Electronic Engineering Imperial College London
a.gyorgy@imperial.ac.uk

Abstract
We consider a sequential learning problem with Gaussian payoffs and side observations: after selecting an action i, the learner receives information about the payoff of every action j in the form of Gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair (i, j) (and may be infinite). The setup allows a more refined information transfer from one action to another than previous partial monitoring setups, including the recently introduced graph-structured feedback case. For the first time in the literature, we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem-dependent lower bounds and finitetime minimax lower bounds available in the literature. We also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to logarithmic factors).
1 Introduction
Online learning in stochastic environments is a sequential decision problem where in each time step a learner chooses an action from a given finite set, observes some random feedback and receives a random payoff. Several feedback models have been considered in the literature: The simplest is the full information case where the learner observes the payoff of all possible actions at the end of every round. A popular setup is the case of bandit feedback, where the learner only observes its own payoff and receives no information about the payoff of other actions [1]. Recently, several papers considered a more refined setup, called graph-structured feedback, that interpolates between the full-information and the bandit case: here the feedback structure is described by a (possibly directed) graph, and choosing an action reveals the payoff of all actions that are connected to the selected one, including the chosen action itself. This problem, motivated for example by social networks, has been studied extensively in both the adversarial [2, 3, 4, 5] and the stochastic cases [6, 7]. However, most algorithms presented heavily depend on the self-observability assumption, that is, that the payoff of the selected action can be observed. Removing this self-loop assumption leads to the so-called partial monitoring case [5]. In the absolutely general partial monitoring setup the learner receives some general feedback that depends on its choice (and the environment), with some arbitrary (but known) dependence [8, 9]. While the partial monitoring setup covers all other problems, its analysis has concentrated on the finite case where both the set of actions and the set of feedback signals are finite [8, 9], which is in contrast to the standard full information and bandit settings where the feedback is typically assumed to be real-valued. To our knowledge there are only a few exceptions to this case: in [5], graph-structured feedback is considered without the self-loop assumption, while continuous action spaces are considered in [10] and [11] with special feedback structure (linear and censored observations, resp.).
In this paper we consider a generalization of the graph-structured feedback model that can also be viewed as a general partial monitoring model with real-valued feedback. We assume that selecting
1

an action i the learner can observe a random variable Xij for each action j whose mean is the same as the payoff of j, but its variance i2j depends on the pair (i, j). For simplicity, throughout the paper we assume that all the payoffs and the Xij are Gaussian. While in the graph-structured feedback case one either has observation on an action or not, but the observation always gives the same amount of information, our model is more refined: Depending on the value of i2j, the information can be of different quality. For example, if i2j = , trying action i gives no information about action j. In general, for any i2j < , the value of the information depends on the time horizon T of the problem: when i2j is large relative to T (and the payoff differences of the actions) essentially no information is received, while a small variance results in useful observations.
After defining the problem formally in Section 2, we provide non-asymptotic problem-dependent lower bounds in Section 3, which depend on the distribution of the observations through their mean payoffs and variances. To our knowledge, these are the first such bounds presented for any stochastic partial monitoring problem beyond the full-information setting: previous work either presented asymptotic problem-dependent lower bounds (e.g., [12, 7]), or finite-time minimax bounds (e.g., [9, 3, 5]). Our bounds can recover all previous bounds up to some universal constant factors not depending on the problem. In Section 4, we present two algorithms with finite-time performance guarantees for the case of graph-structured feedback without the self-observability assumption. While due to their complicated forms it is hard to compare our finite-time upper and lower bounds, we show that our first algorithm achieves the asymptotic problem-dependent lower bound up to
problem-independent multiplicative factors. Regarding the minimax regret, the hardness ((T 1/2)
or (T 2/3) regret1) of partial monitoring problems is characterized by their global/local observability property [9] or, in case of the graph-structured feedback model, by their strong/weak observability property [5]. In the same section we present another algorithm that achieves the minimax regret (up to logarithmic factors) under both strong and weak observability, and achieves an O(log3/2 T ) problem-dependent regret. Earlier results for the stochastic graph-structured feedback problems [6, 7] provided only asymptotic problem-dependent lower bounds and performance bounds that did not match the asymptotic lower bounds or the minimax rate up to constant factors. A related combinatorial partial monitoring problem with linear feedback was considered in [10], where the presented
algorithm was shown to satisfy both an O(T 2/3) minimax bound and a logarithmic problem dependent bound. However, the dependence on the problem structure in that paper is not optimal, and, in particular, the paper does not achieve the O( T ) minimax bound for easy problems. Finally, we draw conclusions and consider some interesting future directions in Section 5. Proofs can be found in the long version of this paper [13].

2 Problem Formulation

Formally, we consider an online learning problem with Gaussian payoffs and side observations:
Suppose a learner has to choose from K actions in every round. When choosing an action, the
learner receives a random payoff and also some side observations corresponding to other actions. More precisely, each action i  [K] = {1, . . . , K} is associated with some parameter i, and the payoff Yt,i to action i in round t is normally distributed random variable with mean i and variance i2i, while the learner observes a K-dimensional Gaussian random vector Xt,i whose jth coordinate is a normal random variable with mean j and variance i2j (we assume 0  ij  ) and the coordinates of Xt,i are independent of each other. We assume the following: (i) the random variables (Xt, Yt)t are independent for all t; (ii) the parameter vector  is unknown to the learner but the variance matrix  = (i2j)i,j[K] is known in advance; (iii)   [0, D]K for some D > 0; (iv) mini[K] ij   <  for all j  [K], that is, the expected payoff of each action can be observed.

The goal of the learner is to maximize its payoff or, in other words, minimize the expected regret

T

RT = T max i -
i[K ]

E [Yt,it ]

t=1

where it is the action selected by the learner in round t. Note that the problem encompasses several common feedback models considered in online learning (modulo the Gaussian assumption), and

makes it possible to examine more delicate observation structures:

1Tilde denotes order up to logarithmic factors.

2

Full information: ij = j <  for all i, j  [K].
Bandit: ii <  and ij =  for all i = j  [K].
Partial monitoring with feedback graphs [5]: Each action i  [K] is associated with an observation set Si  [K] such that ij = j <  if j  Si and ij =  otherwise.
We will call the uniform variance version of these problems when all the finite ij are equal to some   0. Some interesting features of the problem can be seen when considering the generalized full information case , when all entries of  are finite. In this case, the greedy algorithm, which estimates the payoff of each action by the average of the corresponding observed samples and selects the one with the highest average, achieves at most a constant regret for any time horizon T .2 On the other hand, the constant can be quite large: in particular, when the variance of some observations are large relative to the gaps dj = maxi i - j, the situation is rather similar to a partial monitoring setup for a smaller, finite time horizon. In this paper we are going to analyze this problem and present algorithms and lower bounds that are able to "interpolate" between these cases and capture the characteristics of the different regimes.
2.1 Notation
Define CTN = {c  NK : ci  0 , i[K] ci = T } and let N (T )  CTN denote the number of plays over all actions taken by some algorithm in T rounds. Also let CTR = {c  RK : ci  0 , i[K] ci = T }. We will consider environments with different expected payoff vectors   , but the variance matrix  will be fixed. Therefore, an environment can be specified by ; oftentimes, we will explicitly denote the dependence of different quantities on : The probability and expectation functionals under environment  will be denoted by Pr (*; ) and E [*; ], respectively. Furthermore, let ij() be the jth best action (ties are broken arbitrarily, i.e., i1  i2  * * *  iK ) and define di() = i1() - i for any i  [K]. Then the expected regret under environment  is RT () =
i[K] E [Ni(T ); ] di(). For any action i  [K], let Si = {j  [K] : ij < } denote the set of actions whose parameter j is observable by choosing action i. Throughout the paper, log denotes the natural logarithm and n denotes the n-dimensional simplex for any positive integer n.

3 Lower Bounds

The aim of this section is to derive generic, problem-dependent lower bounds to the regret, which are also able to provide minimax lower bounds. The hardness in deriving such bounds is that for any fixed  and , the dumb algorithm that always selects i1() achieves zero regret (obviously, the regret of this algorithm is linear for any  with i1() = i1( )), so in general it is not possible to give a lower bound for a single instance. When deriving asymptotic lower bounds, this is circumvented by only considering consistent algorithms whose regret is sub-polynomial for any problem [12]. However, this asymptotic notion of consistency is not applicable to finite-horizon problems. Therefore, following ideas of [14], for any problem we create a family of related problems (by perturbing the mean payoffs) such that if the regret of an algorithm is "too small" in one of the problems than it will be "large" in another one, while it still depends on the original problem parameters (note that deriving minimax bounds usually only involves perturbing certain special "worst-case" problems).

As a warm-up, and to show the reader what form of a lower bound can be expected, first we present
an asymptotic lower bound for the uniform-variance version of the problem of partial monitoring
with feedback graphs. The result presented below is an easy consequence of [12], hence its proof is omitted. An algorithm is said to be consistent if sup RT () = o(T ) for every  > 0. Now assume for simplicity that there is a unique optimal action in environment , that is, i1() > i for all i = i1 and let



 C = c  [0, )K


22

:

ci
i:jSi



d2j ()

22 

for

all

j

=

i1() ,

ci

i:i1 ()Si



d2i2()() 

.

2To see this, notice that the error of identifying the optimal action decays exponentially with the number of rounds.

3

Then, for any consistent algorithm and for any  with i1() > i2(), lim inf RT ()  inf c, d() . T  log T cC

(1)

Note that the right hand side of (1) is 0 for any generalized full information problem (recall that the expected regret is bounded by a constant for such problems), but it is a finite positive number for other problems. Similar bounds have been provided in [6, 7] for graph-structured feedback with self-observability (under non-Gaussian assumptions on the payoffs). In the following we derive finite time lower bounds that are also able to replicate this result.

3.1 A General Finite Time Lower Bound First we derive a general lower bound. For any ,    and q  |CTN |, define f (, q,  ) as

f (, q,  ) = inf

q (a) a, d( )

q |CTN | aCTN



q(a)

such that

q(a) log



q (a)

Ii(,  )

q(a)ai ,

aCTN

i[K]

aCTN

where Ii(,  ) is the KL-divergence between Xt,i() and Xt,i( ), given by Ii(,  ) = KL(Xt,i(); Xt,i( )) = Kj=1(j - j)2/2i2j. Clearly, f (, q,  ) is a lower bound on RT ( ) for any algorithm for which the distribution of N (T ) is q. The intuition behind the allowed values
of q is that we want q to be as similar to q as the environments  and  look like for the algorithm
(through the feedback (Xt,it )t). Now define

g(, c) = inf sup f (, q,  ),
q|CTN |  

such that

q(a)a = c  CTR.

aCTN

g(, c) is a lower bound of the worst-case regret of any algorithm with E [N (T ); ] = c. Finally, for any x > 0, define

b(, x) = inf c, d()
cC,x

where C,x = {c  CTR ; g(, c)  x}.

Here C,B contains all the possible values of E [N (T ); ] that can be achieved by some algorithm whose lower bound g on the worst-case regret is smaller than x. These definitions give rise to the following theorem:
Theorem 1. Given any B > 0, for any algorithm such that sup  RT ( )  B, we have, for any environment   , RT ()  b(, B).
Remark 2. If B is picked as the minimax value of the problem given the observation structure , the theorem states that for any minimax optimal algorithm the expected regret for a certain  is lower bounded by b(, B).

3.2 A Relaxed Lower Bound
Now we introduce a relaxed but more interpretable version of the finite-time lower bound of Theorem 1, which can be shown to match the asymptotic lower bound (1). The idea of deriving the lower bound is the following: instead of ensuring that the algorithm performs well in the most adversarial environment  , we consider a set of "bad" environments and make sure that the algorithm performs well on them, where each "bad" environment  is the most adversarial one by only perturbing one coordinate i of .
However, in order to get meaningful finite-time lower bounds, we need to perturb  more carefully than in the case of asymptotic lower bounds. The reason for this is that for any sub-optimal action i, if i is very close to i1(), then E [Ni(T ); ] is not necessarily small for a good algorithm for . If it is small, one can increase i to obtain an environment  where i is the best action and the algorithm performs bad; otherwise, when E [Ni(T ); ] is large, we need to decrease i to make the

4

algorithm perform badly in  . Moreover, when perturbing i to be better than i1(), we cannot make i - i1() arbitrarily small as in asymptotic lower-bound arguments, because when i - i1()
is small, large E Ni1();  , and not necessarily large E [Ni(T );  ], may also lead to low finite-time regret in  . In the following we make this argument precise to obtain an interpretable lower bound.

3.2.1 Formulation

We start with defining a subset of CTR that contains the set of "reasonable" values for E [N (T ); ]. For any    and B > 0, let



C,B

 = c  CTR


:

K cj j=1 j2i

 mi(, B)

 for all i  [K]


where mi, the minimum sample size required to distinguish between i and its worst-case perturbation, is defined as follows: For i = i1, if i1 = D,3 then mi(, B) = 0. Otherwise let

mi,+(, B) =

max
(di (),D-i ]

1
2

log

T(

-di 8B

())

,

mi,-(, B) =

max
(0,i ]

1
2

log

T(

+di 8B

())

,

and let i,+ and i,- denote the value of achieving the maximum in mi,+ and mi,-, respectively.

Then, define

mi(, B) =

mi,+(, B) min {mi,+(, B), mi,-(, B)}

if di()  4B/T ; if di() < 4B/T .

For i = i1, then mi1 (, B) = 0 if i2() = 0, else the definitions for i = i1 change by replacing

di() with di2()() (and switching the + and - indices):

mi1(),-(, B) =

max
(di2 () (),i1 () ]

1
2

log T (

,-di2 () ())
8B

mi1(),+(, B) =

max
(0,D-i1 () ]

1
2

log T (

+di2 () ()) 8B

where i1(),- and i1(),+ are the maximizers for in the above expressions. Then, define

mi1()(, B) =

mi1(),-(, B) min mi1(),+(, B), mi1(),-(, B)

if di2()()  4B/T ; if di2()() < 4B/T .

Note that i,+ and i,- can be expressed in closed form using the Lambert W : R  R function satisfying W (x)eW (x) = x: for any i = i1(),

i,+ = min

W D - i , 8 eBe

di ()T 16 eB

/T + di()

,

i,- = min

W i , 8 eBe

-

di ()T 16 eB

/T - di()

,

(2)

and similar results hold for i = i1, as well. Now we can give the main result of this section, a simplified version of Theorem 1: Corollary 3. Given B > 0, for any algorithm such that sup RT ()  B, we have, for any environment   , RT ()  b (, B) = mincC,B c, d() .
Next we compare this bound to existing lower bounds.

3.2.2 Comparison to the Asymptotic Lower Bound (1)
Now we will show that our finite time lower bound in Corollary 3 matches the asymptotic lower bound in (1) up to some constants. Pick B = T  for some  > 0 and 0 <  < 1. For simplicity, we only consider  which is "away from" the boundary of  (so that the minima in (2) are
3Recall that i  [0, D].

5

achieved by the second terms) and has a unique optimal action. Then, for i = i1(), it is easy

to show that

i,+ = 2W (di()Td1i-()/(16e)) + di() by (2) and mi(, B) =

1
2 i,+

log

T(

i,+ -di ()) 8B

for large enough T . Then, using the fact that log x - log log x  W (x)  log x for x  e,

it follows that limT  mi(, B)/ log T = (1 - )/d2i (), and similarly we can show that

limT  mi1()(, B)/ log T

=

(1 - )/d2i2()().

Thus, C,B



(1-) 2

log

T

C

,

under

the

as-

sumptions of (1), as T  . This implies that Corollary 3 matches the asymptotic lower bound of

(1) up to a factor of (1 - )/2.

3.2.3 Comparison to Minimax Bounds

Now we will show that our -dependent finite-time lower bound reproduces the minimax regret bounds of [2] and [5], except for the generalized full information case.

The minimax bounds depend on the following notion of observability: An action i is strongly ob-
servable if either i  Si or [K] \ {i}  {j : i  Sj}. i is weakly observable if it is not strongly observable but there exists j such that i  Sj (note that we already assumed the latter condition for all i). Let W() be the set of all weakly observable actions.  is said to be strongly observable if
W() = .  is weakly observable if W() = .

Next we will define two key qualities introduced by [2] and [5] that characterize the hardness of a problem instance with feedback structure : A set A  [K] is called an independent set if for any i  A, Si  A  {i}. The independence number () is defined as the cardinality of the largest independent set. For any pair of subsets A, A  [K], A is said to be dominating A if for any j  A there exists i  A such that j  Si. The weak domination number () is defined as the cardinality of the smallest set that dominates W().
Corollary 4. Assume that ij =  for some i, j  [K], that is, we are not in the generalized full information case. Then,

(i) if  is strongly observable, with B =  ()T for some  > 0, we have

sup b

(, B)





()T 64e

for T

 64e222()3/D2.

(ii) If  is weakly observable, with B = (()D)1/3(T )2/3 log-2/3 K for some  > 0, we

have sup b

(, B)



.(()D)1/3(T )2/3 log-2/3 K
51200e2 2

Remark 5.

In Corollary 4, picking 

=

1 8e

for strongly observable  and 

=

1 73

for weakly

observable  gives formal minimax lower bounds: (i) If  is strongly observable, for any algorithm

we have sup RT () 



()T 8e

for T

 e2()3/D2. (ii) If  is weakly observable, for any

algorithm

we

have

sup RT ()



.(()D)1/3(T )2/3
73 log2/3 K

4 Algorithms

In this section we present two algorithms and their finite-time analysis for the uniform variance version of our problem (where ij is either  or ). The upper bound for the first algorithm matches the asymptotic lower bound in (1) up to constants. The second algorithm achieves the minimax lower bounds of Corollary 4 up to logarithmic factors, as well as O(log3/2 T ) problem-dependent regret.
In the problem-dependent upper bounds of both algorithms, we assume that the optimal action is unique, that is, di2()() > 0.

4.1 An Asymptotically Optimal Algorithm

Let c() = argmincC c, d() ; note that increasing ci1()() does not change the value of

c, d() (since di1()() = 0), so we take the minimum value of ci1()() in this definition. Let

ni(t) =

t-1 s=1

I

{i



Sis }

be

the

number

of

observations

for

action

i

before

round

t

and

t,i

be

the

empirical estimate of i based on the first ni(t) observations. Let Ni(t) =

t-1 s=1

I

{is

=

i}

be

the

number of plays for action i before round t. Note that this definition of Ni(t) is different from that

in the previous sections since it excludes round t.

6

Algorithm 1

1: Inputs: , ,  : N  [0, ).

2: For t = 1, ..., K, observe each action i at least

once by playing it such that t  Sit . 3: Set exploration count ne(K + 1) = 0.

4: for t = K + 1, K + 2, ... do

5:

if

N (t) 4 log t



Ct

then

6: Play it = i1(t).

7: Set ne(t + 1) = ne(t).

8: else

9: if mini[K] ni(t) < (ne(t))/K then

10: Play it such that argmini[K] ni(t)  Sit . 11: else 12: Play it such that Ni(t) < ci(t)4 log t.

13: end if

14: Set ne(t + 1) = ne(t) + 1.

15: end if

16: end for

Our first algorithm is presented in Algorithm 1. The main idea, coming from [15], is that by forcing exploration over all actions, the solution c() of the linear program can be well approximated while paying a constant price. This solves the main difficulty that, without getting enough observations on each action, we may not have good enough estimates for d() and c(). One advantage of our algorithm compared to that of [15] is that we use a nondecreasing, sublinear exploration schedule (n) ( : N  [0, )) instead of a constant rate (n) = n. This resolves the problem that, to achieve asymptotically optimal performance, some parameter of the algorithm needs to be chosen according to dmin() as in [15]. The expected regret of Algorithm 1 is upper bounded as follows:

Theorem 6. For any   , > 0,  > 2 and any non-decreasing (n) that satisfies 0  (n) 

n/2 and (m + n)  (m) + (n) for m, n  N,

T
RT ()  2K + 2 + 4K/( - 2) dmax() + 4Kdmax() exp

-

(s) 2 2K 2

s=0

+ 2dmax() 4 log T

ci(, ) + K + 4 log T

ci(, )di() .

i[K ]

i[K ]

where ci(, ) = sup{ci( ) : |j - j|  for all j  [K]}.

Further specifying (n) and using the continuity of c() around , it immediately follows that Algorithm 1 achieves asymptotically optimal performance:

Corollary 7. Suppose the conditions of Theorem 6 hold. Assume, furthermore, that (n) satisfies

(n) = o(n) and

 s=0

exp

-

(s) 2 2K  2

<  for any

> 0, then for any  such that c() is unique,

lim sup RT ()/ log T  4 inf c, d() .

T 

cC ()

Note

that

any

(n)

=

anb

with

a



(0,

1 2

],

b



(0, 1)

satisfies

the

requirements

in

Theorem

6

and

Corollary 7. Also note that the algorithms presented in [6, 7] do not achieve this asymptotic bound.

4.2 A Minimax Optimal Algorithm

Next we present an algorithm achieving the minimax bounds. For any A, A  [K], let

c(A, A ) = argmaxc|A| miniA j:iSj cj (ties are broken arbitrarily) and m(A, A ) = miniA j:iSj cj(A, A ). For any A  [K] and |A|  2, let AS = {i  A : j  A, i  Sj}

and AW = A - AS . Furthermore, let gr,i() = 

2 log(8K2r3/) ni (r)

where

ni(r)

=

r-1 s=1

is,i

and

r,i

be the empirical estimate of i based on first ni(r) observations (i.e., the average of the samples).

The algorithm is presented in Algorithm 2. It follows a successive elimination process: it explores all possibly optimal actions (called "good actions" later) based on some confidence intervals until only one action remains. While doing exploration, the algorithm first tries to explore the good actions by only using good ones. However, due to weak observability, some good actions might have to be explored by actions that have already been eliminated. To control this exploration-exploitation trade off, we use a sublinear function  to control the exploration of weakly observable actions.

In the following we present high-probability bounds on the performance of the algorithm, so, with a slight abuse of notation, RT () will denote the regret without expectation in the rest of this section.

7

Algorithm 2

1: Inputs: , .

2: Set t1 = 0, A1 = [K].

3: for r = 1, 2, ... do

4: Let r = min1sr,AWs = m([K] , AWs ) and (r) = (rtr/D)2/3. (Define r = 1 if

5:

AWs =  if AWr =

for all  and

1  s  r.) miniAWr ni(r)

<

miniASr

ni(r)

and

miniAWr

ni(r)

<

(r)

then

6: Set cr = c([K] , AWr ).

7: else

8: Set cr = c(Ar, ASr ).

9: end if

10: Play ir = cr * cr 0 and set tr+1  tr + ir 1. 11: Ar+1  {i  Ar : r+1,i + gr+1,i()  maxjAr r+1,j - gr+1,j ()}. 12: if |Ar+1| = 1 then

13: Play the only action in the remaining rounds.

14: end if

15: end for

Theorem 8. For any   (0, 1) and any   ,

RT ()  (()D)1/3(T )2/3 * 7 6 log(2KT /) + 1252K3/D + 13K3D with probability at least 1 -  if  is weakly observable, while

RT ()  2KD + 80

()T * 6 log K log 2KT 

with probability at least 1 -  if  is strongly observable.

Theorem 9 (Problem-dependent upper bound). For any   (0, 1) and any    such that the optimal action is unique, with probability at least 1 - ,

RT ()



1603()D2 d2min()

(log(2KT /))3/2

+ 14K3D

+ 1252K3/D

+ 15 ()D2 1/3 1252/D2 + 10 K2 (log(2KT /))1/2 .

Remark 10. Picking  = 1/T gives an O(log3/2 T ) upper bound on the expected regret.
Remark 11. Note that Algortihm 2 is similar to the UCB-LP algorithm of [7], which admits a better problem-dependent upper bound (although does not achieve it with optimal problem-dependent constants), but it does not achieve the minimax bound even under strong observability.

5 Conclusions and Open Problems
We considered a novel partial-monitoring setup with Gaussian side observations, which generalizes the recently introduced setting of graph-structured feedback, allowing finer quantification of the observed information from one action to another. We provided non-asymptotic problem-dependent lower bounds that imply existing asymptotic problem-dependent and non-asymptotic minimax lower bounds (up to some constant factors) beyond the full information case. We also provided an algorithm that achieves the asymptotic problem-dependent lower bound (up to some universal constants) and another algorithm that achieves the minimax bounds under both weak and strong observability.
However, we think this is just the beginning. For example, we currently have no algorithm that achieves both the problem dependent and the minimax lower bounds at the same time. Also, our upper bounds only correspond to the graph-structured feedback case. It is of great interest to go beyond the weak/strong observability in characterizing the hardness of the problem, and provide algorithms that can adapt to any correspondence between the mean payoffs and the variances (the hardness is that one needs to identify suboptimal actions with good information/cost trade-off).

Acknowledgments This work was supported by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning (AICML) and NSERC. During this work, A. Gyorgy was with the Department of Computing Science, University of Alberta.

8

References
[1] Sebatien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1-122, 2012.
[2] Shie Mannor and Ohad Shamir. From bandits to experts: on the value of side-observations. In Advances in Neural Information Processing Systems 24 (NIPS), pages 684-692, 2011.
[3] Noga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. From bandits to experts: A tale of domination and independence. In Advances in Neural Information Processing Systems 26 (NIPS), pages 1610-1618, 2013.
[4] Tomas Kocak, Gergely Neu, Michal Valko, and Remi Munos. Efficient learning by implicit exploration in bandit problems with side observations. In Advances in Neural Information Processing Systems 27 (NIPS), pages 613-621, 2014.
[5] Noga Alon, Nicolo Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: beyond bandits. In Proceedings of The 28th Conference on Learning Theory (COLT), pages 23-35, 2015.
[6] Stephane Caron, Branislav Kveton, Marc Lelarge, and Smriti Bhagat. Leveraging side observations in stochastic bandits. In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI), pages 142-151, 2012.
[7] Swapna Buccapatnam, Atilla Eryilmaz, and Ness B. Shroff. Stochastic bandits with side observations on networks. SIGMETRICS Perform. Eval. Rev., 42(1):289-300, June 2014.
[8] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, Cambridge, 2006.
[9] Gabor Bartok, Dean P. Foster, David Pal, Alexander Rakhlin, and Csaba Szepesvari. Partial monitoring - classification, regret bounds, and algorithms. Mathematics of Operations Research, 39:967-997, 2014.
[10] Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen. Combinatorial partial monitoring game with linear feedback and its applications. In Proceedings of the 31st International Conference on Machine Learning (ICML), pages 901-909, 2014.
[11] Tor Lattimore, Andras Gyorgy, and Csaba Szepesvari. On learning the optimal waiting time. In Peter Auer, Alexander Clark, Thomas Zeugmann, and Sandra Zilles, editors, Algorithmic Learning Theory, volume 8776 of Lecture Notes in Computer Science, pages 200-214. Springer International Publishing, 2014.
[12] Todd L. Graves and Tze Leung Lai. Asymptotically efficient adaptive choice of control laws incontrolled markov chains. SIAM Journal on Control and Optimization, 35(3):715-743, 1997.
[13] Yifan Wu, Andras Gyorgy, and Csaba Szepesvari. Online learning with Gaussian payoffs and side observations. arXiv preprint arXiv:1510.08108, 2015.
[14] Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (AISTATS), pages 608-616, 2015.
[15] Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bounds and optimal algorithms. In Proceedings of The 27th Conference on Learning Theory (COLT), pages 975-999, 2014.
[16] Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On the complexity of best arm identification in multi-armed bandit models. The Journal of Machine Learning Research, 2015. (to appear).
[17] Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In Proceedings of the 31st International Conference on Machine Learning (ICML), pages 521-529, 2014.
9

