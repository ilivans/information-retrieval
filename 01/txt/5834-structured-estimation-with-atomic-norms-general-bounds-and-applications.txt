Structured Estimation with Atomic Norms: General Bounds and Applications

Sheng Chen

Arindam Banerjee

Dept. of Computer Science & Engg., University of Minnesota, Twin Cities

{shengc,banerjee}@cs.umn.edu

Abstract
For structured estimation problems with atomic norms, recent advances in the literature express sample complexity and estimation error bounds in terms of certain geometric measures, in particular Gaussian width of the unit norm ball, Gaussian width of a spherical cap induced by a tangent cone, and a restricted norm compatibility constant. However, given an atomic norm, bounding these geometric measures can be difficult. In this paper, we present general upper bounds for such geometric measures, which only require simple information of the atomic norm under consideration, and we establish tightness of these bounds by providing the corresponding lower bounds. We show applications of our analysis to certain atomic norms, especially k-support norm, for which existing result is incomplete.

1 Introduction

Accurate recovery of structured sparse signal/parameter vectors from noisy linear measurements has been extensively studied in the field of compressed sensing, statistics, etc. The goal is to recover a high-dimensional signal (parameter)   Rp which is sparse (only has a few nonzero entries), possibly with additional structure such as group sparsity. Typically one assume linear models, y = X + , in which X  Rnxp is the design matrix consisting of n samples, y  Rn is the observed response vector, and   Rn is an unknown noise vector. By leveraging the sparsity of , previous work has shown that certain L1-norm based estimators [22, 7, 8] can find a good approximation of  using sample size n p. Recent work has extended the notion of unstructured sparsity to other structures in  which can be captured or approximated by some norm R(*) [10, 18, 3, 11, 6, 19] other than L1, e.g., (non)overlapping group sparsity with L1/L2 norm [24, 15], etc. In general, two broad classes of estimators are considered in recovery analysis: (i) Lasso-type estimators [22, 18, 3], which solve the regularized optimization problem

n

=

argmin
Rp

1 2n

X - y

2 2

+

nR()

,

(1)

and (ii) Dantzig-type estimators [7, 11, 6], which solve the constrained problem

n = argmin R() s.t. R(XT (X - y))  n ,
Rp

(2)

where R(*) is the dual norm of R(*). Variants of these estimators exist [10, 19, 23], but the recovery analysis proceeds along similar lines as these two classes of estimators.

To establish recovery guarantees, [18] focused on Lasso-type estimators and R(*) from the class of
decomposable norm, e.g., L1, non-overlapping L1/L2 norm. The upper bound for the estimation e(ir)roardual nno-rmbo2ufnodr ,aansyadnecuopmpeprobsoaubnlednfoorrmRis(cXhTarac)t,e(riiiz)esdaminptleermcosmopf ltehxreitey,gtehoemmetirniicmmaleasasmurpelse:
size needed for a certain restricted eigenvalue (RE) condition to be true [4, 18], and (iii) a restricted

1

norm error

compatibility constant between R(*) bound typically has the form n -

and||L2 2nocr/msn[,1w8,h3er]e.

The non-asymptotic estimation c depends on a product of dual

norm bound and restricted norm compatibility, whereas the sample complexity characterizes the

minimum number of samples after which the error bound starts to be valid. In recent work, [3]

extended the analysis of Lasso-type estimator for decomposable norm to any norm, and gave a more succinct characterization of the dual norm bound for R(XT ) and the sample complexity for the

RE condition in terms of Gaussian widths [14, 10, 20, 1] of suitable sets where, for any set A  Rp,

the Gaussian width is defined as

w(A) = E sup u, g ,
uA

(3)

where g is a standard Gaussian random vector. For Dantzig-type estimators, [11, 6] obtained similar

extensions. To be specific, assume entries in X and  are i.i.d. normal, and define the tangent cone,

Then

one

can

get

TR() = cone {u  Rp | R( + (high-probability) upper bound for R(XT

u) )

 as

R()} . O( nw(R))

where

R

=

(4) {u 

Rp|R(u)  1} is the unit norm ball, and the RE condition is satisfied with O(w2(TR()  Sp-1))

samples, in which Sp-1 is the unit sphere. For convenience, we denote by CR() the spherical

cap TR()  Sp-1 throughout the paper. Further, the restricted norm compatibility is given by

R()

=

supuTR()

R(u) u2

(see

Section

2

for

details).

Thus, for any given norm, it suffices to get a characterization of (i) w(R), the width of the unit norm ball, (ii) w(CR()), the width of the spherical cap induced by the tangent cone TR(), and (iii) R(), the restricted norm compatibility in the tangent cone. For the special case of L1 norm, accurate characterization of all three measures exist [10, 18]. However, for more general
norms, the literature is rather limited. For w(R), the characterization is often reduced to comparison with either w(CR()) [3] or known results on other norm balls [13]. While w(CR()) has been investigated for certain decomposable norms [10, 9, 1], little is known about general nondecomposable norms. One general approach for upper bounding w(CR()) is via the statistical dimension [10, 19, 1], which computes the expected squared distance between a Gaussian random vector and the polar cone of TR(). To specify the polar, one need full information of the subdifferential R(), which could be difficult to obtain for non-decomposable norms. A notable
bound for (overlapping) L1/L2 norms is presented in [21], which yields tight bounds for mildly non-overlapping cases, but is loose for highly overlapping ones. For R(), the restricted norm compatibility, results are only available for decomposable norms [18, 3].

In this paper, we present a general set of bounds for the width w(R) of the norm ball, the width w(CR()) of the spherical cap, and the restricted norm compatibility R(). For the analysis, we consider the class of atomic norms that are invariant under sign-changes, i.e., the norm of a
vector stays unchanged if any entry changes only by flipping its sign. The class is quite general, and covers most of the popular norms used in practical applications, e.g., L1 norm, ordered weighted L1 (OWL) norm [5] and k-support norm [2]. Specifically we show that sharp bounds on w(R) can be obtained using simple calculation based on a decomposition inequality from [16]. To upper bound w(CR()) and R(), instead of a full specification of TR(), we only require some information regarding the subgradient of R(), which is often readily accessible. The key insight
is that bounding statistical dimension often ends up computing the expected distance from Gaussian vector to a single point rather than to the whole polar cone, thus the full information on R() is
unnecessary. In addition, we derive the corresponding lower bounds to show the tightness of our results. As examples, we illustrate the bounds for L1 and OWL norms [5]. Finally, we give sharp bounds for the recently proposed k-support norm [2], for which existing analysis is incomplete.

The rest of the paper is organized as follows: we first review the relevant background for Dantzigtype estimator and atomic norm in Section 2. In Section 3, we introduce the general bounds for the geometric measures. In Section 4, we discuss the tightness of our bounds. Section 5 is dedicated to the example of k-support norm, and we conclude in Section 6.

2 Background
In this section, we briefly review the recovery guarantee for the generalized Dantzig selector in (2) and the basics on atomic norms. The following lemma, originally [11, Theorem 1], provides an error bound for n -  2. Related results have appeared for other estimators [18, 10, 19, 3, 23].

2

Lemma 1 Assume that y = Gaussian random variable. If

X + , where entries of X n  c1 nw(R) and n > c2

and  are i.i.d. copies of w2(TR()  Sp-1) = w2

standard (CR())

for some constant c1, c2 > 1, with high probability, the estimate n given by (2) satisfies

n -  2  O

R()

w(R n

)

.

(5)

In this Lemma, there are three geometric measures--w(R), w(CR()) and R()--which need to be determined for specific R(*) and . In this work, we focus on general atomic norms R(*). Given a set of atomic vectors A  Rp, the corresponding atomic norm of any   Rp is given by

 A = inf

ca :  = caa, ca  0  a  A

(6)

aA

aA

In order for * A to be a valid norm, atomic vectors in A has to span Rp, and a  A iff -a  A. The unit ball of atomic norm * A is given by A = conv(A). In addition, we assume that the atomic set A contains v a for any v  {1}p if a belongs to A, where denotes the elementwise
(Hadamard) product for vectors. This assumption guarantees that both * A and its dual norm
are invariant under sign-changes, which is satisfied by many widely used norms, such as L1 norm, OWL norm [5] and k-support norm [2]. For the rest of the paper, we will use A, TA(), CA() and A() with A replaced by appropriate subscript for specific norms. For any vector u and coordinate set S, we define uS by zeroing out all the coordinates outside S.

3 General Analysis for Atomic Norms
In this section, we present detailed analysis of the general bounds for the geometric measures, w(A), w(CA()) and A(). In general, knowing the atomic set A is sufficient for bounding w(A). For w(CA()) and A(), we only need a single subgradient of  A and some simple additional calculations.

3.1 Gaussian width of unit norm ball

Although the atomic set A may contain uncountably many vectors, we assume that A can be de-
composed as a union of M "simple" sets, A = A1  A2  . . .  AM . By "simple," we mean the Gaussian width of each Ai is easy to compute/bound. Such a decomposition assumption is often satisfied by commonly used atomic norms, e.g., L1, L1/L2, OWL, k-support norm. The Gaussian width of the unit norm ball of * A can be easily obtained using the following lemma, which is
essentially the Lemma 2 in [16]. Related results appear in [16].

Lemma 2 Let M > 4, A1, * * * , AM  Rp, and A = mAm. The Gaussian width of unit norm ball of * A satisfies

w(A) = w(conv(A)) = w(A)  max w(Am) + 2 sup z 2 log M

1mM

zA

(7)

Next we illustrate application of this result to bounding the width of the unit norm ball of L1 and OWL norm.

Example 1.1 (L1 norm): Recall that the L1 norm can be viewed as the atomic norm induced by the set AL1 = {ei : 1  i  p}, where {ei}pi=1 is the canonical basis of Rp. Since the Gaussian width of a singleton is 0, if we treat A as the union of individual {+ei} and {-ei}, we have

w(L1 )  0 + 2 log 2p = O( log p) .

(8)

Example 1.2 (OWL norm): A recent variant of L1 norm is the so-called ordered weighted L1

(OWL) norm [13, 25, 5] defined as  owl =

p i=1

wi||i ,

where

w1



w2



...



wp



0

are

pre-specified ordered weights, and || is the permutation of || with entries sorted in decreasing

order. In [25], the OWL norm is proved to be an atomic norm with atomic set

Aowl =

Ai =

u  Rp : uSc = 0, uS =

1ip

1ip | supp(S)|=i

vS
i j=1

wj

,

v



{1}p

. (9)

3

We first apply Lemma 2 to each set Ai, and note that each Ai

contains 2i

p i

atomic vectors.

w(Ai)  0 + 2

(

i

i j=1

wj

)2

log 2i p i



2i

i j

=1

wj

2 + log p  2 i w

p 2 + log ,
i

where w is the average of w1, . . . , wp. Then we apply the lemma again to Aowl and obtain



w(owl)

=

w(Aowl)



2 w

2 2 + log p +
w

log p = O

log p ,
w

(10)

which matches the result in [13].

3.2 Gaussian width of the intersection of tangent cone and unit sphere

In this subsection, we consider the computation of general w(CA()). Using the definition of dual

norm, we can write * A. The u for

 A which

as  u, 

A= =

sup u  A,

 A

1

is a

u,  , where subgradient of

*

 A

denotes

the

dual

norm

of

 A. One can obtain u by

simply solving the so-called polar operator [26] for the dual norm * A,

u  argmax u,  .

u

 A

1

(11)

Based on polar operator, we start with the Lemma 3, which plays a key role in our analysis.

Lemma 3 Let u be a solution to the polar operator (11), and define the weighted L1 semi-norm

* u as v u =

p i=1

|ui |

*

|vi|.

Then

the

following

relation

holds

TA()  Tu () ,

where Tu () = cone{v  Rp |  + v u   u }.

The proof of this lemma is in supplementary material. Note that the solution to (11) may not be unique. A good criterion for choosing u is to avoid zeros in u, as any ui = 0 will lead to the unboundedness of unit ball of * u , which could potentially increase the size of Tu (). Next we present the upper bound for w(CA()).

Theorem 4 Suppose that u is one of the solutions to (11), and define the following sets,

Q = {i | ui = 0}, S = {i | ui = 0, i = 0},

The Gaussian width w(CA()) is upper bounded by



 p,

if R is empty





w(CA()) 
 

m

+

3 2

s

+

22max 2min

s

log

p-m s

R = {i | ui = 0, i = 0} .
, , if R is nonempty

where m = |Q|, s = |S|, min = miniR |ui | and max = maxiS |ui |.

(12)

Proof: By Lemma 3, we have w(CA())  w(Tu ()  Sp-1) w(Cu ()). Hence we can

focus on bounding w(Cu ()). We first analyze the structure of v that satisfies  + v u 

 u . For the coordinates Q it does not affect the value of

={+i |vuiu= .

0}, the corresponding Thus all possible vQ

entries form a

vi's can be arbitrary since m-dimensional subspace,

where m = |Q|. For S  R = {i | ui = 0}, we define  = S R and v = vSR, and v needs to

satisfy

v +  u   u ,
which is similar to the L1-norm tangent cone except that coordinates are weighted by |u|. Therefore we use the techniques for proving the Proposition 3.10 in [10]. Based on the structure of v, The normal cone at  for Tu () is given by
N () = {z : z, v  0 v s.t. v +  u   u } = {z : zi = 0 for i  Q, zi = |ui |sign(i)t for i  S, |zi|  |ui |t for i  R, for any t  0} .

4

Given a standard Gaussian random vector g, using the relation between Gaussian width and statistical dimension (Proposition 2.4 and 10.2 in [1]), we have

w2(Cu ())



E[ inf
zN ()

z-g

22]

=

E[ inf
zN ()

gi2 + (zj - gj )2 +

(zk - gk)2]

iQ

jS

kR

=

|Q|

+

E[ inf
zSRN ()

(|uj |sign(j)t
jS

-

gj )2

+

(zk
kR

-

gk )2 ]



|Q|

+

t2

jS

|uj |2

+

|S |

+

E[
kR

|zk

inf
||uk

(zk
|t

-

gk )2 ]



|Q|

+

t2

jS

|uj |2

+

|S

|

+

kR

2 2

+
(gk
|uk |t

-

|uk |t)2

exp( -gk2 2

)dgk



|Q|

+

t2

jS

|uj |2

+

|S

|

+

kR

2 2

1 |uk |t

exp

- |uk|2t2 2

() .

The details for the derivation above can be found in Appendix C of [10]. If R is empty, by taking t = 0, we have

()  |Q| + t2 |uj |2 + |S| = |Q| + |S| = p .
jS
If R is nonempty, we denote min = miniR |ui | and max = maxiS |ui |.

Taking t =

1 min

2 log

|S R| |S |

, we obtain

()

 |Q| + |S|(2maxt2 + 1) +

2|R| exp 

- 2mint2
2

2mint

= |Q| + |S|

22max 2min

log

|R||S | +

|S  R|

 log

|S R| |S |



|Q|

+

22max 2min

|S| log

|S  R| |S |

+ 3 |S| . 2

|S  R| |S |

+1

Substituting |Q| = m, |S| = s and |S  R| = p - m into the last inequality completes the proof.

Suppose that  is a s-sparse vector. We illustrate the above bound on the Gaussian width of the spherical cap using L1 norm and OWL norm as examples.
Example 2.1 (L1 norm): The dual norm of L1 is L norm, and its easy to verify that u = [1, 1, . . . , 1]T  Rp is a solution to (11). Applying Theorem 4 to u, we have

w(CL1 ()) 

3p s + 2s log = O
2s

p s + s log
s

.

Example 2.2 (OWL norm): For OWL, its dual norm is given by

u

 owl

=

maxbAowl

b, u .

W.l.o.g. we assume  = ||, and a solution to (11) is given by u = [w1, . . . , ws, w, w, . . . , w]T ,

in which w is the average of ws+1, . . . , wp. If all wi's are nonzero, the Gaussian width satisfies

w(Cowl()) 

3 s
2

+

2w12 w2

s

log

p s

.

3.3 Restricted norm compatibility

The next theorem gives general upper bounds for the restricted norm compatibility A().

Theorem 5 Assume that u A  max{1 u 1, 2 u 2} for all u  Rp. Under the setting of Theorem 4, the restricted norm compatibility A() is upper bounded by

A() 

 , if R is empty

Q + max

2, 1

1 + max
min

 s

, if R is nonempty

,

(13)

where  = supuRp

u u

A 2

and Q

=

supsupp(u)Q

u u

A 2

.

5

Proof: As analyzed in the proof of Theorem 4, vQ for v  Tu () can be arbitrary, and the vSR = vQc satisfies

vQc + Q c u  Q c u =

|i + vi||ui | + |vj ||uj |  |i||ui |

iS

jR

iS

= (|i| - |vi|) |ui | + |vj ||uj |  |i||ui | = min vR 1  max vS 1

iS

jR

iS

If R is empty, by Lemma 3, we obtain

A()  u () If R is nonempty, we have

sup v A  sup v A =  .

vTu () v 2

vRp v 2

A()  u ()  sup
vTu ()

vQ A + vQc A 

sup

v2

supp(v)Q, supp(v )Qc

min vR 1max vS 1

v A+ v A v+v 2

 sup v A + supp(v)Q v 2

sup
supp(v )Qc

max{1 v 1, 2 v 2} v2

min vR 1max vS 1

 Q + max{2, sup
supp(v )S

(1 +

)max
min
v2

v

1 }  Q + max{2, 1

1 + max min

 s} ,

in which the last inequality in the first line uses the property of Tu ().

Remark: We call  the unrestricted norm compatibility, and Q the subspace norm compatibility, both of which are often easier to compute than A(). The 1 and 2 in the assumption of * A can have multiple choices, and one has the flexibility to choose the one that yields the tightest bound.

Example 3.1 (L1 norm): To apply the Theorem 5 to L1 norm, we can choose 1 = 1 and 2 = 0. We recall the u for L1 norm, whose Q is empty while R is nonempty. So we have for s-sparse 

L1 ()  0 + max

0,

1 1+
1

 s

 =2 s.

Example 3.2 (OWL norm): For OWL, note that * owl  w1 * 1. Hence we choose 1 = w1 and 2 = 0. As a result, we similarly have for s-sparse 

owl()  0 + max

0, w1

1 + w1 w

 s



2w12

 s

.

w

4 Tightness of the General Bounds

So far we have shown that the geometric measures can be upper bounded for general atomic norms. One might wonder how tight the bounds in Section 3 are for these measures. For w(A), as the result from [16] depends on the decomposition of A for the ease of computation, it might be tricky to discuss its tightness in general. Hence we will focus on the other two, w(CA()) and A().
To characterize the tightness, we need to compare the lower bounds of w(CA()) and A(), with their upper bounds determined by u. While there can be multiple u, it is easy to see that any convex combination of them is also a solution to (11). Therefore we can always find a u that has the largest support, i.e., supp(u )  supp(u) for any other solution u . We will use such u to generate the lower bounds. First we need the following lemma for the cone TA().

Lemma 6 Consider a solution u to (11), which satisfies supp(u )  supp(u) for any other

solution u . Under the setting of notations in Theorem 4, we define an additional set of coordinates

P = {i | ui = 0, i = 0}. Then the tangent cone TA() satisfies

T1  T2  cl(TA()) ,

(14)

where  denotes the direct (Minkowski) sum operation, cl(*) denotes the closure, T1 = {v  Rp | vi = 0 for i / P} is a |P|-dimensional subspace, and T2 = {v  Rp | sign(vi) = -sign(i) for i  supp(), vi = 0 for i / supp()} is a | supp()|-dimensional orthant.

6

The proof of Lemma 6 is given in supplementary material. The following theorem gives us the lower bound for w(CA()) and A().

Theorem 7

Under

the

setting

of

Twh(eCoAre(m4))andLemOm(a m6,

the following + s) ,

lower

bounds

hold,

(15)

A()  QS .

(16)

Proof: To lower bound w(CA()), we use Lemma 6 and the relation between Gaussian width and statistical dimension (Proposition 10.2 in [1]),

w(TA())  w(T1  T2  Sp-1) 

E[ inf
zNT1T2 ()

z - g 22] - 1

() ,

where the normal cone P, sign(zi) = sign(i)

NT1 for

iT2 (sup) po(fT1)}.

T2 is given by Hence we have

NT1

T2

(



)

=

{z

: zi = 0

for

i

() =

E[ gi2 +

gj2I{gj j<0}] - 1 =

iP

j supp( )

|P |

+

| supp()|

-

1

=

 O( m

+

s)

,

2

where the last equality follows the fact that P  supp() = Q  S. This completes proof of (15). To prove (16), we again use Lemma 6 and the fact P  supp() = Q  S. Noting that * A is invariant under sign-changes, we get

A() = sup
vTA ( )

v A  sup

v2

vT1 T2

v A=

sup

v 2 supp(v)Psupp()

v v

A 2

= QS

.

Remark: We compare the lower bounds (15) (16) with the upper bounds (12) (13). If R is empty,

m + s = p, and the lower bounds actually match the upper bounds up to a constant factor for both

w(CA()) and A(). If R is nonempty, the lower and upper bounds of w(CA()) differ by a

multiplicative

factor

22max 2min

log(

p-m s

),

which 

can

be

small

in

practice.

For

A(),

as

QS



Q,

we usually have at most an additive O( s) term in upper bound, since the assumption on * A

often holds with a constant 1 and 2 = 0 for most norms.

5 Application to the k-Support Norm

In this section, we apply our general results on geometric measures to a non-trivial example, k-
support norm [2], which has been proved effective for sparse recovery [11, 17, 12]. The k-support norm can be viewed as an atomic norm, for which A = {a  Rp | a 0  k, a 2  1}. The k-support norm can be explicitly expressed as an infimum convolution given by



sp k

=

inf
i ui=

ui 2 ui 0  k ,
i

(17)

and its dual norm is the so-called 2-k symmetric gauge norm defined as



sp k

=

 (k) =

||1:k 2 ,

(18)

It is straightforward to see that the dual norm is simply the L2 norm of the largest k entries in ||.

Suppose that all the sets of coordinates with cardinality k can be listed as S1, S2, . . . , S(kp). Then A

can be written as A = A1  . . .  A(kp), where each Ai = {a  Rp | supp(a)  Si,

a 

2



1}.

It is not difficult to see that w(Ai) = E supaAi a, g = E gSi 2 

E

gSi

2 2



k. Using

Lemma 2, we know the Gaussian width of the unit ball of k-support norm

 w(skp)  k + 2

log

p k

  k+2

k log

p k

+k = O

p

k log + k , k

(19)

which matches that seen in the general

in [11]. Now we turn analysis, the solution

to the u to

calculation of w(Cksp()) and skp(). As we have the polar operator (11) is important in characterizing

the two quantities. We first present a simple procedure in Algorithm 1 for solving the polar operator

for

*

sp k

.

The

time

complexity

is

only

O(p

log

p

+

k).

This

procedure

can

be

utilized

to

compute

the k-support norm, or be applied to estimation with

*

sp k

using

generalized

conditional

gradient

method [26], which requires solving the polar operator in each iteration.

7

Algorithm 1 Solving polar operator for

*

sp k

Input:   Rp, positive integer k Output: solution u to the polar operator (11) 1: z = ||, t = 0

2: for i = 1 to k do

3:

1 =

z1:i-1 2, 2 =

zi:p

1, d = k - i + 1,  =

 2 ,  =
22 d+12 d2

 1 , w =
2 1-2d

z1:i-1 2

4:

if

12 2

+

2

>

t

and

 < wi-1 then

5:

t

=

12 2

+ 2,

u

=

[w, 1]T

(1 is (p - i + 1)-dimensional vector with all ones)

6: end if

7: end for 8: change the sign and order of u to conform with  9: return u

Theorem 8 For a given , Algorithm 1 returns a solution to polar operator (11) for

*

sp k

.

The and

proof of skp()

this theorem is for s-sparse 

provided in supplementary material. Now (here s-sparse  means | supp()| = s) in

we consider w(Cksp()) three scenarios: (i) over-

specified k, where s < k, (ii) exactly specified k, where s = k, and (iii) under-specified k, where

s > k. The bounds are given in Theorem 9, and the proof is also in supplementary material.

Theorem 9 For given compatibility skp()

s-sparse   for a specified

Rp, the Gaussian k are given by

width

w(Cksp())

and

the

restricted

norm

 p , if s < k
 


 

2p k

,

if

s<k





  w(Cksp())  

3 2

s

+

s log2m2ax
m2in

p s

, if s = k

,

  skp()  

 2(1

+

)m ax
m in

,

if

s=k









  

3 2

s

+

22max 2min

s

log

p s

, if s > k

  

(1 +

)max
min

2s k

,

if

s>k

(20)

where m ax = maxisupp() |i| and m in = minisupp() |i|.

,

Remark: Previously skp() is unknown and the bound on w(Cksp()) given in [11] is loose, as

it used the result in [21]. Based on Theorem 9, we note that the choice of k can affect the recovery

guarantees. Over-specified and skp(), resulting in a

k leads to a direct weak error bound.

dependence The bounds

on the dimensionality p for w(Cksp()) are sharp for exactly specified or under-

specified k. Thus, it is better to under-specify k in practice. where the estimation error satifies

n -  2  O

s + s log (p/k) n

(21)

6 Conclusions
In this work, we study the problem of structured estimation with general atomic norms that are invariant under sign-changes. Based on Dantzig-type estimators, we provide the general bounds for the geometric measures. In terms of w(A), instead of comparison with other results or direct calculation, we demonstrate a third way to compute it based on decomposition of atomic set A. For w(CA()) and A(), we derive general upper bounds, which only require the knowledge of a single subgradient of  A. We also show that these upper bounds are close to the lower bounds, which makes them practical in general. To illustrate our results, we discuss the application to k-support norm in details and shed light on the choice of k in practice.

Acknowledgements
The research was supported by NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A.

8

References
[1] D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: Phase transitions in convex programs with random data. Inform. Inference, 3(3):224-294, 2014.
[2] A. Argyriou, R. Foygel, and N. Srebro. Sparse prediction with the k-support norm. In Advances in Neural Information Processing Systems (NIPS), 2012.
[3] A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. In Advances in Neural Information Processing Systems (NIPS), 2014.
[4] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The Annals of Statistics, 37(4):1705-1732, 2009.
[5] M. Bogdan, E. van den Berg, W. Su, and E. Candes. Statistical estimation and testing via the sorted L1 norm. arXiv:1310.1969, 2013.
[6] T. T. Cai, T. Liang, and A. Rakhlin. Geometrizing Local Rates of Convergence for High-Dimensional Linear Inverse Problems. arXiv:1404.4408, 2014.
[7] E. Candes and T Tao. The Dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313-2351, 2007.
[8] E. J. Candes, J. K. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications on Pure and Applied Mathematics, 59(8):1207-1223, 2006.
[9] E. J. Cands and B. Recht. Simple bounds for recovering low-complexity models. Math. Program., 141(12):577-589, 2013.
[10] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805-849, 2012.
[11] S. Chatterjee, S. Chen, and A. Banerjee. Generalized dantzig selector: Application to the k-support norm. In Advances in Neural Information Processing Systems (NIPS), 2014.
[12] S. Chen and A. Banerjee. One-bit compressed sensing with the k-support norm. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2015.
[13] M. A. T. Figueiredo and R. D. Nowak. Sparse estimation with strongly correlated variables using ordered weighted l1 regularization. arXiv:1409.4005, 2014.
[14] Y. Gordon. Some inequalities for gaussian processes and applications. Israel Journal of Mathematics, 50(4):265-289, 1985.
[15] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In International Conference on Machine Learning (ICML), 2009.
[16] A. Maurer, M. Pontil, and B. Romera-Paredes. An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning. In Conference on Learning Theory (COLT), 2014.
[17] A. M. McDonald, M. Pontil, and D. Stamos. Spectral k-support norm regularization. In Advances in Neural Information Processing Systems (NIPS), 2014.
[18] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for the analysis of regularized M -estimators. Statistical Science, 27(4):538-557, 2012.
[19] S. Oymak, C. Thrampoulidis, and B. Hassibi. The Squared-Error of Generalized Lasso: A Precise Analysis. arXiv:1311.0830, 2013.
[20] Y. Plan and R. Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. IEEE Transactions on Information Theory, 59(1):482-494, 2013.
[21] N. Rao, B. Recht, and R. Nowak. Universal Measurement Bounds for Structured Sparse Signal Recovery. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.
[22] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1):267-288, 1996.
[23] J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements. In Sampling Theory, a Renaissance. 2015.
[24] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society, Series B, 68:49-67, 2006.
[25] X. Zeng and M. A. T. Figueiredo. The Ordered Weighted 1 Norm: Atomic Formulation, Projections, and Algorithms. arXiv:1409.4271, 2014.
[26] X. Zhang, Y. Yu, and D. Schuurmans. Polar operators for structured sparse estimation. In Advances in Neural Information Processing Systems (NIPS), 2013.
9

