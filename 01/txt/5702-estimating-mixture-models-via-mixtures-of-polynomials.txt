Estimating Mixture Models via Mixtures of Polynomials
Sida I. Wang Arun Tejasvi Chaganty Percy Liang Computer Science Department, Stanford University, Stanford, CA, 94305
{sidaw,chaganty,pliang}@cs.stanford.edu
Abstract
Mixture modeling is a general technique for making any simple model more expressive through weighted combination. This generality and simplicity in part explains the success of the Expectation Maximization (EM) algorithm, in which updates are easy to derive for a wide class of mixture models. However, the likelihood of a mixture model is non-convex, so EM has no known global convergence guarantees. Recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist. In this work, we present Polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in EM. Polymom is applicable when the moments of a single mixture component are polynomials of the parameters. Our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a Generalized Moment Problem. We solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra. This framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation. Simulations show good empirical performance on several models.
1 Introduction
Mixture models play a central role in machine learning and statistics, with diverse applications including bioinformatics, speech, natural language, and computer vision. The idea of mixture modeling is to explain data through a weighted combination of simple parametrized distributions [1, 2]. In practice, maximum likelihood estimation via Expectation Maximization (EM) has been the workhorse for these models, as the parameter updates are often easily derivable. However, EM is well-known to suffer from local optima. The method of moments, dating back to Pearson [3] in 1894, is enjoying a recent revival [4, 5, 6, 7, 8, 9, 10, 11, 12, 13] due to its strong global theoretical guarantees. However, current methods depend strongly on the specific distributions and are not easily extensible to new ones. In this paper, we present a method of moments approach, which we call Polymom, for estimating a wider class of mixture models in which the moment equations are polynomial equations (Section 2). Solving general polynomial equations is NP-hard, but our key insight is that for mixture models, the moments equations are mixtures of polynomials equations and we can hope to solve them if the moment equations for each mixture component are simple polynomials equations that we can solve. Polymom proceeds as follows: First, we recover mixtures of monomials of the parameters from the data moments by solving an instance of the Generalized Moment Problem (GMP) [14, 15] (Section 3). We show that for many mixture models, the GMP can be solved with basic linear algebra and in the general case, can be approximated by an SDP in which the moment equations are linear constraints. Second, we extend multiplication matrix ideas from the computer algebra literature [16,
1

mixture model

xt data point (RD)

zt latent mixture component ([K])

k parameters of component k (RP )

k mixing proportion of p(z = k)

[mko]mKk=en1 ts

all model of data

parameters

n(x) observation function fn() observation function moments of parameters

Ly the Riesz linear functional y y = Ly(), th moment  probability measure for y

y (y) the moment sequence Mr(y) moment matrix of degree r

sizes

D data dimensions

K mixture components

P parameters of mixture components

T data points

N constraints

[N ] {1, . . . , N }

r degree of the moment matrix

s(r) size of the degree r moment matrix

polynomials

R[] polynomial ring in variables  N set of non-negative integers

, , 
an

vmcoeocentfofiorcmioeifanletxQopfoPp=ne1nitnps pf(inn(N)P or ND)

Table 1: Notation: We use lowercase letters (e.g., d) for indexing, and the corresponding uppercase letter to denote the upper limit (e.g., D, in "sizes"). We use lowercase letters (e.g., k,p) for scalars, lowercase bold letters (e.g., ) for vectors, and bold capital letters (e.g., M) for matrices.

1. Write down a mixture model

4. Recover parameter moments (y)

z z  Multinomial(1, 2) x | z  N (z, z) 2 R
x k = (k, k) 2 R2

21

 2

2
3

1 y0,0 y1,0 y2,0 y0,1

Mr(y) =

 2

664

y1,0 y2,0

y2,0 y3,0

y3,0 y4,0

y1,1 y2,1

775

2 y0,1 y1,1 y2,1 y0,2

2. Derive single mixture moment equations
(x) !E f () x x2 2 + 2 x3 3 + 3 2 ... ...

3. Add data
x1  p(x; ) ...
xT  p(x; )

minimize
y
s.t.

tr(Mr (y)) Myyy123,,,r000(++=y)3yT10y,P11,01=,Tt=yxT10tT1,P0P=Tt Ttx1x2t 3t ...

user specified framework specified

5. Solve for parameters

Mr(y) = VPV> # sim. diag.

P = diag([1, 2])

2 v(1)

11

V=


2
2 2
4

666666664

2 3 4 6 9

2 2

12

v(2) 3

1

2 5 4 10 25

777777775

20

Figure 1: An overview of applying the Polymom framework.
17, 18, 19] to extract the parameters by solving a certain generalized eigenvalue problem (Section 4). Polymom improves on previous method of moments approaches in both generality and flexibility. First, while tensor factorization has been the main driver for many of the method of moments approaches for many types of mixture models, [6, 20, 9, 8, 21, 12], each model required specific adaptations which are non-trivial even for experts. In contrast, Polymom provides a unified principle for tackling new models that is as turnkey as computing gradients or EM updates. To use Polymom (Figure 1), one only needs to provide a list of observation functions ( n) and derive their expected values expressed symbolically as polynomials in the parameters of the specified model (fn). Polymom then estimates expectations of n and outputs parameter estimates of the specified model. Since Polymom works in an optimization framework, we can easily incorporate constraints such as non-negativity and parameter tying which is difficult to do in the tensor factorization paradigm. In simulations, we compared Polymom with EM and tensor factorization and found that Polymom performs similarly or better (Section 5). This paper assumes identifiability and infinite data. With the exception of a few specific models in Section 5, we defer issues of general identifiability and sample complexity to future work.

2

2 Problem formulation

2.1 The method of moments estimator

In a mixture model, each data point x 2 RD is associated with a latent component z 2 [K]:

z  Multinomial(), x | z  p(x; z),

(1)

where the kth

 = (1, . . . , K ) are the mixing mixture component, and x 2 RD

icsoethffie criaenndtso,mkva2riaRblPe

are the true model representing data.

parameters We restrict

for our

attention to mixtures where each component distribution comes from the same parameterized family.

For example, for a mixture of covariance of component k.

Gaussians,

k

=

(k

2

RD, k

2

RDD )

consists

of

the

mean

and

We define N observation functions n : RD ! R for n 2 [N ] and define fn() to be the expectation

of n over a single component with parameters , which we assume is a simple polynomial:

X

fn() := Exp(x;)[ n(x)] = an,

(2)



where



=

QP
p=1

pPreKks=se1daksfna (mikx)t.ure

pp . The expectation of each observation of polynomials of the true parameters E[ n

(fxu)n]c=tioPn EKk=[ 1n(kxE)][

can then be n(x)|z = k

ex]=

The method ditions

of

moments

for

mixture

models

seeks

parameters

[k ]Kk=1

that

satisfy

the

moment

con-

XK E[ n(x)] = kfn(k).

(3)

k=1

where E[ is to find

pnar(axm)]ectearnsbseateisstfiyminagtemd formomentthceodnadtiat:ioT1nsPthTta=t1cann(bxetw) !rpitteEn[

n
in

(x)]. The goal the mixture of

of this work polynomial

form (3). We assume that the N observations functions 1, . . . , N uniquely identify the model parameters (up to permutation of the components).

Example 2.1 (1-dimensional Gaussian mixture). Consider a K-mixture of 1D Gaussians with pa-

r(Famigeutreers1: kst=eps[1k,ank2d]

corresponding 2). We choose

to the mean and the observation

variance, respectively, of functions, (x) = [x1, .

the k-th component . . , x6], which have

c(r3eoc)r,orevEse[prxot2nh]de=ipnagPrmamKko=me1teenrkst .(pNok2loy+tneotmhk2ai)at.ltshG,efiv6(emn)o=m(xe[n),tsan2wd+e fu(s2e,h)3a, v+aen3bdede2na,tsa.h,.o.twh].enFPbooyrley[x3ma]omtmoplbfere,aismnuseftfiwacnoietriknattcifanongr

a mixture of two Gaussians.

Example 2.2 (Mixture of linear regressions). Consider a mixture of linear regressions [22, 9],

where each data point x = [x, y] is drawn from component k by sampling x from an unknown

distribution k = (wk, functions to

ibk2n)edaerp(eexnt)hde=enslt[oxop,fexkayn,adxnydn2o,siesxet2ti,vn.ag.r.iya, nx=c3ey2fwo],krfxeoar+cwhhc,iocwmhhptehoreneemnotmk.eNnLte(tp0uo,slytkn2a)ok.me iToauhlsre

parameters observation are f () =

[E[x], E[x2]w, E[x3]w2 + E[x] 2, E[x2], . . .].

In Example 2.1, the coefficients an in the polynomial fn() are just constants determined by integration. For the conditional model in Example 2.2, the coefficients depends on the data. However, we cannot handle arbitrary data dependence, see Section D for sufficient conditions and counterexamples.

2.2 Solving the moment conditions

Our goal is to mixture model

recover model that generated

parameters the data as

we1l,l.

.. as

,theKir

2 RP for respective

each of mixing

the K components proportions 1, . . . ,

of the K 2

R. To start, let's ignore sampling noise and identifiability issues and suppose that we are given exact

moment conditions as defined in (3). Each condition fn 2 R[] is a polynomial of the parameters

, for n = 1, . . . , N .

3

Equation 3 is a polynomial system of N equations in the K + K  P variables [1, . . . , K ] and [1, . . . , K ] 2 RP K . It is natural to ask if standard polynomial solving methods can solve (3) in the case where each fn() is simple. Unfortunately, the complexity of general polynomial equation solving is lower bounded by the number of solutions, and each of the K! permutations of the mixture components corresponds to a distinct solution of (3) under this polynomial system representation. While several methods can take advantage of symmetries in polynomial systems [23, 24], they still cannot be adapted to tractably solve (3) to the best of our knowledge.

The key idea of Polymom is to exploit the mixture representation of the moment equations (3).

Specifically, let  be probability measure).

Ta hpeanrtiwcuelcaarn"mexixptruerses"thoevemrothmeecnotmcopnodnietinotnpsa(r3a)mientetersrms1,o.f.. ,: k

(i.e.



is

a

Z XK E[ n(x)] = fn() (d), where () = k (
k=1

k ).

(4)

As a result, solving the original moment conditions (3) is equivalent to solving the following feasibility problem over , but where we deliberately "forget" the permutation of the components by using  to represent the problem:

find s.t.

R

2 M+(RP ), fn() (d)

the set of probability measures = E[ n(x)], n = 1, . . . , N

over

RP

 is K-atomic (i.e. sum of K deltas).

(5)

If the true model parameters tion, then the measure ()

[=kP]Kk=Kk=1 1cank

be identified by the N observed moments up ( k) solving Problem 5 is also unique.

to

permuta-

Polymom solves Problem 5 in two steps:

1. Moment completion (Section 3): We show that Problem 5 over the measure  can be

driseelgMarxeered(ayttom)ano=sStDPrP. Kko=v1er ak

certain (parameter) moment matrix vr(k)vr(k)>, where vr(k) is

Mr(y) whose optimal solution the vector of all monomials of

2. eSiogleuntidoencoemxtpraocstiitoionn(Sperocbtiloenm4s),:wWhoesteheenigteankvealMuers(yyi)eladn[dcko]Kkn=st1r.uct a series of generalized

Remark. From this point on, distributions and moments refer to  which is over parameters, not over the data. All the structure about the data is captured in the moment conditions (3).

3 Moment completion

The first step is to reformulate Problem 5 as an instance of the Generalized Moment Problem (GMP)

introduced by [15]. A reference on the GMP, algorithms for solving GMPs, and its various exten-

sions is [14]. We start by observing that Problem 5 really only depends on the integrals of monomials

under the measure : for example, if over the constituent monomials (y3,0 the integral over fn. This suggests

tfh:=na(tRw)e=13ca2(nd13o)ptaimn12diz2ye,2,to1hve:en=r wtRheeo12(npla2yran(medeedt)e)tro)inkmnooormdweertnhtteosieenvqtaeulgeurnaactlees

y = (y)2NP , senting measure

rather than  if y =

Rthemea(sdur)e

 itself. We say that the for all , but we do not

moment sequence y has a repreassume that such a  exists. The

Riesz linear functional Ly : R[] ! R is defined to be the linear map such that Ly() := y and

Ly(1) , then

=Ly1.sFimorpleyxammapplse,pLolyyn(o2m13ialsf12

2 to

+ 3) = 2y3,0 integrals of f

y2,1 + 3. against .

If

y

has

a

representing

measure

The key idea of the GMP approach is to convexify the problem by treating y as free variables and then introduce constraints to guarantee that y has a representing measure. First, let vr() := [ : ||  r] 2 R[]s(r) be the vector of all s(r) monomials of degree no greater than r. Then, define the truncated moment matrix as Mr(y) := Ly(vr()vr()T), where the linear functional Ly is applied elementwise (see Example 3.1 below). If y has a representing measure , then Mr(y) is simply a (positive) integral over rank 1 matrices vr()vr()T with respect to , so necessarily

4

Mr(y)  0 holds. Furthermore, by Theorem 1 [25], for y to have a K-atomic representing measure, it is sufficient that rank(Mr(y)) = rank(Mr 1(y)) = K. So Problem 5 is equivalent to

find s.t.

yP2


RN (or equivalently, find any = E[ n(x)], n =

M(y)) 1, . . . , N

Mr(y)  0, y0 = 1

rank(Mr(y)) = K and rank(Mr 1(y)) = K.

(6)

Unfortunately, the rank constraints in Problem 6 are not tractable. We use the following relaxation to obtain our final (convex) optimization problem

minimize
y
s.t.

tPr(CMr (y))  any =

E[

n(x)],

n = 1, . . . , N

Mr(y)  0, y0 = 1

(7)

where C imizing the

0 is a chosen scaling matrix. nuclear norm of the moment

A common choice is C matrix, the usual convex

r=elaIxsa(rti)ocnofrorersrpaonnkd.inSgectotiomninA-

discusses some other choices of C.

Example 3.1 (moment matrix for a 1-dimensional Gaussian mixture). Recall that the parameters  = [, 2] are the mean and variance of a one dimensional Gaussian. Let us choose the monomials v2() = [1, , 2, 2]. Step 4 for Figure 1 shows the moment matrix when using r = 2. Each row and column of the moment matrix is labeled with a monomial and entry (i, j) is subscripted by the product of the monomials in row i and column j. For 2(x) := x2, we have f2() = 2 + c, which leads to the linear constraint y2,0 + y0,1 E[x2] = 0. For 3(x) = x3, f3() = 3 + 3c, leading to the constraint y3,0 + 3y1,1 E[x3] = 0.

Related work. Readers familiar with the sum of squares and polynomial optimization litera-

ture [26, 27, 28, 29] will note that Problem 7 is similar to the SDP relaxation of a polynomial

optimization problem. However, in typical polynomial optimization, we are only interested in so-

lutions  that actually satisfy the given constraints, whereas here we are interested in K solutions

[makc]Kkh=in1e,

whose mixture satisfies constraints learning, generalized PCA has been

corresponding to the moment conditions (3). formulated as a moment problem [30] and the

Within Hankel

matrix (basically the moment matrix) has been used to learn weighted automata [13]. While similar

tools are used, the conceptual approach and the problems considered are different. For example,

the moment matrix of this paper consists of unknown moments of the model parameters, whereas

exisiting works considered moments of the data that are always directly observable.

Constraints. Constraints such as non-negativity (for parameters which represent probabilities or variances) and parameter tying [31] are quite common in graphical models and are not easily addressed with existing method of moments approaches. The GMP framework allows us to incorporate some constraints using localizing matrices [32]. Thus, we can handle constraints during the estimation procedure rather than projecting back onto the constraint set as a post-processing step. This is necessary for models that only become identifiable by the observed moments after constraints are taken into account. We describe this method and its learning implications in Section C.1.

Guarantees and statistical efficiency. In some circumstances, e.g. in three-view mixture models or the mixture of linear regressions, the constraints fully determine the moment matrix - we consider these cases in Section 5 and Appendix B. While there are no general guarantee on Problem 7, the flat extension theorem tells us when the moment matrix corresponds to a unique solution (more discussions in Appendix A): Theorem 1 (Flat extension theorem [25]). Let y be the solution to Problem 7 for a particular r. If Mr(y)  0 and rank(Mr 1(y)) = rank(Mr(y)) then y is the optimal solution to Problem 6 for K = rank(Mr(y)) and there exists a unique K-atomic supporting measure  of Mr(y).
Recovering Mr(y) is linearly dependent on small perturbations of the input [33], suggesting that the method has polynomial sample complexity for most models where the moments concentrate at a polynomially rate. Finally, in Appendix C, we discuss a few other important considerations like noise robustness, making Problem 7 more statistical efficient, along with some technical results on the moment completion problem and some open problems.

5

4 Solution extraction

Hoidfaeevaxisntrfgarocctmoinmgspotllhevetienmdgotmhdeeul(ltppivaararaarmimateeetteeprro)slmy[nokom]mKke=ina1tl.msTyahstetreixsmoMsluwtriho(yenr)eex(tSthreeacctstioioolnunt3imo),newtsheaordneowewiegtpeunrrevnsaetlonutethsiseobfparcsoeebrdlteaominn multiplication matrices [16, 17, 34, 35].1 The main advantage of the solution extraction view is that higher-order moments and structure in parameters are handled in the framework without modelspecific effort.

Recall that the true moment matrix is Mr(y)

=

PK
k=1

k

v(k

)v(k

)T,

where v()

:=

[fokr,pv1a,:=r.i.a.(b,lesk)asp(nrd)d]e[2noktR]eKks[=1t]hsfe(orpr) tthchovenattlrauuienesosfoalltluhtteihokenthms ctooonmtohmpeosieanlevsnautr,piawbtohleidcshe(gncrooetreerertsh.peWobneodlsudstfeoacae)s=.olFu[otri1o,enx. .afo.m,rptlPhee],

variable p. Typically, s(r) K, P and the elements of v() are arranged in a degree ordering so

that ||i||1  ||j||1 for i  j. We can also write Mr(y) as Mr(y) = VPV>, where the canon-

iwcaanl tbtaosifsacVtor:i=ze[vM(r(1y),

. )

.. to

,gve(tVK,

)] 2 Rs(r)K and P := diag(1, . . . , K ). At the high level, we however we cannot simply eigen-decompose Mr(y) since V is

not orthogonal. To overcome this challenge, we will exploit the internal structure of V to construct

several other matrices that share the same factors and perform simultaneous diagonalization.

Sminpovenc.oifiNmcoiaawlllsycw,olnietsthiVdeex[rpot1hn;ee.ne. xt.s;pon1Ke,]n. .2t .

RKK be a sub-matrix of V with only

,
p

K
2

2 NP . Typically, NP which is 1 in

po1s,it.i.o.n,

p

Kanadre0

the rows corresponding to just the first K monomials elsewhere, corresponding

to the monomial  p = p. The key property of the canonical basis is that multiplying each column

k by a monomial k,p just performs a "shift" to another set of rows:

 V[ 1; . . . ; K ] Dp = V 1 + p; . . . ; K + p ,

where Dp := diag(1,p, . . . , K ,p). (8)

Note that Dp contains the pth parameter for all K mixture components.

Example and 2 =

4.1 (Shifting the canonical basis). [ 2, 5]. To extract the solution for

Let  = 1 (which

[1, 2] and the true are (1,1, 2,1)), let

solutions be 1 = (1, 0),

21==(1[2, ,13)],

and 1 = (1, 0).

1

2

v( 1 )
1

V=

1
2 12 1 2 22

6666664

12 2

2 3 4 6 9 12

v( 2 )
1

3

2 5 4 10 25

7777775

20

1
|1 2

 v1 2 6 {z
V[ 1;

v2
2 10
2]

 20 02
} | {z }

12
= |122

 v1 4 12 {z

v2
4 20
}

diag(1,1 ,2,1 )

V[ 1+ 1; 2+ 1]

(9)

While the above reveals the structure of V, we don't know V. However, we recover its column space U 2 Rs(r)K from the moment matrix Mr(y), for example with an SVD. Thus, we can relate U and V by a linear transformation: V = UQ, where Q 2 RKK is some unknown invertible matrix.

Equation 8 can now be rewritten as: 
U[ 1; . . . ; K ]Q Dp = U 1 + p; . . . ; K + p Q, p = 1, . . . , P,

(10)

which is a generalized eigenvalue problem where Dp are the eigenvalues and Q are the eigenvectors.

Crucially, the eigenvalues, Dp = diag(1,p, . . . , K ,p) give us solutions to our parameters. Note

that for any choice of share eigenvectors Q,

tho1u, g. .h.

,theKir

and p 2 [P ], we have generalized eigenvalue problems that eigenvectors Dp may differ. Corresponding eigenvalues (and

hence solutions) can be obtained by solving a simultaneous generalized eigenvalue problem, e.g., by

using random projections like Algorithm B of [4] or more robust [37] simutaneous diagonalization

algorithms [38, 39, 40].

1 [36] is a short overview and [35] is a comprehensive treatment including numerical issues.

6

Table 2: Applications of the Polymom framework. See Appendix B.2 for more details.

Mixture of linear regressions

Model x = [x, ] is observed where x 2 RD is drawn from an unspecified distribution and
 N (w * x, 2I), and is known. The parameters are k = (wk) 2 RD.

Observation functions

,b(x) = x b for 0  ||  3, b 2 [2]. fMfo,,12m((e))n==t pPEo[lxPpy=n1]oEm2[+xiaPl+s Pp,pq]=w1pE[xxpxq]wpwq,

where the where.

p 2 NP is 1 in position p and 0 else-

Mixture of Gaussians

Model x 2 RD is observed where x is drawn from a Gaussian with diagonal covariance: x  N (, diag(c)). The parameters are k = (k, ck) 2 RD+D.

Observation functions (x) = x for 0  ||  4.
Mfo(m)e=ntQpoDd=ly1nhomd (iadl,scd). 2

Multiview mixtures

Model With 3 views, x = [x(1), x(2), x(3)] is observed
where x(1), x(2), x(3) 2 RD and x() is drawn from an unspecified distribution with mean () for  2 [3]. The parameters are k = ((k1), (k2), (k3)) 2 RD+D+D.

Observation functions Mijokm(xe)n=t pxo(il1y)xn(jo2m)xi(ka3)lswhere 1  i, j, k  D. fijk() = i(1)j(2)k(3).

We describe one approach to solve (10), which is similar to Algorithm B of [4]. The idea is to take P

random weighted combinations of the equations (10) and solve the resulting (generalized) eigende-

composition problems. Then for each q = 1, . .

Let . Q,

R2 solve

RP P U[ 1;

be a ...;

raKnd] o1mmPaPpt=rix1

whose entries 
Rq,pU 1 +

are drawn from p; . . . ; K +

N(0, 1). p Q=

QbyDdqe.finTihtieonresuql,tking=eiPgePpn=va1lRueqs,pcakn,pb,esocowlleeccteadn

in  2 simply

RP K invert

, where q,k = Dq,k,k. to obtain [1, . . . , K ]

Note =R

that 1.

Although this simple approach does not have great numerical properties, these eigenvalue problems

are solvable 1 as long as

if the eigenvalues the parameters k

[ q,1, . . . , q,K ] are distinct for are different from each other.

all

q,

which

happens

with

probability

In Appendix B.1, we show how a prior tensor decomposition algorithm from [4] can be seen as solving Equation 10 for a particular instantiation of 1, . . . K .

5 Applications

Let us now look at some applications of Polymom. Table 2 presents several models with corresponding observation functions and moment polynomials. It is fairly straightforward to write down observation functions for a given model. The moment polynomials can then be derived by computing expectations under the model- this step can be compared to deriving gradients for EM.

We implemented Polymom for several mixture models in Python (code: https://github.

com/sidaw/polymom). We used CVXOPT to handle the SDP and the random projections algo-

rithm from to extract solutions. In Table 3, we show the relative error maxk ||k averaged over 10 random models of each class.

k ||2 /||k ||2

In the rest of this section, we will discuss guarantees on parameter recovery for each of these models.

of

2 h( the th

,

c)

=

Pb/2c
i=0

a,

(univariate) Hermite

2i 2ici and a,i be the absolute value of polynomial. For example, the first few are

the coefficient h1(, c) = ,

of the h2(,

degree i c) = 2

term + c,

h3(, c) = 3 + 3c, h4(, c) = 4 + 62c + 3c2.

7

Gaussians spherical diagonal constrained Others 3-view lin. reg.

Methd. K, D 2, 2 2, 2 2, 2
K, D 3, 3 2, 2

EM TF Poly T = 103
0.37 2.05 0.58 0.44 2.15 0.48 0.49 7.52 0.38
T = 104 0.38 0.51 0.57 - - 3.51

EM TF Poly T = 104
0.24 0.73 0.29 0.48 4.03 0.40 0.47 2.56 0.30
T = 105 0.31 0.33 0.26 - - 2.60

EM TF Poly T = 105
0.19 0.36 0.14 0.38 2.46 0.35 0.34 3.02 0.29
T = 106 0.36 0.16 0.12 - - 2.52

Table 3: T is the number of samples, and the error metric is defined above. Methods: EM: sklearn initialized with k-means using 5 random restarts; TF: tensor power method implemented in Python; Poly: Polymom by solving Problem 7. Models: for mixture of Gaussians, we have  2||1 2||2. spherical and diagonal describes the type of covariance matrix. The mean parameters of constrained Gaussians satisfies 1 + 2 = 1. The best result is bolded. TF only handles spherical variance, but it was of interest to see what TF does if the data is drawn from mixture of Gaussians with diagonal covariance, these results are in strikeout.

Mixture of Linear Regressions. We can guarantee that Polymom can recover parameters for this model when K  D by showing that Problem 6 can be solved exactly: observe that while no entry of the moment matrix M3(y) is directly observed, each observation gives us a linear constraint on the entries of the moment matrix and when K  D, there are enough equations that this system admits an unique solution for y. Chaganty et al. [9] were also able to recover parameters for this model under the same conditions (K  D) by solving a series of low-rank tensor recovery problems, which ultimately requires the computation of the same moments described above. In contrast, the Polymom framework makes the dependence on moments upfront and takes care of the heavy-lifting in a problem-agnostic manner. Lastly, the model can be extended to handle per component noise by including as a parameter, an extension that is not possible using the method in [9].

Multiview Mixtures. We can guarantee parameter recovery when K  D by proving that Problem 7 can be solved exactly (see Section B.2).

Mixture of Gaussians. In this case however, the moment conditions are non-trivial and we cannot

guarantee recovery of the true parameters. However, Polymom is guaranteed to recover a mixture of

Gaussians that match the moments. We can also apply constraints to the model: consider the case

of 2d case,

mixture where the mean parameters for all components we just need to add constraints to Problem 7: y(1,0)+

lies on a parabola 1 y(0,2)+ = 0 for

all 22

= 2

0. In this N2 up to

degree | |  2r 2. By incorporating these constraints at estimation time, we can possibly identify

the model parameters with less moments. See Section C for more details.

6 Conclusion

We presented an unifying framework for learning many types of mixture models via the method of moments. For example, for the mixture of Gaussians, we can apply the same algorithm to both mixtures in 1D needing higher-order moments [3, 11] and mixtures in high dimensions where lowerorder moments suffice [6]. The Generalized Moment Problem [15, 14] and its semidefinite relaxation hierarchies is what gives us the generality, although we rely heavily on the ability of nuclear norm minimization to recover the underlying rank. As a result, while we always obtain parameters satisfying the moment conditions, there are no formal guarantees on consistent estimation. The second main tool is solution extraction, which characterizes a more general structure of mixture models compared the tensor structure observed by [6, 4]. This view draws connections to the literature on solving polynomial systems, where many techniques might be useful [35, 18, 19]. Finally, through the connections we've drawn, it is our hope that Polymom can make the method of moments as turnkey as EM on more latent-variable models, as well as improve the statistical efficiency of method of moments procedures.

Acknowledgments. This work was supported by a Microsoft Faculty Research Fellowship to the third author and a NSERC PGS-D fellowship for the first author.

8

References
[1] D. M. Titterington, A. F. Smith, and U. E. Makov. Statistical analysis of finite mixture distributions, volume 7. Wiley New York, 1985.
[2] G. McLachlan and D. Peel. Finite mixture models. John Wiley & Sons, 2004. [3] K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the
Royal Society of London. A, 185:71-110, 1894. [4] A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden
Markov models. In Conference on Learning Theory (COLT), 2012. [5] A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y. Liu. Two SVDs suffice: Spectral decomposi-
tions for probabilistic topic modeling and latent Dirichlet allocation. In Advances in Neural Information Processing Systems (NIPS), 2012. [6] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. arXiv, 2013. [7] D. Hsu, S. M. Kakade, and P. Liang. Identifiability and unmixing of latent parse trees. In Advances in Neural Information Processing Systems (NIPS), 2012. [8] D. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions. In Innovations in Theoretical Computer Science (ITCS), 2013. [9] A. Chaganty and P. Liang. Spectral experts for estimating mixtures of linear regressions. In International Conference on Machine Learning (ICML), 2013. [10] A. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In Symposium on Theory of Computing (STOC), pages 553-562, 2010. [11] M. Hardt and E. Price. Sharp bounds for learning a mixture of two Gaussians. arXiv preprint arXiv:1404.4997, 2014. [12] R. Ge, Q. Huang, and S. M. Kakade. Learning mixtures of Gaussians in high dimensions. arXiv preprint arXiv:1503.00424, 2015. [13] B. Balle, X. Carreras, F. M. Luque, and A. Quattoni. Spectral learning of weighted automata - A forwardbackward perspective. Machine Learning, 96(1):33-63, 2014. [14] J. B. Lasserre. Moments, Positive Polynomials and Their Applications. Imperial College Press, 2011. [15] J. B. Lasserre. A semidefinite programming approach to the generalized problem of moments. Mathematical Programming, 112(1):65-92, 2008. [16] H. J. Stetter. Multivariate polynomial equations as matrix eigenproblems. WSSIA, 2:355-371, 1993. [17] H. M. Moller and H. J. Stetter. Multivariate polynomial equations with multiple zeros solved by matrix eigenproblems. Numerische Mathematik, 70(3):311-329, 1995. [18] B. Sturmfels. Solving systems of polynomial equations. American Mathematical Society, 2002. [19] D. Henrion and J. Lasserre. Detecting global optimality and extracting solutions in GloptiPoly. In Positive polynomials in control, pages 293-310, 2005. [20] A. Anandkumar, R. Ge, D. Hsu, and S. Kakade. A tensor spectral approach to learning mixed membership community models. In Conference on Learning Theory (COLT), pages 867-881, 2013. [21] A. Anandkumar, R. Ge, and M. Janzamin. Provable learning of overcomplete latent variable models: Semi-supervised and unsupervised settings. arXiv preprint arXiv:1408.0553, 2014. [22] K. Viele and B. Tong. Modeling with mixtures of linear regressions. Statistics and Computing, 12(4):315- 330, 2002. [23] B. Sturmfels. Algorithms in invariant theory. Springer Science & Business Media, 2008. [24] R. M. Corless, K. Gatermann, and I. S. Kotsireas. Using symmetries in the eigenvalue method for polynomial systems. Journal of Symbolic Computation, 44(11):1536-1550, 2009. [25] R. E. Curto and L. A. Fialkow. Solution of the truncated complex moment problem for flat data, volume 568. American Mathematical Society, 1996 1996. [26] J. B. Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on Optimization, 11(3):796-817, 2001. [27] M. Laurent. Sums of squares, moment matrices and optimization over polynomials. In Emerging applications of algebraic geometry, pages 157-270, 2009. [28] P. A. Parrilo and B. Sturmfels. Minimizing polynomial functions. Algorithmic and quantitative real algebraic geometry, DIMACS Series in Discrete Mathematics and Theoretical Computer Science, 60:83- 99, 2003. [29] P. A. Parrilo. Semidefinite programming relaxations for semialgebraic problems. Mathematical programming, 96(2):293-320, 2003. [30] N. Ozay, M. Sznaier, C. M. Lagoa, and O. I. Camps. GPCA with denoising: A moments-based convex approach. In Computer Vision and Pattern Recognition (CVPR), pages 3209-3216, 2010.
9

