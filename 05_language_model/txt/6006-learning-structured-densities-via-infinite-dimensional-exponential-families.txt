Learning Structured Densities via Infinite Dimensional Exponential Families

Siqi Sun TTI Chicago siqi.sun@ttic.edu

Mladen Kolar University of Chicago mkolar@chicagobooth.edu

Jinbo Xu TTI Chicago jinbo.xu@gmail.com

Abstract
Learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications. Current approaches are mainly focused on learning the structure under restrictive parametric assumptions, which limits the applicability of these methods. In this paper, we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model. We consider probabilities that are members of an infinite dimensional exponential family [4], which is parametrized by a reproducing kernel Hilbert space (RKHS) H and its kernel k. One difficulty in learning nonparametric densities is the evaluation of the normalizing constant. In order to avoid this issue, our procedure minimizes the penalized score matching objective [10, 11]. We show how to efficiently minimize the proposed objective using existing group lasso solvers. Furthermore, we prove that our procedure recovers the graph structure with high-probability under mild conditions. Simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process.
1 Introduction
Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17]. In an undirected graphical model, conditional independence assumptions underlying a probability distribution are encoded in the graph structure. Furthermore, the joint probability density function can be factorized according to the cliques of the graph [14]. One of the fundamental problems in the literature is learning the structure of a graphical model given an i.i.d. sample from an unknown distribution. A lot of work has been done under specific parametric assumptions on the unknown distribution. For example, in Gaussian Graphical Models the structure of the graph is encoded by the sparsity pattern of the precision matrix [6, 30]. Similarly, in the context of exponential family graphical models, where the node conditional distribution given all the other nodes is a member of an exponential family, the structure is described by the non-zero coefficients [29]. Most existing approaches to learn the structure of a high-dimensional undirected graphical model are based on minimizing a penalized loss objective, where the loss is usually a log-likelihood or a composite likelihood and the penalty induces sparsity on the resulting parameter vector [see, for example, 6, 12, 18, 22, 24, 29, 30]. In addition to sparsity inducing penalties, methods that use other structural constraints have been proposed. For example, since many real-world networks are scale-free [1], several algorithms are designed specifically to learn structure of such networks
1

[5, 19]. Graphs tend to have cluster structure and learning simultaneously the structure and cluster assignment has been investigated [2, 27].
In this paper, we focus on learning the structure of a pairwise graphical models without assuming a parametric class of models. The main challenge in estimating nonparametric graphical models is computation of the log normalizing constant. To get around this problem, we propose to use score matching [10, 11] as a divergence, instead of the usual KL divergence, as it does not require evaluation of the log partition function. The probability density function is estimated by minimizing the expected distance between the model score function and the data score function, where the score function is defined as gradient of the corresponding probability density functions. The advantage of this measure is that the normalization constant is canceled out when computing the distance. In order to learn the underlying graph structure, we assume that the logarithm of the density is additive in node-wise and edge-wise potentials and use a sparsity inducing penalty to select non-zero edge potentials. As we will prove later, our procedure will allow us to consistently estimate the underlying graph structure.
The rest of paper is organized as follows. We first introduce the notations, background and related work. Then we formulate our model, establish a representer theorem and present a group lasso algorithm to optimize the objective. Next we prove that our estimator is consistent by showing that it can recover the true graph with high probability given sufficient number of samples. Finally the results for simulated data are presented to demonstrate the correctness of our algorithm empirically.

1.1 Notations

Let [n] denote the set {1, 2, . . . , n}. For a vector  = (1, . . . , d)T  Rd, let  p =

(

i[d]

|i

|p

)

1 p

denote its lp norm.

Let column vector vec(D) denote the vectorization of ma-

trix D, cat(a, b) denote the concatenation of two vectors a and b, and mat(aT1 , . . . , aTd ) the

matrix with rows given by aT1 , . . . , aTd . For   Rd, let Lp(, p0) denote the space of function for which the p-th power of absolute value is p0 integrable; and for f  Lp(, p0), let

f Lp(,p0) =

f

p

=

(



|f

|p

dx)

1 p

denote its Lp norm.

Throughout the paper, we denote H

(or Hi, Hij) as Hilbert space and *, * H, * H as corresponding inner product and norm.

For any operator C : H1  H2, we use C to denote the usual operator norm, which is defined as

C = inf{a  0 : Cf H2  a f H1 for all f  H1}; and C HS to denote its Hilbert-Schmidt norm, which is defined as

C

2 HS

=

C ei

2 H2

,

iI

where ei is an orthonormal basis of H for an index set I. Also, we use R(C) to denote operator C's range space. For any f  H1 and g  H2, let f  g denote their tensor product.

2 Background & Related Work

2.1 Learning graphical models in exponential families

Let x = (x1, x2, ..., xd) be a d-dimensional random vector from a multivariate Gaussian distribution. It is well known that the conditional independency of two variables given all the others is encoded
in the zero pattern of its precision matrix , that is, xi and xj are conditionally independent given x-ij if and only if ij = 0, where x-ij is the vector of x without xi and xj. A sparse estimate of  can be obtained by maximum-likelihood (joint selection) or pseudo-likelihood (neighborhood
selection) optimization with an added l1 penalty [6, 22, 30]. Given n independent realizations of x (rows of X  Rnxd), the penalized maximum-likelihood estimate of the precision matrix can be obtained as

  = arg min tr(S) - log det  +   1,
0

(1)

where S = n-1XT X and  controls the sparsity level of estimated graph.

2

The pseudo-likelihood method estimates the neighborhood of a node a by the non-zeros of the

solution to a regularized linear model

s

=

arg

min


1 n

Xs - X-s

2 2

+





1.

(2)

The estimated neighborhood is then N (s) = {a : sa = 0}.

Another way to specify a parametric graphical model is by assuming that each node-conditional
distributions is a part of the exponential family [29]. Specifically, the conditional distribution of xs given x-s is assumed to be

P (xs|x-s) = exp(

stxsxt + C(xs) - D(x-s, )),

tN (s)

(3)

where C is the base measure, D is the log-normalization constant and N (s) is the neighborhood a the node s. Similar to (2), the neighborhood of each node can be estimated by minimizing the negative log-likelihood with l1 penalty on . The optimization is tractable when the normalization constant D can be easily computed based on the model assumption. For example, under Poisson graphical model assumptions for count data, the normalization constant is - exp( tN(s) stxt). When using the neighborhood estimation, the graph can be estimated as the union of the neighborhoods of each
node, which leads to consistent graph estimation [22, 29].

2.2 Generalized Exponential Family and RKHS

We say H is a RKHS associated with kernel k :  x   R+ if and only if for each x  , the following two conditions are satisfied: (1) k(*, x)  H and (2) it has reproducing properties such that
f (x) = f, k(*, x) H for all f (*)  H, where k is a symmetric and positive semidefinite function. Denote the RKHS H with kernel k as H(k).

For any f  H(k), there exists a set of xi and i, such that f (*)

for any g  H(k), g(*) =

 j=1

jk(*, yj),

the

inner

product

of

f

= and

 i=1

ik(*,

xi).

g is defined as

Similarly f, g H =

 i,j=1

ij k(xi,

yj ).

Therefore

the

norm

of

f

simply

is

f

H=

i,j ijk(xi, xj). The sum-

mation is guaranteed to be larger than or equal to zero because the kernel k is positive semidefinite.

We consider the exponential family in infinite dimensions [4], where P = {pf (x) = ef(x)-A(f)q0(x), x  ; f  F }
and the function space F is defined as

F = {f  H(k) : A(f ) = log ef(x)q0(x)dx < },

where q0(x) is the base measure, A(f ) is a generalized normalization constant such that pf (x) is a valid probability density function, and H is a RKHS [3] associated with kernel k. To see it as a generalization of the exponential family, we show some examples that can generate useful finite dimension exponential families:

* Normal:  = R, k(x, y) = xy + x2y2 * Poisson:  = N  {0}, k(x, y) = xy * Exponential:  = R+, k(x, y) = xy.

For more detailed information, please refer to [4].

When learning structure of a graphical model, we will further impose structural conditions on H(k) in order ensure that F consists of additive functions.

2.3 Score Matching

Score matching is a convenient procedure that allows for estimating a probability density without

computing the normalizing constant [10, 11]. It is based on minimizing Fisher divergence

1 J(p p0) = 2

p(x)

 log p(x) -  log p0(x)

2
dx,

x x 2

(4)

3

where

 log p(x) x

=

(



log p(x) x1

,

.

.

.

,



log p(x) xd

)

is

the score function.

Observe that for p(x, ) =

1 Z()

q(x,

)

the

normalization

constant

Z ()

cancels

out

in

the

gradient

computation,

which

makes

the divergence independent of Z(). Since the score matching objective involves the unknown or-

acle probability density function p0, it is typically not computable. However, under some mild

conditions which we will discuss in METHODS section, (4) can be rewritten as

J (p p0) =

p0(x)
i[d]

1 (
2

log p(x) )2 xi

+

2

log p(x) x2i dx.

(5)

After substituting the expectation with an empirical average, we get

J(p

p0) =

1 n
a[n] i[d]

1 (  log p(Xa) )2 + 2 xi



2

log p(Xa x2i

)

.

(6)

Compared to maximum likelihood estimation, minimizing J(p p0) is computationally tractable. While we will be able to estimate p0 only up to a scale factor, this will be sufficient for the purpose of graph structure estimation.

3 Methods

3.1 Model Formulation and Assumptions

We assume that the true probability density function p0 is in P. Furthermore, for simplicity we assume that

log p0(x) = f (x) =

f0,ij (xi, xj ),

ij (i,j)S

where f0,ii(xi, xi) is a node potential and f0,ij(xi, xj) is an edge potential. The set S denotes the edge set of the graph. Extensions to models where potentials are defined over larger cliques are possible. We further assume that f0,ij  Hij(kij), where Hij is a RKHS with kernel kij. To simplify the notation, we use f0,ij(x) or kij(*, x) to denote f0,ij(xi, xj) and kij(*, (xi, xj)). If the context is clear, we drop the subscript for norm or inner product. Define

H(S) = {f =

fij |fij  Hij }

(i,j)S

(7)

as a set of functions that decompose as sum of bivariate functions on edge set S. Note that

H(S) is also (a subset of) a RKHS with the norm

f

2 H(S)

=

(i,j)S

fij

2 Hij

and

kernel

k = (i,j)S kij .

Let (f ) = f H,1 = ij fij Hij . For any edge set S (not necessarily the true edge set), we denote S(fS) = sS fs Hs as the norm  reduced to S. Similarly, denote its dual norm as S [fS ] = maxS(gS)1 fS , gS [25].

Under the assumption that the unknown f0 is additive, the loss function becomes

1 J(f ) =
2

p0(x)
i[d]

f (x) - f0(x) xi xi

2
dx

1 =
2

fij - f0,ij ,

i[d] j,j [d]

p0(x)



kij

(*, (xi, xi

xj

))



kij

(*, (xi, xj xi

)) dx(fij

- f0,ij )

1 =
2

fij - f0,ij , Cijij (fij - f0,ij ) .

i[d] j,j [d]

Intuitively, C can be viewed as a d2 matrix, and the operator at position (ij, ij ) is Cij,ij . For general (ij, i j ), i = i the corresponding operator simply is 0. Define CSS as

p0(x)
(i,j)S,(i ,j )S

kij(*, (xi, xj))  ki j (*, (xi , xj )) dx, xi xi

4

which intuitively can be treated as a sub matrix of C with rows S and columns S . We will use this notation intensively in the main theorem and its proof.

Following [26], we make the following assumptions.

A1. Each kij is twice differentiable on  x .

A2. For any i and xj  j = [aj, bj], we assume that

lim
xia+i or b-i

2kij(x, y) xiyi

|y=x

p20(x)

=

0,

where x = (xi, xj) and ai, bi could be - or .

A3. This condition ensures that J(p p0) <  for any p  P [for more details see 26]:

kij(*, x) xi

Hij  L2(, p0),

2kij(*, x) x2i

Hij  L2(, p0).

A4. The operator CSS, is compact and the smallest eigenvalue min = min(CSS) > 0.
A5. Sc [CScSCS-S1]  1 - , where  > 0.
A6. f0  R(C), which means there exists   H, such that f0 = C. f0 is the oracle function.
We will discuss the definition of operator C and  in section 4. Compared with [29], A4 can be interpreted as the dependency condition and the A5 is the incoherence condition, which is a standard condition for structure learning in high dimensional statistical estimators.

3.2 Estimation Procedure

We estimate f by minimizing the following penalized score matching objective

min
f

L(f )

=

J(f )

+

 2

f

H,1

s.t. fij  Hij ,

(8)

where J(f ) is given in (6). The norm f H,1 = ij fij Hij is used as a sparsity inducing penalty. A simplified form of J(f ) is given below that will lead to efficient algorithm for solving
(8).

The following theorem states that the score matching objective can be written as a penalized quadratic function on f .

Theorem 3.1 (i) The score matching objective can be represented as

L(f )

=

1 2

f - f0, C(f - f0)

 +
2

f

H,1

where C =

p0(x)

i[d]

k(*,x) xi



k(*,x) xi

dx

is

a

trace

operator.

(ii) Given observed data Xnxd, the empirical estimation of L is

(9)

L(f

)

=

1 2

f, Cf

+

fij , -ij

 +
2

f

H,1 + const

ij

(10)

where C

=

1 n

a[n]

 k(*,Xa ) i[d] xi



 k(*,Xa ) xi

and

ij

=

1 n

+2kij (*,(Xai,Xaj ))

a[n]

 x2i

2kij (*,(Xai,Xaj ))  x2j

if i

=

j, or ij

=

1 n

an

2kij (*,(Xai,Xaj ))  x2i

otherwise.

Please refer to our supplementary material for detailed proof 1.

The above theorem still requires us to minimize over F. Our next results shows that the solution is finite dimensional. That is, we establish a representer theorem for our problem.

1Please visit ttic.uchicago.edu/siqi for supplementary material and code.

5

Theorem 3.2 (i) The solution to (10) can be represented as

fij (*)

= bij
b[n]

kij (*, (Xbi, Xbj )) xi

+

bj

i

kij

(*,

(Xbi, xj

Xbj

))

+

ij ij ,

(11)

where i  j. (ii) Minimizing (10) is equivalent to minimizing the following quadratic function:

1 2n
ai

2

(bij Gaijb11 + bjiGaijb12) +

ij h1ija

bj j

+

(bij h1ijb + bjih2ijb) +

ij

ij

2+  2

f

H,1

ij b

ij

1 =
2n

(DaTi

*

)2

+

Et

+

 2

ai ij

itj Fij ij

(12)

where Gaijbrs

=

, h2kij (Xa,Xb)
xr ys

rb ij

=



kij (*,Xb xr

)

,

ij

are constant that only depends on X,  =

cat(vec(), vec()) is the vector parameter and ij = cat(ij, vec(*ij)) is a group of parameters.

Dai, E and F are corresponding constant vectors and matrices based on G, h and the order of

parameters. Then the above problem can be solved by group lasso [7, 21].

The first part of theorem states our representer theorem, and the second part is obtained by plugging in (11) to (10). See supplementary material for a detailed proof. Theorem 3.2 provides us with an efficient way to minimize (8), as it reduced the optimization to a group lasso problem for which many efficient solvers exist.

Let f = arg minfH L(f ) denote the solution to (12). We can estimate the graph as follows:

S = {(i, j) : fij = 0},

(13)

That is, the graph is encoded in the sparsity pattern of f.

4 Statistical Guarantees

In this section we study statistical properties of the proposed estimator (13). Let S denote the true edge set and Sc its complement. We prove that S recovers S with high probability when the sample size n is sufficiently large.
Denote D = mat(D1T1, . . . , DaTi, . . . , DnTd). We will need the following result on the estimated operator C,

Proposition 4.1 (Lemma 5 in [8] or Theorem 5 in [26] ) (Properties of C)

1.

C - C

HS

=

Op0

(n-

1 2

)

2.

(C + L)-1





min

1 diag(L)

,

C(C + L)-1

 1, where  > 0 and L is diagonal with

positive constants.

The following result gives first order optimality conditions for the optimization problem (8).

Proposition 4.2 (Optimality Condition)

J(f )

+

 2

(f )2

achieves

optimality

when

the

following

two

conditions

are

satisfied:

(1)

fs J(f ) + (f )

fs = 0 fs Hs

(2) Sc [fSc J(f )]  (f ).

s  S

6

With these preliminary results, we have the following main results.

Theorem 4.3 Assume that conditions A1-A7 are satisfied. The regularization parameter  is se-

lected

at

the

order

of

n-

1 4

and satisfies 



 min min

4(1-)max

|S

|+

 5

,

where

min

=

minsS

fs

>0

and max = maxsS fs > 0. Then P (S = S)  1.

Proof Idea: The theorem above is the main theoretical guarantee for our score matching estimator. We use the "witness" proof framework inspired by [23, 29]. Let f  denote the true density function

and p the probability density function. We first construct a solution fS on true edge set S as

fS

=

min
fSc =0

J(f ) +

 (
2
(i,j)S

fij

)2

(14)

and set fSc as zero. Using Proposition 4.1, we prove that fS - fS

=

Op

(n-

1 4

).

Then we

compute the subgradient on Sc and prove that its dual norm is upper bounded by (f ) by using

assumptions A4, A5 and A6. Therefore we construct a solution that satisfied the optimality condition

and converges in probability to the true graph. Refer to supplementary material for detailed proof.

5 Experiments

We illustrate performance of our method on two simulations. In our experiments, we use the same kernel defined as follows:

k(x, y) = exp(-

x-y 22

2
2 ) + r(xT y + c)2,

(15)

that is, the summation of a Gaussian kernel and a polynomial kernel. We set 2 = 1.5, r = 0.1 and c = 0.5 for all the simulations.

We report the true positive rate vs false positive rate (ROC) curve to measure the performance of

different procedures. Let S be the true edge set, and let S be the estimated graph. The true positive

rate is defined as TPR

=

|S=1 and S=1| |S=1|

,

and

false

positive

rate

is

FPR

=

|S=1 and S=0| |S=0|

,

where

|

*

|

is the cardinality of the set. The curve is then plotted based on 100 uniformly-sampled regularization

parameters and based on 20 independent runs.

In the first simulation, we apply our algorithm to data sampled from a simple chain graph-based
Gaussian model (see Figure 1 for detail), and compare its performance with glasso [6]. We use the same sampling method as in [31] to generate the data: we set s = 0.4 for s  S and its diagonal to a constant such that  is positive definite. We set the dimension d to 25 and change the sample size n  {20, 40, 60, 80, 100} data points.

Except for the low sample size case (n = 20), the performance of our method is comparable with glasso, without utilizing the fact that the underlying distribution is of a particular parametric form. Intuitively, to capture the graph structure, the proposed nonparametric method requires more data because of much weaker assumptions.

To further show the strength of our algorithm, we test it on a nonparanormal (NPN) distribution
([18]). A random vector x = (x1, . . . , xp) has a nonparanormal distribution if there exist functions (f1, . . . , fp) such that (f1(x1), . . . , fd(xd))  N (, ). When f is monotone and differentiable, the probability density function is given by

P (x)

1

=

(2

)

p 2

||

1 2

exp{- 1 (f (x) - )T -1(f (x) - )} 2

j

|fj |.

Here the graph structure is still encoded in the sparsity pattern of  = -1, that is, xixj|x-i,j if and only if ij = 0 [18].

In our experiments we use the "Symmetric Power Transformation" [18], that is,

fj(zj) = j(

g0(zj - j g02(t - j)(

)
t-j j

)dt

)

+

j

,

7

0.0 0.2 0.4 0.6 0.8 1.0

Adjacent Matrix 0.0 0.2 0.4 0.6 0.8 1.0

TruePositiveRate 0.0 0.2 0.4 0.6 0.8 1.0

Glasso

qq q qqq q q

qq qq

q

q qq q q q q qq q q
q
q q

q q

q

q q
q
q
q q q q
q
q q q
q q q q q
q q q q
qq q
qq q q qq q qq qqq qqqq qq qqqqq qqqqqq

q

q

q

q

q

0.0 0.2 0.4 0.6 0.8 FalsePositiveRate

q
q 20 40 60 80 100 1.0

TruePositiveRate 0.0 0.2 0.4 0.6 0.8 1.0

SME

qq
q qq q
qq

qq q
q

q q

qq qq

q

q
qq q q
q

q q q

q q
q

q
q qq

q q
q
q q
q
q
q
q q q q q
q q
q q q q q qq q q q q qq q q q qq q q q qqqqqqqqqqqqqqqq

q 20 40 60 80 100

0.0 0.2 0.4 0.6 0.8 1.0 FalsePositiveRate

Figure 1: The estimation results for Gaussian graphical models. left: The adjacent matrix of true graph. center: the ROC curve of glasso. right: the ROC curve of score matching estimator (SME).

TruePositiveRate 0.0 0.2 0.4 0.6 0.8 1.0

Glasso

q
q
q
q
q q
q
q q
q
q q q q q q q q
q q qq q q q q q qqqq qqqqq qqqqqqqqqqqqqqqqq qqqqqqqqqqqqqqqqqqq

q

q

q

0.0 0.2 0.4 0.6 FalsePositiveRate

q
q 20 40 60 80 100
0.8 1.0

TruePositiveRate 0.0 0.2 0.4 0.6 0.8 1.0

NonParaNormal

qq

q

q

q

qq qqqq q q q q

q

qq q

q

qqq q

q qq q

q

qqq

q qqqq

q q

q qq q q
qqq q
q q qqq

q qqq

q q

q
q
q q q q

q q
q
q q
q q q

q q q
q
q q
q
q
q q q
q q q
qq
qq q qqq qqqqqq qqqqq

q

0.0 0.2 0.4 0.6 0.8 FalsePositiveRate

q
q 20 40 60 80 100 1.0

TruePositiveRate 0.0 0.2 0.4 0.6 0.8 1.0

SME

q
qq q qq q qq q q

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq qq qqqq q q qqqqqqq qq qq qqq
q qq q q q q q qq q q q q q qq q
q
qq q

q qq q q q
q

q qq q
q
q q q q
q qq

q q
qq q q
q q qq q qqqqq qq q qqqqqqqqqqqqqqqqqq

q

20 40 60 80 100

0.0 0.2 0.4 0.6 0.8 1.0 FalsePositiveRate

Figure 2: The estimated ROC curves of nonparanormal graphical models for glasso (left), NPN (center) and SME (right).
where g0(t) = sign(t)|t|, to transform data. For comparison with graph lasso, we first use a truncation method to Gaussianize the data, and then apply graphical lasso to the transformed data. See [18, 31] for details. From figure 2, without knowing the underlying data distribution, the score matching estimator outperforms glasso, and show similar results to nonparanormal when the sample size is large.
6 Discussion
In this paper, we have proposed a new procedure for learning the structure of a nonparametric graphical model. Our procedure is based on minimizing a penalized score matching objective, which can be performed efficiently using existing group lasso solvers. Particularly appealing aspect of our approach is that it does not require computing the normalization constant. Therefore, our procedure can be applied to a very broad family of infinite dimensional exponential families. We have established that the procedure provably recovers the true underlying graphical structure with highprobability under mild conditions. In the future, we plan to investigate more efficient algorithms for solving (10), since it is often the case that C is well structured and can be efficiently approximated.
Acknowledgments
The authors are grateful to the financial support from National Institutes of Health R01GM0897532, National Science Foundation CAREER award CCF-1149811 and IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business. This work was completed in part with resources provided by the University of Chicago Research Computing Center.
8

References
[1] R. Albert. Scale-free networks in cell biology. Journal of cell science, 118(21):4947-4957, 2005. [2] C. Ambroise, J. Chiquet, C. Matias, et al. Inferring sparse gaussian graphical models with latent structure.
Electronic Journal of Statistics, 3:205-238, 2009. [3] N. Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, pages
337-404, 1950. [4] S. Canu and A. Smola. Kernel methods and the exponential family. Neurocomputing, 69(7):714-720,
2006. [5] A. Defazio and T. S. Caetano. A convex formulation for learning scale-free networks via submodular
relaxation. In Advances in Neural Information Processing Systems, pages 1250-1258, 2012. [6] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso.
Biostatistics, 9(3):432-441, 2008. [7] J. Friedman, T. Hastie, and R. Tibshirani. A note on the group lasso and a sparse group lasso. arXiv
preprint arXiv:1001.0736, 2010. [8] K. Fukumizu, F. R. Bach, and A. Gretton. Statistical consistency of kernel canonical correlation analysis.
The Journal of Machine Learning Research, 8:361-383, 2007. [9] S. Geman and C. Graffigne. Markov random field image models and their applications to computer vision.
In Proceedings of the International Congress of Mathematicians, volume 1, page 2, 1986. [10] A. Hyvarinen. Estimation of non-normalized statistical models by score matching. In Journal of Machine
Learning Research, pages 695-709, 2005. [11] A. Hyvarinen. Some extensions of score matching. Computational statistics & data analysis, 51(5):2499-
2512, 2007. [12] Y. Jeon and Y. Lin. An effective method for high-dimensional log-density anova estimation, with appli-
cation to nonparametric graphical model building. Statistica Sinica, 16(2):353, 2006. [13] R. Kindermann, J. L. Snell, et al. Markov random fields and their applications, volume 1. American
Mathematical Society Providence, RI, 1980. [14] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009. [15] Y. A. Kourmpetis, A. D. Van Dijk, M. C. Bink, R. C. van Ham, and C. J. ter Braak. Bayesian markov
random field analysis for protein function prediction based on network data. PloS one, 5(2):e9293, 2010. [16] J. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields: Probabilistic models for segment-
ing and labeling sequence data. 2001. [17] S. Z. Li. Markov random field modeling in Image Analysis. 2011. [18] H. Liu, J. Lafferty, and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimen-
sional undirected graphs. The Journal of Machine Learning Research, 10:2295-2328, 2009. [19] Q. Liu and A. T. Ihler. Learning scale free networks by reweighted l1 regularization. In International
Conference on Artificial Intelligence and Statistics, pages 40-48, 2011. [20] C. D. Manning and H. Schutze. Foundations of statistical natural language processing. MIT press, 1999. [21] L. Meier, S. Van De Geer, and P. Buhlmann. The group lasso for logistic regression. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 70(1):53-71, 2008. [22] N. Meinshausen and P. Buhlmann. High-dimensional graphs and variable selection with the lasso. The
Annals of Statistics, pages 1436-1462, 2006. [23] P. Ravikumar, M. J. Wainwright, and J. Lafferty. High-dimensional graphical model selection using l1-
regularized logistic regression. 2008. [24] P. Ravikumar, M. J. Wainwright, J. D. Lafferty, et al. High-dimensional ising model selection using
1-regularized logistic regression. The Annals of Statistics, 38(3):1287-1319, 2010. [25] R. T. Rockafellar. Convex analysis. Number 28. Princeton university press, 1970. [26] B. Sriperumbudur, K. Fukumizu, R. Kumar, A. Gretton, and A. Hyvarinen. Density estimation in infinite
dimensional exponential families. arXiv preprint arXiv:1312.3516, 2013. [27] S. Sun, H. Wang, and J. Xu. Inferring block structure of graphical models in exponential families. In
Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages 939-947, 2015. [28] Z. Wei and H. Li. A markov random field model for network-based analysis of genomic data. Bioinformatics, 23(12):1537-1544, 2007. [29] E. Yang, G. Allen, Z. Liu, and P. K. Ravikumar. Graphical models via generalized linear models. In Advances in Neural Information Processing Systems, pages 1358-1366, 2012. [30] M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model. Biometrika, 94(1):19-35, 2007. [31] T. Zhao, H. Liu, K. Roeder, J. Lafferty, and L. Wasserman. The huge package for high-dimensional undirected graph estimation in r. The Journal of Machine Learning Research, 13(1):1059-1062, 2012.
9

